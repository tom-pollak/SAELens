{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook with Example Config for Different Models / Hooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from sae_training.config import LanguageModelSAERunnerConfig\n",
    "from sae_training.lm_runner import language_model_sae_runner\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "print(\"Using device:\", device)\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gelu-2L\n",
    "\n",
    "An example of a toy language model we're able to train on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP Out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = LanguageModelSAERunnerConfig(\n",
    "    # Data Generating Function (Model + Training Distibuion)\n",
    "    model_name=\"gelu-2l\",\n",
    "    hook_point=\"blocks.0.hook_mlp_out\",\n",
    "    hook_point_layer=0,\n",
    "    d_in=512,\n",
    "    dataset_path=\"NeelNanda/c4-tokenized-2b\",\n",
    "    is_dataset_tokenized=True,\n",
    "    # SAE Parameters\n",
    "    expansion_factor=[16,32,64],\n",
    "    b_dec_init_method=\"geometric_median\",  # geometric median is better but slower to get started\n",
    "    # Training Parameters\n",
    "    lr=0.0012,\n",
    "    lr_scheduler_name=\"constantwithwarmup\",\n",
    "    l1_coefficient=0.00016,\n",
    "    train_batch_size=4096,\n",
    "    context_size=128,\n",
    "    # Activation Store Parameters\n",
    "    n_batches_in_buffer=128,\n",
    "    total_training_tokens=1_000_000 * 100,\n",
    "    store_batch_size=32,\n",
    "    # Resampling protocol\n",
    "    use_ghost_grads=True,\n",
    "    feature_sampling_window=5000,\n",
    "    dead_feature_window=5000,\n",
    "    dead_feature_threshold=1e-4,\n",
    "    # WANDB\n",
    "    log_to_wandb=True,\n",
    "    wandb_project=\"mats_sae_training_language_models_gelu_2l_test\",\n",
    "    wandb_log_frequency=10,\n",
    "    # Misc\n",
    "    device=device,\n",
    "    seed=42,\n",
    "    n_checkpoints=0,\n",
    "    checkpoint_path=\"checkpoints\",\n",
    "    dtype=torch.float32,\n",
    ")\n",
    "\n",
    "\n",
    "sparse_autoencoder = language_model_sae_runner(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT2 - Small"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residual Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sae_training.config import LanguageModelSAERunnerConfig\n",
    "from sae_training.lm_runner import language_model_sae_runner\n",
    "\n",
    "layer = 3\n",
    "cfg = LanguageModelSAERunnerConfig(\n",
    "    # Data Generating Function (Model + Training Distibuion)\n",
    "    model_name=\"gpt2-small\",\n",
    "    hook_point=f\"blocks.{layer}.hook_resid_pre\",\n",
    "    hook_point_layer=layer,\n",
    "    d_in=768,\n",
    "    dataset_path=\"Skylion007/openwebtext\",\n",
    "    is_dataset_tokenized=False,\n",
    "    # SAE Parameters\n",
    "    expansion_factor=32,  # determines the dimension of the SAE.\n",
    "    b_dec_init_method=\"mean\",  # geometric median is better but slower to get started\n",
    "    # Training Parameters\n",
    "    lr=0.0004,\n",
    "    l1_coefficient=0.00008,\n",
    "    lr_scheduler_name=\"constantwithwarmup\",\n",
    "    train_batch_size=4096,\n",
    "    context_size=128,\n",
    "    lr_warm_up_steps=5000,\n",
    "    # Activation Store Parameters\n",
    "    n_batches_in_buffer=128,\n",
    "    total_training_tokens=1_000_000 * 300,  # 200M tokens seems doable overnight.\n",
    "    store_batch_size=32,\n",
    "    # Resampling protocol\n",
    "    use_ghost_grads=True,\n",
    "    feature_sampling_window=2500,\n",
    "    dead_feature_window=5000,\n",
    "    dead_feature_threshold=1e-8,\n",
    "    # WANDB\n",
    "    log_to_wandb=True,\n",
    "    wandb_project=\"mats_sae_training_language_models_resid_pre_test\",\n",
    "    wandb_entity=None,\n",
    "    wandb_log_frequency=100,\n",
    "    # Misc\n",
    "    device=\"cuda\",\n",
    "    seed=42,\n",
    "    n_checkpoints=10,\n",
    "    checkpoint_path=\"checkpoints\",\n",
    "    dtype=torch.float32,\n",
    ")\n",
    "\n",
    "sparse_autoencoder = language_model_sae_runner(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pythia 70-M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "from sae_training.config import LanguageModelSAERunnerConfig\n",
    "from sae_training.lm_runner import language_model_sae_runner\n",
    "\n",
    "import cProfile\n",
    "\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "cfg = LanguageModelSAERunnerConfig(\n",
    "    # Data Generating Function (Model + Training Distibuion)\n",
    "    model_name=\"pythia-70m-deduped\",\n",
    "    hook_point=\"blocks.0.hook_mlp_out\",\n",
    "    hook_point_layer=0,\n",
    "    d_in=512,\n",
    "    dataset_path=\"EleutherAI/the_pile_deduplicated\",\n",
    "    is_dataset_tokenized=False,\n",
    "    # SAE Parameters\n",
    "    expansion_factor=64,\n",
    "    # Training Parameters\n",
    "    lr=3e-4,\n",
    "    l1_coefficient=4e-5,\n",
    "    train_batch_size=8192,\n",
    "    context_size=128,\n",
    "    lr_scheduler_name=\"constantwithwarmup\",\n",
    "    lr_warm_up_steps=10_000,\n",
    "    # Activation Store Parameters\n",
    "    n_batches_in_buffer=64,\n",
    "    total_training_tokens=1_000_000 * 800,\n",
    "    store_batch_size=32,\n",
    "    # Resampling protocol\n",
    "    feature_sampling_method=\"anthropic\",\n",
    "    feature_sampling_window=2000,  # Doesn't currently matter.\n",
    "    feature_reinit_scale=0.2,\n",
    "    dead_feature_window=40000,\n",
    "    dead_feature_threshold=1e-8,\n",
    "    # WANDB\n",
    "    log_to_wandb=True,\n",
    "    wandb_project=\"mats_sae_training_language_benchmark_tests\",\n",
    "    wandb_entity=None,\n",
    "    wandb_log_frequency=20,\n",
    "    # Misc\n",
    "    device=\"cuda\",\n",
    "    seed=42,\n",
    "    n_checkpoints=0,\n",
    "    checkpoint_path=\"checkpoints\",\n",
    "    dtype=torch.float32,\n",
    ")\n",
    "\n",
    "\n",
    "sparse_autoencoder = language_model_sae_runner(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pythia 70M Hook Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "from sae_training.config import LanguageModelSAERunnerConfig\n",
    "from sae_training.lm_runner import language_model_sae_runner\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "cfg = LanguageModelSAERunnerConfig(\n",
    "    # Data Generating Function (Model + Training Distibuion)\n",
    "    model_name=\"pythia-70m-deduped\",\n",
    "    hook_point=\"blocks.2.attn.hook_q\",\n",
    "    hook_point_layer=2,\n",
    "    hook_point_head_index=7,\n",
    "    d_in=64,\n",
    "    dataset_path=\"EleutherAI/the_pile_deduplicated\",\n",
    "    is_dataset_tokenized=False,\n",
    "    # SAE Parameters\n",
    "    expansion_factor=16,\n",
    "    # Training Parameters\n",
    "    lr=0.0012,\n",
    "    l1_coefficient=0.003,\n",
    "    lr_scheduler_name=\"constantwithwarmup\",\n",
    "    lr_warm_up_steps=1000,  # about 4 million tokens.\n",
    "    train_batch_size=4096,\n",
    "    context_size=128,\n",
    "    # Activation Store Parameters\n",
    "    n_batches_in_buffer=128,\n",
    "    total_training_tokens=1_000_000 * 1500,\n",
    "    store_batch_size=32,\n",
    "    # Resampling protocol\n",
    "    feature_sampling_method=\"anthropic\",\n",
    "    feature_sampling_window=1000,  # doesn't do anything currently.\n",
    "    feature_reinit_scale=0.2,\n",
    "    resample_batches=8,\n",
    "    dead_feature_window=60000,\n",
    "    dead_feature_threshold=1e-5,\n",
    "    # WANDB\n",
    "    log_to_wandb=True,\n",
    "    wandb_project=\"mats_sae_training_pythia_70M_hook_q_L2H7\",\n",
    "    wandb_entity=None,\n",
    "    wandb_log_frequency=100,\n",
    "    # Misc\n",
    "    device=\"mps\",\n",
    "    seed=42,\n",
    "    n_checkpoints=15,\n",
    "    checkpoint_path=\"checkpoints\",\n",
    "    dtype=torch.float32,\n",
    ")\n",
    "\n",
    "sparse_autoencoder = language_model_sae_runner(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tiny Stories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run name: 4096-L1-[0.0002, 0.0003, 0.0006]-LR-0.0001-Tokens-1.000e+07\n",
      "n_tokens_per_buffer (millions): 0.524288\n",
      "Lower bound: n_contexts_per_buffer (millions): 0.004096\n",
      "Total training steps: 2441\n",
      "Total wandb updates: 244\n",
      "n_tokens_per_feature_sampling_window (millions): 262.144\n",
      "n_tokens_per_dead_feature_window (millions): 131.072\n",
      "We will reset the sparsity calculation 4 times.\n",
      "Number tokens in sparsity calculation window: 2.05e+06\n",
      "Loaded pretrained model tiny-stories-1M into HookedTransformer\n",
      "Moving model to device:  mps\n",
      "Run name: 4096-L1-0.0002-LR-0.0001-Tokens-1.000e+07\n",
      "n_tokens_per_buffer (millions): 0.524288\n",
      "Lower bound: n_contexts_per_buffer (millions): 0.004096\n",
      "Total training steps: 2441\n",
      "Total wandb updates: 244\n",
      "n_tokens_per_feature_sampling_window (millions): 262.144\n",
      "n_tokens_per_dead_feature_window (millions): 131.072\n",
      "We will reset the sparsity calculation 4 times.\n",
      "Number tokens in sparsity calculation window: 2.05e+06\n",
      "Run name: 4096-L1-0.0002-LR-0.0001-Tokens-1.000e+07\n",
      "n_tokens_per_buffer (millions): 0.524288\n",
      "Lower bound: n_contexts_per_buffer (millions): 0.004096\n",
      "Total training steps: 2441\n",
      "Total wandb updates: 244\n",
      "n_tokens_per_feature_sampling_window (millions): 262.144\n",
      "n_tokens_per_dead_feature_window (millions): 131.072\n",
      "We will reset the sparsity calculation 4 times.\n",
      "Number tokens in sparsity calculation window: 2.05e+06\n",
      "Run name: 4096-L1-0.0003-LR-0.0001-Tokens-1.000e+07\n",
      "n_tokens_per_buffer (millions): 0.524288\n",
      "Lower bound: n_contexts_per_buffer (millions): 0.004096\n",
      "Total training steps: 2441\n",
      "Total wandb updates: 244\n",
      "n_tokens_per_feature_sampling_window (millions): 262.144\n",
      "n_tokens_per_dead_feature_window (millions): 131.072\n",
      "We will reset the sparsity calculation 4 times.\n",
      "Number tokens in sparsity calculation window: 2.05e+06\n",
      "Run name: 4096-L1-0.0003-LR-0.0001-Tokens-1.000e+07\n",
      "n_tokens_per_buffer (millions): 0.524288\n",
      "Lower bound: n_contexts_per_buffer (millions): 0.004096\n",
      "Total training steps: 2441\n",
      "Total wandb updates: 244\n",
      "n_tokens_per_feature_sampling_window (millions): 262.144\n",
      "n_tokens_per_dead_feature_window (millions): 131.072\n",
      "We will reset the sparsity calculation 4 times.\n",
      "Number tokens in sparsity calculation window: 2.05e+06\n",
      "Run name: 4096-L1-0.0006-LR-0.0001-Tokens-1.000e+07\n",
      "n_tokens_per_buffer (millions): 0.524288\n",
      "Lower bound: n_contexts_per_buffer (millions): 0.004096\n",
      "Total training steps: 2441\n",
      "Total wandb updates: 244\n",
      "n_tokens_per_feature_sampling_window (millions): 262.144\n",
      "n_tokens_per_dead_feature_window (millions): 131.072\n",
      "We will reset the sparsity calculation 4 times.\n",
      "Number tokens in sparsity calculation window: 2.05e+06\n",
      "Run name: 4096-L1-0.0006-LR-0.0001-Tokens-1.000e+07\n",
      "n_tokens_per_buffer (millions): 0.524288\n",
      "Lower bound: n_contexts_per_buffer (millions): 0.004096\n",
      "Total training steps: 2441\n",
      "Total wandb updates: 244\n",
      "n_tokens_per_feature_sampling_window (millions): 262.144\n",
      "n_tokens_per_dead_feature_window (millions): 131.072\n",
      "We will reset the sparsity calculation 4 times.\n",
      "Number tokens in sparsity calculation window: 2.05e+06\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/josephbloom/GithubRepositories/mats_sae_training/scripts/wandb/run-20240325_154839-voo84o5b</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jbloom/mats_sae_training_language_benchmark_tests/runs/voo84o5b' target=\"_blank\">4096-L1-[0.0002, 0.0003, 0.0006]-LR-0.0001-Tokens-1.000e+07</a></strong> to <a href='https://wandb.ai/jbloom/mats_sae_training_language_benchmark_tests' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jbloom/mats_sae_training_language_benchmark_tests' target=\"_blank\">https://wandb.ai/jbloom/mats_sae_training_language_benchmark_tests</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jbloom/mats_sae_training_language_benchmark_tests/runs/voo84o5b' target=\"_blank\">https://wandb.ai/jbloom/mats_sae_training_language_benchmark_tests/runs/voo84o5b</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Objective value: 339329.6875:   4%|▍         | 4/100 [00:00<00:04, 22.66it/s]\n",
      "/Users/josephbloom/GithubRepositories/mats_sae_training/sae_training/sparse_autoencoder.py:161: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.tensor(origin, dtype=self.dtype, device=self.device)\n",
      "/Users/josephbloom/miniforge3/envs/mats_sae_training/lib/python3.11/site-packages/wandb/sdk/wandb_run.py:2171: UserWarning: Run (qy1ho0vw) is finished. The call to `_console_raw_callback` will be ignored. Please make sure that you are using an active run.\n",
      "  lambda data: self._console_raw_callback(\"stderr\", data),\n",
      "2442| MSE Loss 0.000 | L1 0.001: : 10002432it [12:04, 13803.94it/s]\n",
      "100%|██████████| 10/10 [00:01<00:00,  5.05it/s]\n",
      "100%|██████████| 10/10 [00:01<00:00,  5.09it/s]\n",
      "100%|██████████| 10/10 [00:01<00:00,  5.12it/s]\n",
      "100%|██████████| 10/10 [00:02<00:00,  4.96it/s]\n",
      "100%|██████████| 10/10 [00:01<00:00,  5.22it/s]\n",
      "100%|██████████| 10/10 [00:01<00:00,  5.21it/s]\n",
      "100%|██████████| 10/10 [00:01<00:00,  5.12it/s]\n",
      "100%|██████████| 10/10 [00:01<00:00,  5.12it/s]\n",
      "100%|██████████| 10/10 [00:01<00:00,  5.20it/s]\n",
      "100%|██████████| 10/10 [00:01<00:00,  5.09it/s]\n",
      "100%|██████████| 10/10 [00:01<00:00,  5.23it/s]\n",
      "100%|██████████| 10/10 [00:02<00:00,  5.00it/s]\n",
      "100%|██████████| 10/10 [00:02<00:00,  4.71it/s]\n",
      "100%|██████████| 10/10 [00:02<00:00,  4.89it/s]\n",
      "100%|██████████| 10/10 [00:02<00:00,  4.96it/s]\n",
      "100%|██████████| 10/10 [00:02<00:00,  4.93it/s]\n",
      "100%|██████████| 10/10 [00:01<00:00,  5.23it/s]\n",
      "100%|██████████| 10/10 [00:01<00:00,  5.22it/s]\n",
      "100%|██████████| 10/10 [00:02<00:00,  4.96it/s]\n",
      "100%|██████████| 10/10 [00:01<00:00,  5.13it/s]\n",
      "100%|██████████| 10/10 [00:01<00:00,  5.15it/s]\n",
      "100%|██████████| 10/10 [00:01<00:00,  5.03it/s]\n",
      "100%|██████████| 10/10 [00:01<00:00,  5.14it/s]\n",
      "100%|██████████| 10/10 [00:02<00:00,  4.97it/s]\n",
      "'(ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')), '(Request ID: 7e8a4e24-d581-41e5-a4c6-efe32048b72f)')' thrown while requesting GET https://huggingface.co/datasets/apollo-research/roneneldan-TinyStories-tokenizer-gpt2/resolve/bc8db71bbc792977b43d430bddeeb9906e193f8d/data/train-00000-of-00004.parquet\n",
      "Retrying in 1s [Retry 1/5].\n",
      "'(ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')), '(Request ID: d09e549c-a5e6-45c9-aed2-3cc66d288c0b)')' thrown while requesting GET https://huggingface.co/datasets/apollo-research/roneneldan-TinyStories-tokenizer-gpt2/resolve/bc8db71bbc792977b43d430bddeeb9906e193f8d/data/train-00000-of-00004.parquet\n",
      "Retrying in 2s [Retry 2/5].\n",
      "100%|██████████| 10/10 [00:02<00:00,  4.97it/s]\n",
      "100%|██████████| 10/10 [00:01<00:00,  5.05it/s]\n",
      "100%|██████████| 10/10 [00:02<00:00,  4.79it/s]\n",
      "100%|██████████| 10/10 [00:02<00:00,  4.84it/s]\n",
      "100%|██████████| 10/10 [00:01<00:00,  5.16it/s]\n",
      "100%|██████████| 10/10 [00:01<00:00,  5.20it/s]\n",
      "100%|██████████| 10/10 [00:02<00:00,  4.94it/s]\n",
      "100%|██████████| 10/10 [00:01<00:00,  5.15it/s]\n",
      "100%|██████████| 10/10 [00:01<00:00,  5.18it/s]\n",
      "100%|██████████| 10/10 [00:01<00:00,  5.02it/s]\n",
      "100%|██████████| 10/10 [00:01<00:00,  5.18it/s]\n",
      "100%|██████████| 10/10 [00:01<00:00,  5.15it/s]\n",
      "100%|██████████| 10/10 [00:02<00:00,  5.00it/s]\n",
      "100%|██████████| 10/10 [00:02<00:00,  4.74it/s]\n",
      "100%|██████████| 10/10 [00:02<00:00,  4.98it/s]\n",
      "100%|██████████| 10/10 [00:02<00:00,  4.69it/s]\n",
      "100%|██████████| 10/10 [00:01<00:00,  5.05it/s]\n",
      "100%|██████████| 10/10 [00:01<00:00,  5.13it/s]\n",
      "100%|██████████| 10/10 [00:02<00:00,  4.75it/s]\n",
      "100%|██████████| 10/10 [00:02<00:00,  4.95it/s]\n",
      "100%|██████████| 10/10 [00:02<00:00,  4.82it/s]\n",
      "100%|██████████| 10/10 [00:02<00:00,  4.90it/s]\n",
      "100%|██████████| 10/10 [00:01<00:00,  5.12it/s]\n",
      "100%|██████████| 10/10 [00:01<00:00,  5.09it/s]\n",
      "100%|██████████| 10/10 [00:01<00:00,  5.03it/s]\n",
      "100%|██████████| 10/10 [00:01<00:00,  5.14it/s]\n",
      "100%|██████████| 10/10 [00:01<00:00,  5.02it/s]\n",
      "100%|██████████| 10/10 [00:02<00:00,  4.99it/s]\n",
      "100%|██████████| 10/10 [00:05<00:00,  1.90it/s]\n",
      "100%|██████████| 10/10 [00:01<00:00,  5.03it/s]\n",
      "100%|██████████| 10/10 [00:02<00:00,  4.88it/s]\n",
      "100%|██████████| 10/10 [00:01<00:00,  5.15it/s]\n",
      "100%|██████████| 10/10 [00:01<00:00,  5.13it/s]\n",
      "100%|██████████| 10/10 [00:02<00:00,  4.71it/s]\n",
      "100%|██████████| 10/10 [00:02<00:00,  4.87it/s]\n",
      "100%|██████████| 10/10 [00:01<00:00,  5.09it/s]\n",
      "100%|██████████| 10/10 [00:02<00:00,  4.70it/s]\n",
      "100%|██████████| 10/10 [00:02<00:00,  4.96it/s]\n",
      "100%|██████████| 10/10 [00:01<00:00,  5.02it/s]\n",
      "100%|██████████| 10/10 [00:02<00:00,  4.83it/s]\n",
      "100%|██████████| 10/10 [00:02<00:00,  4.98it/s]\n",
      "100%|██████████| 10/10 [00:02<00:00,  4.91it/s]\n",
      "100%|██████████| 10/10 [00:02<00:00,  4.97it/s]\n",
      "100%|██████████| 10/10 [00:01<00:00,  5.02it/s]\n",
      "100%|██████████| 10/10 [00:02<00:00,  4.89it/s]\n",
      "100%|██████████| 10/10 [00:02<00:00,  4.72it/s]\n",
      "100%|██████████| 10/10 [00:01<00:00,  5.05it/s]\n",
      "100%|██████████| 10/10 [00:01<00:00,  5.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to checkpoints/dchpw62o/final_sae_group_tiny-stories-1M_blocks.1.mlp.hook_post_4096.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f1e8cf1010b46f9b0fb5d3cd23e5bd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.038 MB of 24.149 MB uploaded\\r'), FloatProgress(value=0.0015833850643276347, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>details/current_learning_rate_coeff0.0002</td><td>▁▂▃▄▅▅▆▇████████████████████████████████</td></tr><tr><td>details/current_learning_rate_coeff0.0003</td><td>▁▂▃▄▅▅▆▇████████████████████████████████</td></tr><tr><td>details/current_learning_rate_coeff0.0006</td><td>▁▂▃▄▅▅▆▇████████████████████████████████</td></tr><tr><td>details/n_training_tokens</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>losses/ghost_grad_loss_coeff0.0002</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>losses/ghost_grad_loss_coeff0.0003</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>losses/ghost_grad_loss_coeff0.0006</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>losses/l1_loss_coeff0.0002</td><td>██▇▆▄▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>losses/l1_loss_coeff0.0003</td><td>██▇▅▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>losses/l1_loss_coeff0.0006</td><td>█▇▆▄▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>losses/mse_loss_coeff0.0002</td><td>█▅▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>losses/mse_loss_coeff0.0003</td><td>█▅▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>losses/mse_loss_coeff0.0006</td><td>█▆▃▃▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>losses/overall_loss_coeff0.0002</td><td>█▆▅▄▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>losses/overall_loss_coeff0.0003</td><td>█▇▅▄▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>losses/overall_loss_coeff0.0006</td><td>█▇▅▄▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>metrics/CE_loss_score_coeff0.0002</td><td>▁▄▅▅▆▆▇▇▇▇▇█████████████</td></tr><tr><td>metrics/CE_loss_score_coeff0.0003</td><td>▁▄▄▄▅▅▆▆▇▇▇▇▇▇▇█████████</td></tr><tr><td>metrics/CE_loss_score_coeff0.0006</td><td>▁▄▂▂▃▄▄▅▅▅▆▆▆▇▇▇▇▇██████</td></tr><tr><td>metrics/ce_loss_with_ablation_coeff0.0002</td><td>▂▃▂▅▃▃▄▇▆▇█▄▅▃▄▇▆▂▁▄▄▄█▂</td></tr><tr><td>metrics/ce_loss_with_ablation_coeff0.0003</td><td>▆▄▁▅█▅▅█▄▄▆▅▇█▇▆▄▇▅▅▅▇▆▅</td></tr><tr><td>metrics/ce_loss_with_ablation_coeff0.0006</td><td>▄▆▅▆▃▄▅▆▇▆▆█▄▅▅▅▆▆▆▅▅▅▁▄</td></tr><tr><td>metrics/ce_loss_with_sae_coeff0.0002</td><td>█▅▄▄▃▃▂▂▂▂▂▁▁▁▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>metrics/ce_loss_with_sae_coeff0.0003</td><td>█▅▅▅▄▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>metrics/ce_loss_with_sae_coeff0.0006</td><td>█▆▇▇▆▅▅▄▄▄▃▃▃▂▂▂▂▂▂▂▂▁▁▁</td></tr><tr><td>metrics/ce_loss_without_sae_coeff0.0002</td><td>▃▅▄▃▃▃▅▅▃▅█▂▄▁█▇▄▄▁▂▃▄▅▇</td></tr><tr><td>metrics/ce_loss_without_sae_coeff0.0003</td><td>▃▄▆▅▅▅▆▆▄▁▄▄▅▅█▆▇▃▅▅▆▇▅▆</td></tr><tr><td>metrics/ce_loss_without_sae_coeff0.0006</td><td>▃▃▄▃▁▂▃▃▄█▃▂▅▅▅▃▆▅▃▄▅▄▂▂</td></tr><tr><td>metrics/explained_variance_coeff0.0002</td><td>▁▄▆▇▇▇▇█████████████████████████████████</td></tr><tr><td>metrics/explained_variance_coeff0.0003</td><td>▁▄▆▇▇▇▇▇▇▇██████████████████████████████</td></tr><tr><td>metrics/explained_variance_coeff0.0006</td><td>▁▃▆▆▆▆▆▆▇▇▇▇▇▇▇█████████████████████████</td></tr><tr><td>metrics/explained_variance_std_coeff0.0002</td><td>▇██▆▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>metrics/explained_variance_std_coeff0.0003</td><td>▆██▆▅▄▄▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>metrics/explained_variance_std_coeff0.0006</td><td>▅▇█▆▄▄▅▆▆▆▅▅▅▅▄▄▄▄▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>metrics/l0_coeff0.0002</td><td>███▇▆▅▄▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>metrics/l0_coeff0.0003</td><td>███▇▆▄▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>metrics/l0_coeff0.0006</td><td>██▇▇▅▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>metrics/l2_norm_coeff0.0002</td><td>▂▂▁▁▂▄▄▅▅▆▆▆▇▇▇▇▇▇█▇████</td></tr><tr><td>metrics/l2_norm_coeff0.0003</td><td>▄▃▁▁▃▄▄▅▆▆▆▆▇▇▇▇▇▇▇█████</td></tr><tr><td>metrics/l2_norm_coeff0.0006</td><td>▆▄▁▁▂▃▄▅▅▆▆▆▇▇▇▇▇▇██████</td></tr><tr><td>metrics/l2_ratio_coeff0.0002</td><td>▂▂▁▁▂▄▄▅▅▆▆▆▇▇▇▇▇▇██████</td></tr><tr><td>metrics/l2_ratio_coeff0.0003</td><td>▄▃▁▁▃▄▅▅▆▆▆▆▇▇▇▇▇▇██████</td></tr><tr><td>metrics/l2_ratio_coeff0.0006</td><td>▆▄▁▁▂▃▄▅▅▆▆▆▇▇▇▇▇███████</td></tr><tr><td>metrics/mean_log10_feature_sparsity_coeff0.0002</td><td>█▄▂▁</td></tr><tr><td>metrics/mean_log10_feature_sparsity_coeff0.0003</td><td>█▄▂▁</td></tr><tr><td>metrics/mean_log10_feature_sparsity_coeff0.0006</td><td>█▄▂▁</td></tr><tr><td>sparsity/below_1e-5_coeff0.0002</td><td>▁▁▁█</td></tr><tr><td>sparsity/below_1e-5_coeff0.0003</td><td>▁▁▂█</td></tr><tr><td>sparsity/below_1e-5_coeff0.0006</td><td>▁▁▁█</td></tr><tr><td>sparsity/below_1e-6_coeff0.0002</td><td>▁▁▁▁</td></tr><tr><td>sparsity/below_1e-6_coeff0.0003</td><td>▁▁▁▁</td></tr><tr><td>sparsity/below_1e-6_coeff0.0006</td><td>▁▁▁▁</td></tr><tr><td>sparsity/dead_features_coeff0.0002</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>sparsity/dead_features_coeff0.0003</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▅▁▁▁▁▁██▅▁▁▁▁▅</td></tr><tr><td>sparsity/dead_features_coeff0.0006</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▁▁▁▁▁▁▄▅█▆▆▇</td></tr><tr><td>sparsity/mean_passes_since_fired_coeff0.0002</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▃▃▃▄▄▅▅▄▇▆▅▇▇██</td></tr><tr><td>sparsity/mean_passes_since_fired_coeff0.0003</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▃▃▃▃▄▄▄▅▆▆▇▆▆▇██</td></tr><tr><td>sparsity/mean_passes_since_fired_coeff0.0006</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▅▅▆▆▆▇██</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>details/current_learning_rate_coeff0.0002</td><td>0.0001</td></tr><tr><td>details/current_learning_rate_coeff0.0003</td><td>0.0001</td></tr><tr><td>details/current_learning_rate_coeff0.0006</td><td>0.0001</td></tr><tr><td>details/n_training_tokens</td><td>9994240</td></tr><tr><td>losses/ghost_grad_loss_coeff0.0002</td><td>0.0</td></tr><tr><td>losses/ghost_grad_loss_coeff0.0003</td><td>0.0</td></tr><tr><td>losses/ghost_grad_loss_coeff0.0006</td><td>0.0</td></tr><tr><td>losses/l1_loss_coeff0.0002</td><td>3.09463</td></tr><tr><td>losses/l1_loss_coeff0.0003</td><td>2.58137</td></tr><tr><td>losses/l1_loss_coeff0.0006</td><td>1.83998</td></tr><tr><td>losses/mse_loss_coeff0.0002</td><td>0.00019</td></tr><tr><td>losses/mse_loss_coeff0.0003</td><td>0.00026</td></tr><tr><td>losses/mse_loss_coeff0.0006</td><td>0.00053</td></tr><tr><td>losses/overall_loss_coeff0.0002</td><td>0.00081</td></tr><tr><td>losses/overall_loss_coeff0.0003</td><td>0.00104</td></tr><tr><td>losses/overall_loss_coeff0.0006</td><td>0.00163</td></tr><tr><td>metrics/CE_loss_score_coeff0.0002</td><td>0.92625</td></tr><tr><td>metrics/CE_loss_score_coeff0.0003</td><td>0.88257</td></tr><tr><td>metrics/CE_loss_score_coeff0.0006</td><td>0.74837</td></tr><tr><td>metrics/ce_loss_with_ablation_coeff0.0002</td><td>7.74356</td></tr><tr><td>metrics/ce_loss_with_ablation_coeff0.0003</td><td>7.77858</td></tr><tr><td>metrics/ce_loss_with_ablation_coeff0.0006</td><td>7.76302</td></tr><tr><td>metrics/ce_loss_with_sae_coeff0.0002</td><td>3.10251</td></tr><tr><td>metrics/ce_loss_with_sae_coeff0.0003</td><td>3.29611</td></tr><tr><td>metrics/ce_loss_with_sae_coeff0.0006</td><td>3.90444</td></tr><tr><td>metrics/ce_loss_without_sae_coeff0.0002</td><td>2.73316</td></tr><tr><td>metrics/ce_loss_without_sae_coeff0.0003</td><td>2.70042</td></tr><tr><td>metrics/ce_loss_without_sae_coeff0.0006</td><td>2.60843</td></tr><tr><td>metrics/explained_variance_coeff0.0002</td><td>0.96407</td></tr><tr><td>metrics/explained_variance_coeff0.0003</td><td>0.9494</td></tr><tr><td>metrics/explained_variance_coeff0.0006</td><td>0.89856</td></tr><tr><td>metrics/explained_variance_std_coeff0.0002</td><td>0.03024</td></tr><tr><td>metrics/explained_variance_std_coeff0.0003</td><td>0.04194</td></tr><tr><td>metrics/explained_variance_std_coeff0.0006</td><td>0.08105</td></tr><tr><td>metrics/l0_coeff0.0002</td><td>119.72095</td></tr><tr><td>metrics/l0_coeff0.0003</td><td>77.5647</td></tr><tr><td>metrics/l0_coeff0.0006</td><td>33.18384</td></tr><tr><td>metrics/l2_norm_coeff0.0002</td><td>1.39449</td></tr><tr><td>metrics/l2_norm_coeff0.0003</td><td>1.36607</td></tr><tr><td>metrics/l2_norm_coeff0.0006</td><td>1.28269</td></tr><tr><td>metrics/l2_ratio_coeff0.0002</td><td>0.93444</td></tr><tr><td>metrics/l2_ratio_coeff0.0003</td><td>0.91607</td></tr><tr><td>metrics/l2_ratio_coeff0.0006</td><td>0.86204</td></tr><tr><td>metrics/mean_log10_feature_sparsity_coeff0.0002</td><td>-1.81471</td></tr><tr><td>metrics/mean_log10_feature_sparsity_coeff0.0003</td><td>-2.2457</td></tr><tr><td>metrics/mean_log10_feature_sparsity_coeff0.0006</td><td>-3.13876</td></tr><tr><td>sparsity/below_1e-5_coeff0.0002</td><td>1</td></tr><tr><td>sparsity/below_1e-5_coeff0.0003</td><td>6</td></tr><tr><td>sparsity/below_1e-5_coeff0.0006</td><td>27</td></tr><tr><td>sparsity/below_1e-6_coeff0.0002</td><td>0</td></tr><tr><td>sparsity/below_1e-6_coeff0.0003</td><td>0</td></tr><tr><td>sparsity/below_1e-6_coeff0.0006</td><td>0</td></tr><tr><td>sparsity/dead_features_coeff0.0002</td><td>0</td></tr><tr><td>sparsity/dead_features_coeff0.0003</td><td>1</td></tr><tr><td>sparsity/dead_features_coeff0.0006</td><td>7</td></tr><tr><td>sparsity/mean_passes_since_fired_coeff0.0002</td><td>0.23755</td></tr><tr><td>sparsity/mean_passes_since_fired_coeff0.0003</td><td>1.10229</td></tr><tr><td>sparsity/mean_passes_since_fired_coeff0.0006</td><td>5.02368</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">4096-L1-[0.0002, 0.0003, 0.0006]-LR-0.0001-Tokens-1.000e+07</strong> at: <a href='https://wandb.ai/jbloom/mats_sae_training_language_benchmark_tests/runs/voo84o5b' target=\"_blank\">https://wandb.ai/jbloom/mats_sae_training_language_benchmark_tests/runs/voo84o5b</a><br/>Synced 7 W&B file(s), 0 media file(s), 2 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240325_154839-voo84o5b/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "from sae_training.config import LanguageModelSAERunnerConfig\n",
    "from sae_training.lm_runner import language_model_sae_runner\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "if device == \"cpu\" and torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "cfg = LanguageModelSAERunnerConfig(\n",
    "    # Data Generating Function (Model + Training Distibuion)\n",
    "    model_name=\"tiny-stories-1M\",\n",
    "    hook_point=\"blocks.1.mlp.hook_post\",\n",
    "    hook_point_layer=1,\n",
    "    d_in=256,\n",
    "    # dataset_path=\"roneneldan/TinyStories\",\n",
    "    # is_dataset_tokenized=False,\n",
    "    # Dan at Apollo pretokenized this dataset for us which will speed up training.\n",
    "    dataset_path=\"apollo-research/roneneldan-TinyStories-tokenizer-gpt2\",\n",
    "    is_dataset_tokenized=True,\n",
    "    # SAE Parameters\n",
    "    expansion_factor=16,\n",
    "    # Training Parameters\n",
    "    lr=1e-4,\n",
    "    l1_coefficient=[2e-4,3e-4,6e-4],\n",
    "    train_batch_size=4096,\n",
    "    context_size=128,\n",
    "    # Activation Store Parameters\n",
    "    n_batches_in_buffer=128,\n",
    "    total_training_tokens=1_000_000 * 100,\n",
    "    store_batch_size=32,\n",
    "    feature_sampling_window=500,  # So we see the histograms. \n",
    "    dead_feature_window=250,\n",
    "    # WANDB\n",
    "    log_to_wandb=True,\n",
    "    wandb_project=\"mats_sae_training_language_benchmark_tests\",\n",
    "    wandb_log_frequency=10,\n",
    "    # Misc\n",
    "    device=device,\n",
    "    seed=42,\n",
    "    n_checkpoints=0,\n",
    "    checkpoint_path=\"checkpoints\",\n",
    "    dtype=torch.float32,\n",
    ")\n",
    "\n",
    "sparse_autoencoder = language_model_sae_runner(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toy Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sae_training.toy_model_runner import SAEToyModelRunnerConfig, toy_model_sae_runner\n",
    "\n",
    "\n",
    "cfg = SAEToyModelRunnerConfig(\n",
    "    # Model Details\n",
    "    n_features=200,\n",
    "    n_hidden=5,\n",
    "    n_correlated_pairs=0,\n",
    "    n_anticorrelated_pairs=0,\n",
    "    feature_probability=0.025,\n",
    "    model_training_steps=10_000,\n",
    "    # SAE Parameters\n",
    "    d_sae=240,\n",
    "    l1_coefficient=0.001,\n",
    "    # SAE Train Config\n",
    "    train_batch_size=1028,\n",
    "    feature_sampling_window=3_000,\n",
    "    dead_feature_window=1_000,\n",
    "    feature_reinit_scale=0.5,\n",
    "    total_training_tokens=4096 * 300,\n",
    "    # Other parameters\n",
    "    log_to_wandb=True,\n",
    "    wandb_project=\"sae-training-test\",\n",
    "    wandb_log_frequency=5,\n",
    "    device=\"mps\",\n",
    ")\n",
    "\n",
    "trained_sae = toy_model_sae_runner(cfg)\n",
    "\n",
    "assert trained_sae is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run caching of activations to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"WANDB__SERVICE_WAIT\"] = \"300\"\n",
    "\n",
    "from sae_training.config import CacheActivationsRunnerConfig\n",
    "from sae_training.cache_activations_runner import cache_activations_runner\n",
    "\n",
    "cfg = CacheActivationsRunnerConfig(\n",
    "    # Data Generating Function (Model + Training Distibuion)\n",
    "    model_name=\"gpt2-small\",\n",
    "    hook_point=\"blocks.10.attn.hook_q\",\n",
    "    hook_point_layer=10,\n",
    "    hook_point_head_index=7,\n",
    "    d_in=64,\n",
    "    dataset_path=\"Skylion007/openwebtext\",\n",
    "    is_dataset_tokenized=False,\n",
    "    cached_activations_path=\"../activations/\",\n",
    "    # Activation Store Parameters\n",
    "    n_batches_in_buffer=16,\n",
    "    total_training_tokens=500_000_000,\n",
    "    store_batch_size=32,\n",
    "    # Activation caching shuffle parameters\n",
    "    n_shuffles_final=16,\n",
    "    # Misc\n",
    "    device=\"mps\",\n",
    "    seed=42,\n",
    "    dtype=torch.float32,\n",
    ")\n",
    "\n",
    "cache_activations_runner(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train an SAE using the cached activations stored on disk\n",
    "Pass `use_cached_activations=True` into the config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"WANDB__SERVICE_WAIT\"] = \"300\"\n",
    "from sae_training.config import LanguageModelSAERunnerConfig\n",
    "from sae_training.lm_runner import language_model_sae_runner\n",
    "\n",
    "cfg = LanguageModelSAERunnerConfig(\n",
    "    # Data Generating Function (Model + Training Distibuion)\n",
    "    model_name=\"gpt2-small\",\n",
    "    hook_point=\"blocks.10.hook_resid_pre\",\n",
    "    hook_point_layer=11,\n",
    "    d_in=768,\n",
    "    dataset_path=\"Skylion007/openwebtext\",\n",
    "    is_dataset_tokenized=False,\n",
    "    use_cached_activations=True,\n",
    "    # SAE Parameters\n",
    "    expansion_factor=64,  # determines the dimension of the SAE.\n",
    "    # Training Parameters\n",
    "    lr=1e-5,\n",
    "    l1_coefficient=5e-4,\n",
    "    lr_scheduler_name=None,\n",
    "    train_batch_size=4096,\n",
    "    context_size=128,\n",
    "    # Activation Store Parameters\n",
    "    n_batches_in_buffer=64,\n",
    "    total_training_tokens=200_000,\n",
    "    store_batch_size=32,\n",
    "    # Resampling protocol\n",
    "    feature_sampling_method=\"l2\",\n",
    "    feature_sampling_window=1000,\n",
    "    feature_reinit_scale=0.2,\n",
    "    dead_feature_window=5000,\n",
    "    dead_feature_threshold=1e-7,\n",
    "    # WANDB\n",
    "    log_to_wandb=True,\n",
    "    wandb_project=\"mats_sae_training_gpt2_small\",\n",
    "    wandb_entity=None,\n",
    "    wandb_log_frequency=50,\n",
    "    # Misc\n",
    "    device=\"mps\",\n",
    "    seed=42,\n",
    "    n_checkpoints=5,\n",
    "    checkpoint_path=\"checkpoints\",\n",
    "    dtype=torch.float32,\n",
    ")\n",
    "\n",
    "sparse_autoencoder = language_model_sae_runner(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mats_sae_training",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
