{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Notebook with Example Config for Different Models / Hooks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: mps\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import os\n",
        "import sys\n",
        "\n",
        "sys.path.append(\"..\")\n",
        "\n",
        "from sae_lens.training.config import LanguageModelSAERunnerConfig\n",
        "from sae_lens.training.lm_runner import language_model_sae_runner\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "elif torch.backends.mps.is_available():\n",
        "    device = \"mps\"\n",
        "else:\n",
        "    device = \"cpu\"\n",
        "\n",
        "print(\"Using device:\", device)\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Gelu-2L\n",
        "\n",
        "An example of a toy language model we're able to train on."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### MLP Out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cfg = LanguageModelSAERunnerConfig(\n",
        "    # Data Generating Function (Model + Training Distibuion)\n",
        "    model_name=\"gelu-2l\",\n",
        "    hook_point=\"blocks.0.hook_mlp_out\",\n",
        "    hook_point_layer=0,\n",
        "    d_in=512,\n",
        "    dataset_path=\"NeelNanda/c4-tokenized-2b\",\n",
        "    is_dataset_tokenized=True,\n",
        "    # SAE Parameters\n",
        "    expansion_factor=[16, 32, 64],\n",
        "    b_dec_init_method=\"geometric_median\",  # geometric median is better but slower to get started\n",
        "    # Training Parameters\n",
        "    lr=0.0012,\n",
        "    lr_scheduler_name=\"constantwithwarmup\",\n",
        "    l1_coefficient=0.00016,\n",
        "    train_batch_size=4096,\n",
        "    context_size=128,\n",
        "    # Activation Store Parameters\n",
        "    n_batches_in_buffer=128,\n",
        "    total_training_tokens=1_000_000 * 100,\n",
        "    store_batch_size=32,\n",
        "    # Resampling protocol\n",
        "    use_ghost_grads=True,\n",
        "    feature_sampling_window=5000,\n",
        "    dead_feature_window=5000,\n",
        "    dead_feature_threshold=1e-4,\n",
        "    # WANDB\n",
        "    log_to_wandb=True,\n",
        "    wandb_project=\"mats_sae_training_language_models_gelu_2l_test\",\n",
        "    wandb_log_frequency=10,\n",
        "    # Misc\n",
        "    device=device,\n",
        "    seed=42,\n",
        "    n_checkpoints=0,\n",
        "    checkpoint_path=\"checkpoints\",\n",
        "    dtype=torch.float32,\n",
        ")\n",
        "\n",
        "\n",
        "sparse_autoencoder = language_model_sae_runner(cfg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## GPT2 - Small"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Residual Stream"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sae_lens.training.config import LanguageModelSAERunnerConfig\n",
        "from sae_lens.training.lm_runner import language_model_sae_runner\n",
        "\n",
        "layer = 3\n",
        "cfg = LanguageModelSAERunnerConfig(\n",
        "    # Data Generating Function (Model + Training Distibuion)\n",
        "    model_name=\"gpt2-small\",\n",
        "    hook_point=f\"blocks.{layer}.hook_resid_pre\",\n",
        "    hook_point_layer=layer,\n",
        "    d_in=768,\n",
        "    dataset_path=\"Skylion007/openwebtext\",\n",
        "    is_dataset_tokenized=False,\n",
        "    # SAE Parameters\n",
        "    expansion_factor=32,  # determines the dimension of the SAE.\n",
        "    b_dec_init_method=\"mean\",  # geometric median is better but slower to get started\n",
        "    # Training Parameters\n",
        "    lr=0.0004,\n",
        "    l1_coefficient=0.00008,\n",
        "    lr_scheduler_name=\"constantwithwarmup\",\n",
        "    train_batch_size=4096,\n",
        "    context_size=128,\n",
        "    lr_warm_up_steps=5000,\n",
        "    # Activation Store Parameters\n",
        "    n_batches_in_buffer=128,\n",
        "    total_training_tokens=1_000_000 * 300,  # 200M tokens seems doable overnight.\n",
        "    store_batch_size=32,\n",
        "    # Resampling protocol\n",
        "    use_ghost_grads=True,\n",
        "    feature_sampling_window=2500,\n",
        "    dead_feature_window=5000,\n",
        "    dead_feature_threshold=1e-8,\n",
        "    # WANDB\n",
        "    log_to_wandb=True,\n",
        "    wandb_project=\"mats_sae_training_language_models_resid_pre_test\",\n",
        "    wandb_entity=None,\n",
        "    wandb_log_frequency=100,\n",
        "    # Misc\n",
        "    device=\"cuda\",\n",
        "    seed=42,\n",
        "    n_checkpoints=10,\n",
        "    checkpoint_path=\"checkpoints\",\n",
        "    dtype=torch.float32,\n",
        ")\n",
        "\n",
        "sparse_autoencoder = language_model_sae_runner(cfg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Pythia 70-M"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import os\n",
        "import sys\n",
        "\n",
        "sys.path.append(\"..\")\n",
        "from sae_lens.training.config import LanguageModelSAERunnerConfig\n",
        "from sae_lens.training.lm_runner import language_model_sae_runner\n",
        "\n",
        "import cProfile\n",
        "\n",
        "\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "cfg = LanguageModelSAERunnerConfig(\n",
        "    # Data Generating Function (Model + Training Distibuion)\n",
        "    model_name=\"pythia-70m-deduped\",\n",
        "    hook_point=\"blocks.0.hook_mlp_out\",\n",
        "    hook_point_layer=0,\n",
        "    d_in=512,\n",
        "    dataset_path=\"EleutherAI/the_pile_deduplicated\",\n",
        "    is_dataset_tokenized=False,\n",
        "    # SAE Parameters\n",
        "    expansion_factor=64,\n",
        "    # Training Parameters\n",
        "    lr=3e-4,\n",
        "    l1_coefficient=4e-5,\n",
        "    train_batch_size=8192,\n",
        "    context_size=128,\n",
        "    lr_scheduler_name=\"constantwithwarmup\",\n",
        "    lr_warm_up_steps=10_000,\n",
        "    # Activation Store Parameters\n",
        "    n_batches_in_buffer=64,\n",
        "    total_training_tokens=1_000_000 * 800,\n",
        "    store_batch_size=32,\n",
        "    # Resampling protocol\n",
        "    feature_sampling_window=2000,  # Doesn't currently matter.\n",
        "    dead_feature_window=40000,\n",
        "    dead_feature_threshold=1e-8,\n",
        "    # WANDB\n",
        "    log_to_wandb=True,\n",
        "    wandb_project=\"mats_sae_training_language_benchmark_tests\",\n",
        "    wandb_entity=None,\n",
        "    wandb_log_frequency=20,\n",
        "    # Misc\n",
        "    device=\"cuda\",\n",
        "    seed=42,\n",
        "    n_checkpoints=0,\n",
        "    checkpoint_path=\"checkpoints\",\n",
        "    dtype=torch.float32,\n",
        ")\n",
        "\n",
        "\n",
        "sparse_autoencoder = language_model_sae_runner(cfg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Pythia 70M Hook Q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import os\n",
        "import sys\n",
        "\n",
        "sys.path.append(\"../\")\n",
        "\n",
        "from sae_lens.training.config import LanguageModelSAERunnerConfig\n",
        "from sae_lens.training.lm_runner import language_model_sae_runner\n",
        "\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "cfg = LanguageModelSAERunnerConfig(\n",
        "    # Data Generating Function (Model + Training Distibuion)\n",
        "    model_name=\"pythia-70m-deduped\",\n",
        "    hook_point=\"blocks.2.attn.hook_q\",\n",
        "    hook_point_layer=2,\n",
        "    hook_point_head_index=7,\n",
        "    d_in=64,\n",
        "    dataset_path=\"EleutherAI/the_pile_deduplicated\",\n",
        "    is_dataset_tokenized=False,\n",
        "    # SAE Parameters\n",
        "    expansion_factor=16,\n",
        "    # Training Parameters\n",
        "    lr=0.0012,\n",
        "    l1_coefficient=0.003,\n",
        "    lr_scheduler_name=\"constantwithwarmup\",\n",
        "    lr_warm_up_steps=1000,  # about 4 million tokens.\n",
        "    train_batch_size=4096,\n",
        "    context_size=128,\n",
        "    # Activation Store Parameters\n",
        "    n_batches_in_buffer=128,\n",
        "    total_training_tokens=1_000_000 * 1500,\n",
        "    store_batch_size=32,\n",
        "    # Resampling protocol\n",
        "    feature_sampling_method=\"anthropic\",\n",
        "    feature_sampling_window=1000,  # doesn't do anything currently.\n",
        "    feature_reinit_scale=0.2,\n",
        "    resample_batches=8,\n",
        "    dead_feature_window=60000,\n",
        "    dead_feature_threshold=1e-5,\n",
        "    # WANDB\n",
        "    log_to_wandb=True,\n",
        "    wandb_project=\"mats_sae_training_pythia_70M_hook_q_L2H7\",\n",
        "    wandb_entity=None,\n",
        "    wandb_log_frequency=100,\n",
        "    # Misc\n",
        "    device=\"mps\",\n",
        "    seed=42,\n",
        "    n_checkpoints=15,\n",
        "    checkpoint_path=\"checkpoints\",\n",
        "    dtype=torch.float32,\n",
        ")\n",
        "\n",
        "sparse_autoencoder = language_model_sae_runner(cfg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Tiny Stories"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## MLP Out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import os\n",
        "\n",
        "from sae_lens.training.config import LanguageModelSAERunnerConfig\n",
        "from sae_lens.training.lm_runner import language_model_sae_runner\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "if device == \"cpu\" and torch.backends.mps.is_available():\n",
        "    device = \"mps\"\n",
        "\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "cfg = LanguageModelSAERunnerConfig(\n",
        "    # Data Generating Function (Model + Training Distibuion)\n",
        "    model_name=\"tiny-stories-1M\",\n",
        "    hook_point=\"blocks.1.mlp.hook_post\",\n",
        "    hook_point_layer=1,\n",
        "    d_in=256,\n",
        "    # dataset_path=\"roneneldan/TinyStories\",\n",
        "    # is_dataset_tokenized=False,\n",
        "    # Dan at Apollo pretokenized this dataset for us which will speed up training.\n",
        "    dataset_path=\"apollo-research/roneneldan-TinyStories-tokenizer-gpt2\",\n",
        "    is_dataset_tokenized=True,\n",
        "    # SAE Parameters\n",
        "    expansion_factor=16,\n",
        "    # Training Parameters\n",
        "    lr=1e-4,\n",
        "    lp_norm=1.0,\n",
        "    l1_coefficient=2e-4,\n",
        "    train_batch_size=4096,\n",
        "    context_size=128,\n",
        "    # Activation Store Parameters\n",
        "    n_batches_in_buffer=128,\n",
        "    total_training_tokens=1_000_000 * 20,\n",
        "    store_batch_size=32,\n",
        "    feature_sampling_window=500,  # So we see the histograms.\n",
        "    dead_feature_window=250,\n",
        "    # WANDB\n",
        "    log_to_wandb=True,\n",
        "    wandb_project=\"mats_sae_training_language_benchmark_tests\",\n",
        "    wandb_log_frequency=10,\n",
        "    # Misc\n",
        "    device=device,\n",
        "    seed=42,\n",
        "    n_checkpoints=0,\n",
        "    checkpoint_path=\"checkpoints\",\n",
        "    dtype=torch.float32,\n",
        ")\n",
        "\n",
        "sparse_autoencoder = language_model_sae_runner(cfg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Hook Z\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Run name: 1024-L1-0.0002-LR-0.0001-Tokens-2.000e+07\n",
            "n_tokens_per_buffer (millions): 0.524288\n",
            "Lower bound: n_contexts_per_buffer (millions): 0.004096\n",
            "Total training steps: 4882\n",
            "Total wandb updates: 488\n",
            "n_tokens_per_feature_sampling_window (millions): 262.144\n",
            "n_tokens_per_dead_feature_window (millions): 131.072\n",
            "We will reset the sparsity calculation 9 times.\n",
            "Number tokens in sparsity calculation window: 2.05e+06\n",
            "Loaded pretrained model tiny-stories-1M into HookedTransformer\n",
            "Moving model to device:  mps\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Run name: 1024-L1-0.0002-LR-0.0001-Tokens-2.000e+07\n",
            "n_tokens_per_buffer (millions): 0.524288\n",
            "Lower bound: n_contexts_per_buffer (millions): 0.004096\n",
            "Total training steps: 4882\n",
            "Total wandb updates: 488\n",
            "n_tokens_per_feature_sampling_window (millions): 262.144\n",
            "n_tokens_per_dead_feature_window (millions): 131.072\n",
            "We will reset the sparsity calculation 9 times.\n",
            "Number tokens in sparsity calculation window: 2.05e+06\n",
            "Run name: 1024-L1-0.0002-LR-0.0001-Tokens-2.000e+07\n",
            "n_tokens_per_buffer (millions): 0.524288\n",
            "Lower bound: n_contexts_per_buffer (millions): 0.004096\n",
            "Total training steps: 4882\n",
            "Total wandb updates: 488\n",
            "n_tokens_per_feature_sampling_window (millions): 262.144\n",
            "n_tokens_per_dead_feature_window (millions): 131.072\n",
            "We will reset the sparsity calculation 9 times.\n",
            "Number tokens in sparsity calculation window: 2.05e+06\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjbloom\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "wandb version 0.16.5 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.16.3"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/josephbloom/GithubRepositories/mats_sae_training/scripts/wandb/run-20240326_191703-ec6k6v87</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jbloom/mats_sae_training_language_benchmark_tests/runs/ec6k6v87' target=\"_blank\">1024-L1-0.0002-LR-0.0001-Tokens-2.000e+07</a></strong> to <a href='https://wandb.ai/jbloom/mats_sae_training_language_benchmark_tests' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/jbloom/mats_sae_training_language_benchmark_tests' target=\"_blank\">https://wandb.ai/jbloom/mats_sae_training_language_benchmark_tests</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/jbloom/mats_sae_training_language_benchmark_tests/runs/ec6k6v87' target=\"_blank\">https://wandb.ai/jbloom/mats_sae_training_language_benchmark_tests/runs/ec6k6v87</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Objective value: 116883.7422:  10%|█         | 10/100 [00:00<00:00, 128.72it/s]\n",
            "/Users/josephbloom/GithubRepositories/mats_sae_training/sae_training/sparse_autoencoder.py:161: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  out = torch.tensor(origin, dtype=self.dtype, device=self.device)\n",
            "100%|██████████| 10/10 [00:02<00:00,  4.93it/s] 405504/20000000 [00:14<08:53, 36739.57it/s]\n",
            "100%|██████████| 10/10 [00:01<00:00,  5.01it/s]| 811008/20000000 [00:31<18:45, 17042.47it/s] \n",
            "100%|██████████| 10/10 [00:01<00:00,  5.04it/s]| 1224704/20000000 [00:47<10:43, 29194.89it/s] \n",
            "100%|██████████| 10/10 [00:02<00:00,  4.98it/s]| 1634304/20000000 [01:05<08:10, 37468.33it/s]\n",
            "100%|██████████| 10/10 [00:02<00:00,  4.64it/s]| 2039808/20000000 [01:20<07:36, 39322.02it/s]\n",
            "100%|██████████| 10/10 [00:01<00:00,  5.08it/s]| 2453504/20000000 [01:37<07:55, 36873.53it/s]\n",
            "100%|██████████| 10/10 [00:01<00:00,  5.04it/s]| 2863104/20000000 [01:52<07:16, 39292.24it/s]\n",
            "100%|██████████| 10/10 [00:01<00:00,  5.01it/s]| 3272704/20000000 [02:09<06:52, 40537.06it/s] \n",
            "100%|██████████| 10/10 [00:02<00:00,  4.90it/s]| 3678208/20000000 [02:26<27:40, 9829.56it/s] \n",
            "100%|██████████| 10/10 [00:02<00:00,  4.90it/s]| 4087808/20000000 [02:41<06:11, 42798.13it/s]\n",
            "100%|██████████| 10/10 [00:02<00:00,  4.50it/s] | 4497408/20000000 [03:01<08:53, 29055.95it/s] \n",
            "100%|██████████| 10/10 [00:02<00:00,  4.51it/s] | 4911104/20000000 [03:16<06:55, 36330.89it/s]\n",
            "100%|██████████| 10/10 [00:02<00:00,  4.57it/s] | 5316608/20000000 [03:34<06:31, 37461.30it/s]\n",
            "100%|██████████| 10/10 [00:02<00:00,  4.87it/s] | 5726208/20000000 [03:50<05:45, 41309.20it/s]\n",
            "100%|██████████| 10/10 [00:02<00:00,  4.51it/s] | 6139904/20000000 [04:07<06:03, 38122.10it/s]\n",
            "100%|██████████| 10/10 [00:02<00:00,  4.90it/s] | 6549504/20000000 [04:24<05:43, 39198.19it/s]\n",
            "100%|██████████| 10/10 [00:02<00:00,  4.91it/s] | 6955008/20000000 [04:43<05:01, 43328.38it/s]\n",
            "100%|██████████| 10/10 [00:02<00:00,  4.84it/s] | 7368704/20000000 [05:00<12:14, 17200.22it/s]\n",
            "100%|██████████| 10/10 [00:01<00:00,  5.04it/s] | 7778304/20000000 [05:14<04:44, 43005.09it/s]\n",
            "100%|██████████| 10/10 [00:02<00:00,  4.78it/s] | 8183808/20000000 [05:32<06:31, 30153.11it/s]\n",
            "100%|██████████| 10/10 [00:02<00:00,  4.80it/s] | 8597504/20000000 [05:47<04:22, 43375.86it/s]\n",
            "100%|██████████| 10/10 [00:02<00:00,  5.00it/s] | 9007104/20000000 [06:09<05:16, 34784.52it/s]\n",
            "100%|██████████| 10/10 [00:02<00:00,  4.55it/s] | 9416704/20000000 [06:24<04:36, 38252.78it/s]\n",
            "100%|██████████| 10/10 [00:02<00:00,  4.75it/s] | 9822208/20000000 [06:42<03:58, 42593.01it/s]\n",
            "100%|██████████| 10/10 [00:02<00:00,  4.99it/s] | 10235904/20000000 [06:59<19:05, 8524.91it/s] \n",
            "100%|██████████| 10/10 [00:02<00:00,  4.98it/s] | 10645504/20000000 [07:14<03:30, 44384.65it/s]\n",
            "100%|██████████| 10/10 [00:02<00:00,  4.89it/s] | 11055104/20000000 [07:31<05:24, 27562.66it/s]\n",
            "100%|██████████| 10/10 [00:02<00:00,  4.83it/s] | 11464704/20000000 [07:45<03:26, 41316.56it/s]\n",
            "100%|██████████| 10/10 [00:02<00:00,  4.81it/s] | 11870208/20000000 [08:02<03:44, 36217.25it/s]\n",
            "100%|██████████| 10/10 [00:02<00:00,  4.89it/s] | 12279808/20000000 [08:16<02:52, 44715.52it/s]\n",
            "100%|██████████| 10/10 [00:02<00:00,  4.85it/s] | 12693504/20000000 [08:34<03:02, 40061.41it/s]\n",
            "100%|██████████| 10/10 [00:01<00:00,  5.02it/s] | 13103104/20000000 [08:48<02:38, 43563.35it/s]\n",
            "100%|██████████| 10/10 [00:04<00:00,  2.17it/s] | 13508608/20000000 [09:05<02:34, 41937.09it/s]\n",
            "100%|██████████| 10/10 [00:01<00:00,  5.03it/s] | 13922304/20000000 [09:24<05:07, 19779.09it/s]\n",
            "100%|██████████| 10/10 [00:01<00:00,  5.04it/s] | 14327808/20000000 [09:38<02:05, 45367.15it/s]\n",
            "100%|██████████| 10/10 [00:01<00:00,  5.09it/s] | 14741504/20000000 [09:54<02:49, 30943.53it/s]\n",
            "100%|██████████| 10/10 [00:01<00:00,  5.05it/s] | 15147008/20000000 [10:08<01:46, 45610.98it/s]\n",
            "100%|██████████| 10/10 [00:01<00:00,  5.06it/s] | 15556608/20000000 [10:24<01:49, 40440.85it/s]\n",
            "100%|██████████| 10/10 [00:01<00:00,  5.03it/s] | 15966208/20000000 [10:38<01:29, 45251.75it/s]\n",
            "100%|██████████| 10/10 [00:01<00:00,  5.09it/s] | 16379904/20000000 [10:55<01:22, 43941.70it/s]\n",
            "100%|██████████| 10/10 [00:01<00:00,  5.09it/s] | 16789504/20000000 [11:11<04:30, 11859.26it/s]\n",
            "100%|██████████| 10/10 [00:01<00:00,  5.04it/s] | 17195008/20000000 [11:25<01:02, 44607.68it/s]\n",
            "100%|██████████| 10/10 [00:02<00:00,  4.97it/s] | 17608704/20000000 [11:41<01:38, 24188.35it/s]\n",
            "100%|██████████| 10/10 [00:01<00:00,  5.00it/s] | 18018304/20000000 [11:54<00:42, 46425.69it/s]\n",
            "100%|██████████| 10/10 [00:01<00:00,  5.06it/s]▏| 18423808/20000000 [12:13<00:44, 35420.18it/s]\n",
            "100%|██████████| 10/10 [00:02<00:00,  4.97it/s]▍| 18837504/20000000 [12:27<00:26, 43914.73it/s]\n",
            "100%|██████████| 10/10 [00:01<00:00,  5.01it/s]▌| 19243008/20000000 [12:45<00:19, 38931.67it/s]\n",
            "100%|██████████| 10/10 [00:02<00:00,  4.95it/s]▊| 19656704/20000000 [12:59<00:07, 43804.93it/s]\n",
            "4883| MSE Loss 0.000 | L1 0.000: 100%|█████████▉| 19996672/20000000 [13:14<00:00, 37714.53it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved model to checkpoints/sf7u2imk/final_sae_group_tiny-stories-1M_blocks.1.attn.hook_z_1024.pt\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1dffd84a387d4cf48100fbe143287481",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='0.053 MB of 0.569 MB uploaded\\r'), FloatProgress(value=0.0935266880101429, max=1.0…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>details/current_learning_rate</td><td>▁▃▅▆████████████████████████████████████</td></tr><tr><td>details/n_training_tokens</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>losses/ghost_grad_loss</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>losses/l1_loss</td><td>██▇▆▅▄▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>losses/mse_loss</td><td>█▄▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>losses/overall_loss</td><td>█▄▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>metrics/CE_loss_score</td><td>▁▄▅▆▆▇▇▇▇▇▇▇▇███████████████████████████</td></tr><tr><td>metrics/ce_loss_with_ablation</td><td>▂▃▂▅▃▆▅▃▄▆▇▆▅▇▅▄▇▅▁▆▄▅▆▄█▄▅▆▄▅▅▃▂▄▄▅▅█▆▆</td></tr><tr><td>metrics/ce_loss_with_sae</td><td>█▅▄▃▃▃▂▂▂▂▃▂▂▂▂▂▂▂▁▂▂▂▂▁▂▂▂▂▁▂▂▁▁▂▁▂▁▂▂▂</td></tr><tr><td>metrics/ce_loss_without_sae</td><td>▄▄▁▃▄▆▅▃▆▅█▆▅▆▅▄▅▆▁▇▆▅▆▃█▆▆▆▄▇▆▃▃▆▃▆▄█▇▅</td></tr><tr><td>metrics/explained_variance</td><td>▁▅▇▇▇███████████████████████████████████</td></tr><tr><td>metrics/explained_variance_std</td><td>██▆▄▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>metrics/l0</td><td>██▇▆▅▅▄▄▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>metrics/l2_norm</td><td>▁▄▆▆▇▇▇▆▆▆▇█▇▇▆▇▇▆▇▇▇▆▇████▇▇▇▇▇▇▇▇▇█▇▇▇</td></tr><tr><td>metrics/l2_ratio</td><td>▁▃▁▂▄▃▂▄▆▅▅▅▅▆▅▆▇▆▆▇▇▆▆▆▇▆▆▇▆▇▆▇▇▇█▆▆▇▇▇</td></tr><tr><td>metrics/mean_log10_feature_sparsity</td><td>█▇▅▄▃▃▂▁▁</td></tr><tr><td>sparsity/below_1e-5</td><td>▁▁▁▁▁▁▁▁▁</td></tr><tr><td>sparsity/below_1e-6</td><td>▁▁▁▁▁▁▁▁▁</td></tr><tr><td>sparsity/dead_features</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>sparsity/mean_passes_since_fired</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▁▂▂▂▄▇▄▃▅▆▄▇██</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>details/current_learning_rate</td><td>0.0001</td></tr><tr><td>details/n_training_tokens</td><td>19988480</td></tr><tr><td>losses/ghost_grad_loss</td><td>0.0</td></tr><tr><td>losses/l1_loss</td><td>1.41017</td></tr><tr><td>losses/mse_loss</td><td>8e-05</td></tr><tr><td>losses/overall_loss</td><td>0.00036</td></tr><tr><td>metrics/CE_loss_score</td><td>0.98362</td></tr><tr><td>metrics/ce_loss_with_ablation</td><td>5.49512</td></tr><tr><td>metrics/ce_loss_with_sae</td><td>2.71813</td></tr><tr><td>metrics/ce_loss_without_sae</td><td>2.67199</td></tr><tr><td>metrics/explained_variance</td><td>0.98647</td></tr><tr><td>metrics/explained_variance_std</td><td>0.00905</td></tr><tr><td>metrics/l0</td><td>166.02246</td></tr><tr><td>metrics/l2_norm</td><td>1.39317</td></tr><tr><td>metrics/l2_ratio</td><td>0.99823</td></tr><tr><td>metrics/mean_log10_feature_sparsity</td><td>-1.53525</td></tr><tr><td>sparsity/below_1e-5</td><td>0</td></tr><tr><td>sparsity/below_1e-6</td><td>0</td></tr><tr><td>sparsity/dead_features</td><td>0</td></tr><tr><td>sparsity/mean_passes_since_fired</td><td>0.02051</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">1024-L1-0.0002-LR-0.0001-Tokens-2.000e+07</strong> at: <a href='https://wandb.ai/jbloom/mats_sae_training_language_benchmark_tests/runs/ec6k6v87' target=\"_blank\">https://wandb.ai/jbloom/mats_sae_training_language_benchmark_tests/runs/ec6k6v87</a><br/>Synced 7 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240326_191703-ec6k6v87/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "4883| MSE Loss 0.000 | L1 0.000: : 20000768it [13:29, 37714.53it/s]                            /Users/josephbloom/miniforge3/envs/mats_sae_training/lib/python3.11/site-packages/wandb/sdk/wandb_run.py:2171: UserWarning: Run (ec6k6v87) is finished. The call to `_console_raw_callback` will be ignored. Please make sure that you are using an active run.\n",
            "  lambda data: self._console_raw_callback(\"stderr\", data),\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import os\n",
        "\n",
        "from sae_lens.training.config import LanguageModelSAERunnerConfig\n",
        "from sae_lens.training.lm_runner import language_model_sae_runner\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "if device == \"cpu\" and torch.backends.mps.is_available():\n",
        "    device = \"mps\"\n",
        "\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "cfg = LanguageModelSAERunnerConfig(\n",
        "    # Data Generating Function (Model + Training Distibuion)\n",
        "    model_name=\"tiny-stories-1M\",\n",
        "    hook_point=\"blocks.1.attn.hook_z\",\n",
        "    hook_point_layer=1,\n",
        "    d_in=64,\n",
        "    # dataset_path=\"roneneldan/TinyStories\",\n",
        "    # is_dataset_tokenized=False,\n",
        "    # Dan at Apollo pretokenized this dataset for us which will speed up training.\n",
        "    dataset_path=\"apollo-research/roneneldan-TinyStories-tokenizer-gpt2\",\n",
        "    is_dataset_tokenized=True,\n",
        "    # SAE Parameters\n",
        "    expansion_factor=16,\n",
        "    # Training Parameters\n",
        "    lr=1e-4,\n",
        "    lp_norm=1.0,\n",
        "    l1_coefficient=2e-4,\n",
        "    train_batch_size=4096,\n",
        "    context_size=128,\n",
        "    # Activation Store Parameters\n",
        "    n_batches_in_buffer=128,\n",
        "    total_training_tokens=1_000_000 * 20,\n",
        "    store_batch_size=32,\n",
        "    feature_sampling_window=500,  # So we see the histograms.\n",
        "    dead_feature_window=250,\n",
        "    # WANDB\n",
        "    log_to_wandb=True,\n",
        "    wandb_project=\"mats_sae_training_language_benchmark_tests\",\n",
        "    wandb_log_frequency=10,\n",
        "    # Misc\n",
        "    device=device,\n",
        "    seed=42,\n",
        "    n_checkpoints=0,\n",
        "    checkpoint_path=\"checkpoints\",\n",
        "    dtype=torch.float32,\n",
        ")\n",
        "\n",
        "sparse_autoencoder = language_model_sae_runner(cfg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Toy Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sae_lens.training.toy_model_runner import SAEToyModelRunnerConfig, toy_model_sae_runner\n",
        "\n",
        "\n",
        "cfg = SAEToyModelRunnerConfig(\n",
        "    # Model Details\n",
        "    n_features=200,\n",
        "    n_hidden=5,\n",
        "    n_correlated_pairs=0,\n",
        "    n_anticorrelated_pairs=0,\n",
        "    feature_probability=0.025,\n",
        "    model_training_steps=10_000,\n",
        "    # SAE Parameters\n",
        "    d_sae=240,\n",
        "    l1_coefficient=0.001,\n",
        "    # SAE Train Config\n",
        "    train_batch_size=1028,\n",
        "    feature_sampling_window=3_000,\n",
        "    dead_feature_window=1_000,\n",
        "    feature_reinit_scale=0.5,\n",
        "    total_training_tokens=4096 * 300,\n",
        "    # Other parameters\n",
        "    log_to_wandb=True,\n",
        "    wandb_project=\"sae-training-test\",\n",
        "    wandb_log_frequency=5,\n",
        "    device=\"mps\",\n",
        ")\n",
        "\n",
        "trained_sae = toy_model_sae_runner(cfg)\n",
        "\n",
        "assert trained_sae is not None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Run caching of activations to disk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import os\n",
        "import sys\n",
        "\n",
        "sys.path.append(\"..\")\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "os.environ[\"WANDB__SERVICE_WAIT\"] = \"300\"\n",
        "\n",
        "from sae_lens.training.config import CacheActivationsRunnerConfig\n",
        "from sae_lens.training.cache_activations_runner import cache_activations_runner\n",
        "\n",
        "cfg = CacheActivationsRunnerConfig(\n",
        "    # Data Generating Function (Model + Training Distibuion)\n",
        "    model_name=\"gpt2-small\",\n",
        "    hook_point=\"blocks.10.attn.hook_q\",\n",
        "    hook_point_layer=10,\n",
        "    hook_point_head_index=7,\n",
        "    d_in=64,\n",
        "    dataset_path=\"Skylion007/openwebtext\",\n",
        "    is_dataset_tokenized=False,\n",
        "    cached_activations_path=\"../activations/\",\n",
        "    # Activation Store Parameters\n",
        "    n_batches_in_buffer=16,\n",
        "    total_training_tokens=500_000_000,\n",
        "    store_batch_size=32,\n",
        "    # Activation caching shuffle parameters\n",
        "    n_shuffles_final=16,\n",
        "    # Misc\n",
        "    device=\"mps\",\n",
        "    seed=42,\n",
        "    dtype=torch.float32,\n",
        ")\n",
        "\n",
        "cache_activations_runner(cfg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train an SAE using the cached activations stored on disk\n",
        "Pass `use_cached_activations=True` into the config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import os\n",
        "\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "os.environ[\"WANDB__SERVICE_WAIT\"] = \"300\"\n",
        "from sae_lens.training.config import LanguageModelSAERunnerConfig\n",
        "from sae_lens.training.lm_runner import language_model_sae_runner\n",
        "\n",
        "cfg = LanguageModelSAERunnerConfig(\n",
        "    # Data Generating Function (Model + Training Distibuion)\n",
        "    model_name=\"gpt2-small\",\n",
        "    hook_point=\"blocks.10.hook_resid_pre\",\n",
        "    hook_point_layer=11,\n",
        "    d_in=768,\n",
        "    dataset_path=\"Skylion007/openwebtext\",\n",
        "    is_dataset_tokenized=False,\n",
        "    use_cached_activations=True,\n",
        "    # SAE Parameters\n",
        "    expansion_factor=64,  # determines the dimension of the SAE.\n",
        "    # Training Parameters\n",
        "    lr=1e-5,\n",
        "    l1_coefficient=5e-4,\n",
        "    lr_scheduler_name=None,\n",
        "    train_batch_size=4096,\n",
        "    context_size=128,\n",
        "    # Activation Store Parameters\n",
        "    n_batches_in_buffer=64,\n",
        "    total_training_tokens=200_000,\n",
        "    store_batch_size=32,\n",
        "    # Resampling protocol\n",
        "    feature_sampling_method=\"l2\",\n",
        "    feature_sampling_window=1000,\n",
        "    feature_reinit_scale=0.2,\n",
        "    dead_feature_window=5000,\n",
        "    dead_feature_threshold=1e-7,\n",
        "    # WANDB\n",
        "    log_to_wandb=True,\n",
        "    wandb_project=\"mats_sae_training_gpt2_small\",\n",
        "    wandb_entity=None,\n",
        "    wandb_log_frequency=50,\n",
        "    # Misc\n",
        "    device=\"mps\",\n",
        "    seed=42,\n",
        "    n_checkpoints=5,\n",
        "    checkpoint_path=\"checkpoints\",\n",
        "    dtype=torch.float32,\n",
        ")\n",
        "\n",
        "sparse_autoencoder = language_model_sae_runner(cfg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "mats_sae_training",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
