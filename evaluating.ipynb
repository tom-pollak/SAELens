{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import json\n",
    "import plotly.express as px\n",
    "from transformer_lens import utils\n",
    "from datasets import load_dataset\n",
    "from typing import  Dict\n",
    "from pathlib import Path\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from sae_training.utils import LMSparseAutoencoderSessionloader\n",
    "from sae_analysis.visualizer.data_fns import get_feature_data, FeatureData\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = \"mps\" \n",
    "else:\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start by downloading them from huggingface\n",
    "from huggingface_hub import hf_hub_download\n",
    "REPO_ID = \"Benw8888/lp_saes\"\n",
    "layer = 6 # any layer from 0 - 11 works here\n",
    "uuid_str = \"kng5efo4\"\n",
    "FILENAME = f\"{uuid_str}/final_sae_group_gpt2-small_blocks.{layer}.hook_resid_pre_49152.pt\"\n",
    "\n",
    "# this is great because if you've already downloaded the SAE it won't download it twice!\n",
    "path = hf_hub_download(repo_id=REPO_ID, filename=FILENAME)\n",
    "# path = \"/root/mats_sae_training/checkpoints/nd2win18/o5mr946t/150003712_sparse_autoencoder_gpt2-small_blocks.6.hook_resid_pre_49152.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can then load the SAE, dataset and model using the session loader\n",
    "model, sparse_autoencoders, activation_store = LMSparseAutoencoderSessionloader.load_session_from_pretrained(\n",
    "    path = path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, sae in enumerate(sparse_autoencoders):\n",
    "    hyp = sae.cfg\n",
    "    print(f\"{i}: Layer {hyp.hook_point_layer}, p_norm {hyp.lp_norm}, alpha {hyp.l1_coefficient}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Metric Evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "from transformer_lens import HookedTransformer\n",
    "\n",
    "import wandb\n",
    "from sae_training.activations_store import ActivationsStore\n",
    "from sae_training.evals import run_evals\n",
    "from sae_training.optim import get_scheduler\n",
    "from sae_training.sparse_autoencoder import SparseAutoencoder\n",
    "from sae_training.sae_group import SAEGroup\n",
    "from sae_training.geom_median.src.geom_median.torch import compute_geometric_median\n",
    "\n",
    "def wandb_log_suffix(cfg, hyperparams):\n",
    "    # Create a mapping from cfg list keys to their corresponding hyperparams attributes\n",
    "    key_mapping = {\n",
    "        \"hook_point_layer\": \"layer\",\n",
    "        \"l1_coefficient\": \"coeff\",\n",
    "        \"lp_norm\": \"l\",\n",
    "        \"lr\": \"lr\"\n",
    "    }\n",
    "\n",
    "    # Generate the suffix by iterating over the keys that have list values in cfg\n",
    "    suffix = \"\".join(f\"_{key_mapping.get(key, key)}{getattr(hyperparams, key, '')}\" \n",
    "                    for key, value in vars(cfg).items() if isinstance(value, list))\n",
    "    return suffix\n",
    "\n",
    "\n",
    "batch_size = 1024,\n",
    "n_checkpoints = 0,\n",
    "feature_sampling_window = 1000,  # how many training steps between resampling the features / considiring neurons dead\n",
    "dead_feature_threshold = 1e-8,  # how infrequently a feature has to be active to be considered dead\n",
    "use_wandb = False,\n",
    "wandb_log_frequency = 50,\n",
    "\n",
    "total_training_tokens = sparse_autoencoders.cfg.total_training_tokens\n",
    "total_training_steps = total_training_tokens // batch_size\n",
    "n_training_steps = 0\n",
    "n_training_tokens = 0\n",
    "\n",
    "if n_checkpoints > 0:\n",
    "    checkpoint_thresholds = list(\n",
    "        range(0, total_training_tokens, total_training_tokens // n_checkpoints)\n",
    "    )[1:]\n",
    "\n",
    "# things to store for each sae:\n",
    "# act_freq_scores, n_forward_passes_since_fired, n_frac_active_tokens, optimizer, scheduler, \n",
    "num_saes = len(sparse_autoencoders)\n",
    "# track active features\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    act_freq_scores = [\n",
    "        torch.zeros(\n",
    "            sparse_autoencoders.cfg.d_sae, device=sparse_autoencoders.cfg.device\n",
    "        ) for _ in range(num_saes)\n",
    "    ]\n",
    "    n_forward_passes_since_fired = [\n",
    "        torch.zeros(\n",
    "            sparse_autoencoders.cfg.d_sae, device=sparse_autoencoders.cfg.device\n",
    "        ) for _ in range(num_saes)\n",
    "    ]\n",
    "    n_frac_active_tokens = [0  for _ in range(num_saes)]\n",
    "\n",
    "    all_layers = sparse_autoencoders.cfg.hook_point_layer\n",
    "    if not isinstance(all_layers, list):\n",
    "        all_layers = [all_layers]\n",
    "            \n",
    "    for sae in sparse_autoencoders:\n",
    "        hyperparams = sae.cfg\n",
    "        sae.eval()\n",
    "\n",
    "    pbar = tqdm(total=total_training_tokens, desc=\"Training SAE\")\n",
    "    while n_training_tokens < total_training_tokens:\n",
    "        # Do a training step.\n",
    "        layer_acts = activation_store.next_batch()\n",
    "        n_training_tokens += batch_size\n",
    "        \n",
    "        for i, (sparse_autoencoder), in enumerate(sparse_autoencoders):\n",
    "            hyperparams = sparse_autoencoder.cfg\n",
    "            layer_id = all_layers.index(hyperparams.hook_point_layer)\n",
    "            sae_in = layer_acts[:,layer_id,:]\n",
    "            \n",
    "            sparse_autoencoder.eval()\n",
    "            # Make sure the W_dec is still zero-norm\n",
    "            sparse_autoencoder.set_decoder_norm_to_unit_norm()\n",
    "\n",
    "            # log and then reset the feature sparsity every feature_sampling_window steps\n",
    "            if (n_training_steps + 1) % feature_sampling_window == 0:\n",
    "                feature_sparsity = act_freq_scores[i] / n_frac_active_tokens[i]\n",
    "                log_feature_sparsity = torch.log10(feature_sparsity + 1e-10).detach().cpu()\n",
    "\n",
    "                if use_wandb:\n",
    "                    suffix = wandb_log_suffix(sparse_autoencoder.cfg, hyperparams)\n",
    "                    wandb_histogram = wandb.Histogram(log_feature_sparsity.numpy())\n",
    "                    wandb.log(\n",
    "                        {\n",
    "                            f\"metrics/mean_log10_feature_sparsity{suffix}\": log_feature_sparsity.mean().item(),\n",
    "                            f\"plots/feature_density_line_chart{suffix}\": wandb_histogram,\n",
    "                            f\"sparsity/below_1e-5{suffix}\": (feature_sparsity < 1e-5).sum().item(),\n",
    "                            f\"sparsity/below_1e-6{suffix}\": (feature_sparsity < 1e-6).sum().item(),\n",
    "                        },\n",
    "                        step=n_training_steps,\n",
    "                    )\n",
    "\n",
    "                act_freq_scores[i] = torch.zeros(\n",
    "                    sparse_autoencoder.cfg.d_sae, device=sparse_autoencoder.cfg.device\n",
    "                )\n",
    "                n_frac_active_tokens[i] = 0\n",
    "\n",
    "            ghost_grad_neuron_mask = (\n",
    "                n_forward_passes_since_fired[i] > sparse_autoencoder.cfg.dead_feature_window\n",
    "            ).bool()\n",
    "            \n",
    "\n",
    "            # Forward and Backward Passes\n",
    "            (\n",
    "                sae_out,\n",
    "                feature_acts,\n",
    "                loss,\n",
    "                mse_loss,\n",
    "                l1_loss,\n",
    "                ghost_grad_loss,\n",
    "            ) = sparse_autoencoder(\n",
    "                sae_in,\n",
    "                ghost_grad_neuron_mask,\n",
    "            )\n",
    "            did_fire = (feature_acts > 0).float().sum(-2) > 0\n",
    "            n_forward_passes_since_fired[i] += 1\n",
    "            n_forward_passes_since_fired[i][did_fire] = 0\n",
    "\n",
    "            # Calculate the sparsities, and add it to a list, calculate sparsity metrics\n",
    "            act_freq_scores[i] += (feature_acts.abs() > 0).float().sum(0)\n",
    "            n_frac_active_tokens[i] += batch_size\n",
    "            feature_sparsity = act_freq_scores[i] / n_frac_active_tokens[i]\n",
    "\n",
    "            if use_wandb and ((n_training_steps + 1) % wandb_log_frequency == 0):\n",
    "                # metrics for currents acts\n",
    "                l0 = (feature_acts > 0).float().sum(-1).mean()\n",
    "                current_learning_rate = sparse_autoencoder.cfg.lr\n",
    "\n",
    "                per_token_l2_loss = (sae_out - sae_in).pow(2).sum(dim=-1).squeeze()\n",
    "                total_variance = (sae_in - sae_in.mean(0)).pow(2).sum(-1)\n",
    "                explained_variance = 1 - per_token_l2_loss / total_variance\n",
    "\n",
    "                suffix = wandb_log_suffix(sparse_autoencoder.cfg, hyperparams)\n",
    "                wandb.log(\n",
    "                    {\n",
    "                        # losses\n",
    "                        f\"losses/mse_loss{suffix}\": mse_loss.item(),\n",
    "                        f\"losses/l1_loss{suffix}\": l1_loss.item()\n",
    "                        / sparse_autoencoder.l1_coefficient,  # normalize by l1 coefficient\n",
    "                        f\"losses/ghost_grad_loss{suffix}\": ghost_grad_loss.item(),\n",
    "                        f\"losses/overall_loss{suffix}\": loss.item(),\n",
    "                        # variance explained\n",
    "                        f\"metrics/explained_variance{suffix}\": explained_variance.mean().item(),\n",
    "                        f\"metrics/explained_variance_std{suffix}\": explained_variance.std().item(),\n",
    "                        f\"metrics/l0{suffix}\": l0.item(),\n",
    "                        # sparsity\n",
    "                        f\"sparsity/mean_passes_since_fired{suffix}\": n_forward_passes_since_fired[i].mean().item(),\n",
    "                        f\"sparsity/dead_features{suffix}\": ghost_grad_neuron_mask.sum().item(),\n",
    "                        f\"details/n_training_tokens{suffix}\": n_training_tokens,\n",
    "                        f\"details/current_learning_rate{suffix}\": current_learning_rate,\n",
    "                    },\n",
    "                    step=n_training_steps,\n",
    "                )\n",
    "\n",
    "            # record loss frequently, but not all the time.\n",
    "            if use_wandb and ((n_training_steps + 1) % (wandb_log_frequency * 10) == 0):\n",
    "                sparse_autoencoder.eval()\n",
    "                suffix = wandb_log_suffix(sparse_autoencoder.cfg, hyperparams)\n",
    "                run_evals(sparse_autoencoder, activation_store, model, n_training_steps, suffix=suffix)\n",
    "                sparse_autoencoder.eval()\n",
    "\n",
    "            # checkpoint if at checkpoint frequency\n",
    "            if n_checkpoints > 0 and n_training_tokens > checkpoint_thresholds[0]:\n",
    "                sparse_autoencoder.set_decoder_norm_to_unit_norm()\n",
    "                path = f\"{sparse_autoencoder.cfg.checkpoint_path}/{n_training_tokens}_{sparse_autoencoder.get_name()}.pt\"\n",
    "                log_feature_sparsity_path = f\"{sparse_autoencoder.cfg.checkpoint_path}/{n_training_tokens}_{sparse_autoencoder.get_name()}_log_feature_sparsity.pt\"\n",
    "                sparse_autoencoder.save_model(path)\n",
    "                log_feature_sparsity = torch.log10(feature_sparsity + 1e-10).detach().cpu()\n",
    "                torch.save(log_feature_sparsity, log_feature_sparsity_path)\n",
    "                checkpoint_thresholds.pop(0)\n",
    "                if len(checkpoint_thresholds) == 0:\n",
    "                    n_checkpoints = 0\n",
    "                if sparse_autoencoder.cfg.log_to_wandb:\n",
    "                    model_artifact = wandb.Artifact(\n",
    "                        f\"{sparse_autoencoder.get_name()}\",\n",
    "                        type=\"model\",\n",
    "                        metadata=dict(sparse_autoencoder.cfg.__dict__),\n",
    "                    )\n",
    "                    model_artifact.add_file(path)\n",
    "                    wandb.log_artifact(model_artifact)\n",
    "\n",
    "                    sparsity_artifact = wandb.Artifact(\n",
    "                        f\"{sparse_autoencoder.get_name()}_log_feature_sparsity\",\n",
    "                        type=\"log_feature_sparsity\",\n",
    "                        metadata=dict(sparse_autoencoder.cfg.__dict__),\n",
    "                    )\n",
    "                    sparsity_artifact.add_file(log_feature_sparsity_path)\n",
    "                    wandb.log_artifact(sparsity_artifact)\n",
    "\n",
    "        n_training_steps += 1\n",
    "        pbar.set_description(\n",
    "            f\"{n_training_steps}| MSE Loss {mse_loss.item():.3f} | L1 {l1_loss.item():.3f}\"\n",
    "        )\n",
    "        pbar.update(batch_size)\n",
    "\n",
    "\n",
    "    # save sae to checkpoints folder\n",
    "    path = f\"{sparse_autoencoder.cfg.checkpoint_path}/final_{sparse_autoencoder.get_name()}.pt\"\n",
    "    sparse_autoencoder.set_decoder_norm_to_unit_norm()\n",
    "    sparse_autoencoder.save_model(path)\n",
    "\n",
    "    if sparse_autoencoder.cfg.log_to_wandb:\n",
    "        model_artifact = wandb.Artifact(\n",
    "            f\"{sparse_autoencoder.get_name()}\",\n",
    "            type=\"model\",\n",
    "            metadata=dict(sparse_autoencoder.cfg.__dict__),\n",
    "        )\n",
    "        model_artifact.add_file(path)\n",
    "        wandb.log_artifact(model_artifact, aliases=[\"final_model\"])\n",
    "\n",
    "    log_feature_sparsity_path = f\"{sparse_autoencoder.cfg.checkpoint_path}/final_{sparse_autoencoder.get_name()}_log_feature_sparsity.pt\"\n",
    "    torch.save(log_feature_sparsity, log_feature_sparsity_path)\n",
    "    if sparse_autoencoder.cfg.log_to_wandb:\n",
    "        sparsity_artifact = wandb.Artifact(\n",
    "            f\"{sparse_autoencoder.get_name()}_log_feature_sparsity\",\n",
    "            type=\"log_feature_sparsity\",\n",
    "            metadata=dict(sparse_autoencoder.cfg.__dict__),\n",
    "        )\n",
    "        sparsity_artifact.add_file(log_feature_sparsity_path)\n",
    "        wandb.log_artifact(sparsity_artifact)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mats_sae_training",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
