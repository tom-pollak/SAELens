{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Outputs for Neuronpedia Upload\n",
    "\n",
    "We use Callum McDougall's `sae_vis` library for generating JSON data to upload to Neuronpedia.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sae_lens.toolkit.pretrained_saes import download_sae_from_hf\n",
    "import os\n",
    "\n",
    "MODEL_ID = \"gpt2-small\"\n",
    "SAE_ID = \"res-jb\"\n",
    "\n",
    "(_, SAE_WEIGHTS_PATH, _) = download_sae_from_hf(\n",
    "    \"jbloom/GPT2-Small-SAEs-Reformatted\", \"blocks.0.hook_resid_pre\"\n",
    ")\n",
    "\n",
    "SAE_PATH = os.path.dirname(SAE_WEIGHTS_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save JSON to neuronpedia_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/johnnylin/.cache/huggingface/hub/models--jbloom--GPT2-Small-SAEs-Reformatted/snapshots/5bd69d8ccac6b19d91934c5aeed4866f8b6e50c7/blocks.0.hook_resid_pre\n",
      "Loaded pretrained model gpt2-small into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johnnylin/Documents/Projects/SAELens/.venv/lib/python3.12/site-packages/datasets/load.py:1461: FutureWarning: The repository for Skylion007/openwebtext contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/Skylion007/openwebtext\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Starting at batch: 1\n",
      "==== Ending at batch: 1\n",
      "Total features to run: 19321\n",
      "Total skipped: 5255\n",
      "Total batches: 806\n",
      "Hook Point Layer: 0\n",
      "Hook Point: blocks.0.hook_resid_pre\n",
      "Writing files to: ../../neuronpedia_outputs/gpt2-small_res-jb_blocks.0.hook_resid_pre\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 3435/4096 [02:41<00:31, 21.29it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 19\u001b[0m\n\u001b[1;32m      4\u001b[0m NP_OUTPUT_FOLDER \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../../neuronpedia_outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      5\u001b[0m runner \u001b[38;5;241m=\u001b[39m NeuronpediaRunner(\n\u001b[1;32m      6\u001b[0m     sae_path\u001b[38;5;241m=\u001b[39mSAE_PATH,\n\u001b[1;32m      7\u001b[0m     model_id\u001b[38;5;241m=\u001b[39mMODEL_ID,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     17\u001b[0m     end_batch_inclusive\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     18\u001b[0m )\n\u001b[0;32m---> 19\u001b[0m \u001b[43mrunner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Projects/SAELens/sae_lens/analysis/neuronpedia_runner.py:219\u001b[0m, in \u001b[0;36mNeuronpediaRunner.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m# get tokens:\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 219\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_tokens\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_batches_to_sample_from\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_prompts_to_select\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    222\u001b[0m     end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    223\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTime to get tokens: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mstart\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/Projects/SAELens/sae_lens/analysis/neuronpedia_runner.py:123\u001b[0m, in \u001b[0;36mNeuronpediaRunner.get_tokens\u001b[0;34m(self, n_batches_to_sample_from, n_prompts_to_select)\u001b[0m\n\u001b[1;32m    121\u001b[0m pbar \u001b[38;5;241m=\u001b[39m tqdm(\u001b[38;5;28mrange\u001b[39m(n_batches_to_sample_from))\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m pbar:\n\u001b[0;32m--> 123\u001b[0m     batch_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactivation_store\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_batch_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    124\u001b[0m     batch_tokens \u001b[38;5;241m=\u001b[39m batch_tokens[torch\u001b[38;5;241m.\u001b[39mrandperm(batch_tokens\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])][\n\u001b[1;32m    125\u001b[0m         : batch_tokens\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    126\u001b[0m     ]\n\u001b[1;32m    127\u001b[0m     all_tokens_list\u001b[38;5;241m.\u001b[39mappend(batch_tokens)\n",
      "File \u001b[0;32m~/Documents/Projects/SAELens/sae_lens/training/activations_store.py:227\u001b[0m, in \u001b[0;36mActivationsStore.get_batch_tokens\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m current_length \u001b[38;5;241m==\u001b[39m context_size:\n\u001b[1;32m    226\u001b[0m     full_batch \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(current_batch, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m--> 227\u001b[0m     batch_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfull_batch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\n\u001b[1;32m    229\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    230\u001b[0m     current_batch \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    231\u001b[0m     current_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sae_lens.analysis.neuronpedia_runner import NeuronpediaRunner\n",
    "\n",
    "print(SAE_PATH)\n",
    "NP_OUTPUT_FOLDER = \"../../neuronpedia_outputs\"\n",
    "runner = NeuronpediaRunner(\n",
    "    sae_path=SAE_PATH,\n",
    "    model_id=MODEL_ID,\n",
    "    sae_id=SAE_ID,\n",
    "    neuronpedia_outputs_folder=NP_OUTPUT_FOLDER,\n",
    "    init_session=True,\n",
    "    n_batches_to_sample_from=2**12,\n",
    "    n_prompts_to_select=4096 * 6,\n",
    "    n_features_at_a_time=24,\n",
    "    buffer_tokens_left=64,\n",
    "    buffer_tokens_right=62,\n",
    "    start_batch_inclusive=1,\n",
    "    end_batch_inclusive=1,\n",
    ")\n",
    "runner.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload to Neuronpedia\n",
    "#### This currently only works if you have admin access to the Neuronpedia database via localhost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpers that fix weird NaN stuff\n",
    "from decimal import Decimal\n",
    "from typing import Any\n",
    "import math\n",
    "import json\n",
    "import os\n",
    "import requests\n",
    "\n",
    "folder_path = \"../../neuronpedia_outputs/gpt2-small_blocks.0.hook_resid_pre_24576\" #runner.neuronpedia_folder\n",
    "\n",
    "\n",
    "def nanToNeg999(obj: Any) -> Any:\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: nanToNeg999(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [nanToNeg999(v) for v in obj]\n",
    "    elif (isinstance(obj, float) or isinstance(obj, Decimal)) and math.isnan(obj):\n",
    "        return -999\n",
    "    return obj\n",
    "\n",
    "\n",
    "class NanConverter(json.JSONEncoder):\n",
    "    def encode(self, o: Any, *args: Any, **kwargs: Any):\n",
    "        return super().encode(nanToNeg999(o), *args, **kwargs)\n",
    "\n",
    "\n",
    "# Server info\n",
    "host = \"http://localhost:3000\"\n",
    "sourceName = str(LAYER) + \"-\" + SOURCE\n",
    "\n",
    "# Upload alive features\n",
    "for file_name in os.listdir(folder_path):\n",
    "    if file_name.startswith(\"batch-\") and file_name.endswith(\".json\"):\n",
    "        print(\"Uploading file: \" + file_name)\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        f = open(file_path, \"r\")\n",
    "        data = json.load(f)\n",
    "\n",
    "        # Replace NaNs\n",
    "        data_fixed = json.dumps(data, cls=NanConverter)\n",
    "        data = json.loads(data_fixed)\n",
    "\n",
    "        url = host + \"/api/local/upload-features\"\n",
    "        resp = requests.post(\n",
    "            url,\n",
    "            json={\n",
    "                \"modelId\": MODEL,\n",
    "                \"layer\": sourceName,\n",
    "                \"features\": data,\n",
    "            },\n",
    "        )\n",
    "\n",
    "# Upload dead features (just makes blanks features)\n",
    "# We want this for completeness\n",
    "# skipped_path = os.path.join(folder_path, \"skipped_indexes.json\")\n",
    "# f = open(skipped_path, \"r\")\n",
    "# data = json.load(f)\n",
    "# skipped_indexes = data[\"skipped_indexes\"]\n",
    "# url = host + \"/api/internal/upload-dead-features\"\n",
    "# resp = requests.post(\n",
    "#     url,\n",
    "#     json={\n",
    "#         \"modelId\": MODEL,\n",
    "#         \"layer\": sourceName,\n",
    "#         \"deadIndexes\": skipped_indexes,\n",
    "#     },\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Automatically validate the uploaded data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mats_sae_training",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
