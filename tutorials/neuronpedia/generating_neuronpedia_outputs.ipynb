{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Outputs for Neuronpedia Upload\n",
    "\n",
    "We use Callum McDougall's `sae_vis` library for generating JSON data to upload to Neuronpedia.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from huggingface_hub import hf_hub_download\n",
    "\n",
    "# MODEL = \"gpt2-small\"\n",
    "# LAYER = 0\n",
    "# SOURCE = \"res-jb\"\n",
    "# REPO_ID = \"jbloom/GPT2-Small-SAEs\"\n",
    "# FILENAME = f\"final_sparse_autoencoder_gpt2-small_blocks.{LAYER}.hook_resid_pre_24576.pt\"\n",
    "# SAE_PATH = hf_hub_download(repo_id=REPO_ID, filename=FILENAME)\n",
    "\n",
    "# Change these\n",
    "MODEL = \"pythia-70m-deduped\"\n",
    "LAYER = 0\n",
    "TYPE = \"resid\"\n",
    "SOURCE_AUTHOR_SUFFIX = \"sm\"\n",
    "SOURCE = \"res-sm\"\n",
    "\n",
    "# Change these depending on how your files are named\n",
    "SAE_PATH = f\"../data/{SOURCE_AUTHOR_SUFFIX}/sae_{LAYER}_{TYPE}.pt\"\n",
    "FEATURE_SPARSITY_PATH = (\n",
    "    f\"../data/{SOURCE_AUTHOR_SUFFIX}/feature_sparsity_{LAYER}_{TYPE}.pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save JSON to neuronpedia_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sae_lens.analysis.neuronpedia_runner import NeuronpediaRunner\n",
    "\n",
    "NP_OUTPUT_FOLDER = \"../neuronpedia_outputs\"\n",
    "\n",
    "runner = NeuronpediaRunner(\n",
    "    sae_path=SAE_PATH,\n",
    "    feature_sparsity_path=FEATURE_SPARSITY_PATH,\n",
    "    neuronpedia_parent_folder=NP_OUTPUT_FOLDER,\n",
    "    init_session=True,\n",
    "    n_batches_to_sample_from=2**12,\n",
    "    n_prompts_to_select=4096 * 6,\n",
    "    n_features_at_a_time=512,\n",
    "    buffer_tokens_left=64,\n",
    "    buffer_tokens_right=63,\n",
    "    start_batch_inclusive=22,\n",
    "    end_batch_inclusive=23,\n",
    ")\n",
    "runner.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload to Neuronpedia\n",
    "#### This currently only works if you have admin access to the Neuronpedia database via localhost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpers that fix weird NaN stuff\n",
    "from decimal import Decimal\n",
    "from typing import Any\n",
    "import math\n",
    "import json\n",
    "import os\n",
    "import requests\n",
    "\n",
    "folder_path = runner.neuronpedia_folder\n",
    "\n",
    "\n",
    "def nanToNeg999(obj: Any) -> Any:\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: nanToNeg999(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [nanToNeg999(v) for v in obj]\n",
    "    elif (isinstance(obj, float) or isinstance(obj, Decimal)) and math.isnan(obj):\n",
    "        return -999\n",
    "    return obj\n",
    "\n",
    "\n",
    "class NanConverter(json.JSONEncoder):\n",
    "    def encode(self, o: Any, *args: Any, **kwargs: Any):\n",
    "        return super().encode(nanToNeg999(o), *args, **kwargs)\n",
    "\n",
    "\n",
    "# Server info\n",
    "host = \"http://localhost:3000\"\n",
    "sourceName = str(LAYER) + \"-\" + SOURCE\n",
    "\n",
    "# Upload alive features\n",
    "for file_name in os.listdir(folder_path):\n",
    "    if file_name.startswith(\"batch-\") and file_name.endswith(\".json\"):\n",
    "        print(\"Uploading file: \" + file_name)\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        f = open(file_path, \"r\")\n",
    "        data = json.load(f)\n",
    "\n",
    "        # Replace NaNs\n",
    "        data_fixed = json.dumps(data, cls=NanConverter)\n",
    "        data = json.loads(data_fixed)\n",
    "\n",
    "        url = host + \"/api/internal/upload-features\"\n",
    "        resp = requests.post(\n",
    "            url,\n",
    "            json={\n",
    "                \"modelId\": MODEL,\n",
    "                \"layer\": sourceName,\n",
    "                \"features\": data,\n",
    "            },\n",
    "        )\n",
    "\n",
    "# Upload dead features (just makes blanks features)\n",
    "# We want this for completeness\n",
    "# skipped_path = os.path.join(folder_path, \"skipped_indexes.json\")\n",
    "# f = open(skipped_path, \"r\")\n",
    "# data = json.load(f)\n",
    "# skipped_indexes = data[\"skipped_indexes\"]\n",
    "# url = host + \"/api/internal/upload-dead-features\"\n",
    "# resp = requests.post(\n",
    "#     url,\n",
    "#     json={\n",
    "#         \"modelId\": MODEL,\n",
    "#         \"layer\": sourceName,\n",
    "#         \"deadIndexes\": skipped_indexes,\n",
    "#     },\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Automatically validate the uploaded data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mats_sae_training",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
