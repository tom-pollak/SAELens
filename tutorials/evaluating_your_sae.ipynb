{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Evaluating your SAE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Set Up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import torch\n",
        "import json\n",
        "import plotly.express as px\n",
        "from transformer_lens import utils\n",
        "from datasets import load_dataset\n",
        "from typing import Dict\n",
        "from pathlib import Path\n",
        "\n",
        "from functools import partial\n",
        "\n",
        "sys.path.append(\"..\")\n",
        "\n",
        "from sae_lens.training.session_loader import LMSparseAutoencoderSessionloader\n",
        "from sae_lens.analysis.visualizer.data_fns import get_feature_data, FeatureData\n",
        "\n",
        "if torch.backends.mps.is_available():\n",
        "    device = \"mps\"\n",
        "else:\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "torch.set_grad_enabled(False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Load your Autoencoder\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Start by downloading them from huggingface\n",
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "REPO_ID = \"jbloom/GPT2-Small-SAEs\"\n",
        "\n",
        "\n",
        "layer = 8  # any layer from 0 - 11 works here\n",
        "FILENAME = f\"final_sparse_autoencoder_gpt2-small_blocks.{layer}.hook_resid_pre_24576.pt\"\n",
        "\n",
        "# this is great because if you've already downloaded the SAE it won't download it twice!\n",
        "path = hf_hub_download(repo_id=REPO_ID, filename=FILENAME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# We can then load the SAE, dataset and model using the session loader\n",
        "model, sparse_autoencoders, activation_store = (\n",
        "    LMSparseAutoencoderSessionloader.load_session_from_pretrained(path=path)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for i, sae in enumerate(sparse_autoencoders):\n",
        "    hyp = sae.cfg\n",
        "    print(\n",
        "        f\"{i}: Layer {hyp.hook_point_layer}, p_norm {hyp.lp_norm}, alpha {hyp.l1_coefficient}\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# pick which sae you wnat to evaluate. Default is 0\n",
        "sparse_autoencoder = sparse_autoencoders[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test the Autoencoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### L0 Test and Reconstruction Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sparse_autoencoder.eval()  # prevents error if we're expecting a dead neuron mask for who grads\n",
        "with torch.no_grad():\n",
        "    batch_tokens = activation_store.get_batch_tokens()\n",
        "    _, cache = model.run_with_cache(batch_tokens, prepend_bos=True)\n",
        "    sae_out, feature_acts, loss, mse_loss, l1_loss, _ = sparse_autoencoder(\n",
        "        cache[sparse_autoencoder.cfg.hook_point]\n",
        "    )\n",
        "    del cache\n",
        "\n",
        "    # ignore the bos token, get the number of features that activated in each token, averaged accross batch and position\n",
        "    l0 = (feature_acts[:, 1:] > 0).float().sum(-1).detach()\n",
        "    print(\"average l0\", l0.mean().item())\n",
        "    px.histogram(l0.flatten().cpu().numpy()).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# next we want to do a reconstruction test.\n",
        "def reconstr_hook(activation, hook, sae_out):\n",
        "    return sae_out\n",
        "\n",
        "\n",
        "def zero_abl_hook(activation, hook):\n",
        "    return torch.zeros_like(activation)\n",
        "\n",
        "\n",
        "print(\"Orig\", model(batch_tokens, return_type=\"loss\").item())\n",
        "print(\n",
        "    \"reconstr\",\n",
        "    model.run_with_hooks(\n",
        "        batch_tokens,\n",
        "        fwd_hooks=[\n",
        "            (\n",
        "                utils.get_act_name(\"resid_pre\", 10),\n",
        "                partial(reconstr_hook, sae_out=sae_out),\n",
        "            )\n",
        "        ],\n",
        "        return_type=\"loss\",\n",
        "    ).item(),\n",
        ")\n",
        "print(\n",
        "    \"Zero\",\n",
        "    model.run_with_hooks(\n",
        "        batch_tokens,\n",
        "        return_type=\"loss\",\n",
        "        fwd_hooks=[(utils.get_act_name(\"resid_pre\", 10), zero_abl_hook)],\n",
        "    ).item(),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Specific Capability Test\n",
        "\n",
        "Validating model performance on specific tasks when using the reconstructed activation is quite important when studying specific tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "example_prompt = \"When John and Mary went to the shops, John gave the bag to\"\n",
        "example_answer = \" Mary\"\n",
        "utils.test_prompt(example_prompt, example_answer, model, prepend_bos=True)\n",
        "\n",
        "logits, cache = model.run_with_cache(example_prompt, prepend_bos=True)\n",
        "tokens = model.to_tokens(example_prompt)\n",
        "sae_out, feature_acts, loss, mse_loss, l1_loss, _ = sparse_autoencoder(\n",
        "    cache[sparse_autoencoder.cfg.hook_point]\n",
        ")\n",
        "\n",
        "\n",
        "def reconstr_hook(activations, hook, sae_out):\n",
        "    return sae_out\n",
        "\n",
        "\n",
        "def zero_abl_hook(mlp_out, hook):\n",
        "    return torch.zeros_like(mlp_out)\n",
        "\n",
        "\n",
        "hook_point = sparse_autoencoder.cfg.hook_point\n",
        "\n",
        "print(\"Orig\", model(tokens, return_type=\"loss\").item())\n",
        "print(\n",
        "    \"reconstr\",\n",
        "    model.run_with_hooks(\n",
        "        tokens,\n",
        "        fwd_hooks=[\n",
        "            (\n",
        "                hook_point,\n",
        "                partial(reconstr_hook, sae_out=sae_out),\n",
        "            )\n",
        "        ],\n",
        "        return_type=\"loss\",\n",
        "    ).item(),\n",
        ")\n",
        "print(\n",
        "    \"Zero\",\n",
        "    model.run_with_hooks(\n",
        "        tokens,\n",
        "        return_type=\"loss\",\n",
        "        fwd_hooks=[(hook_point, zero_abl_hook)],\n",
        "    ).item(),\n",
        ")\n",
        "\n",
        "\n",
        "with model.hooks(\n",
        "    fwd_hooks=[\n",
        "        (\n",
        "            hook_point,\n",
        "            partial(reconstr_hook, sae_out=sae_out),\n",
        "        )\n",
        "    ]\n",
        "):\n",
        "    utils.test_prompt(example_prompt, example_answer, model, prepend_bos=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Generating Feature Interfaces"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "vals, inds = torch.topk(feature_acts[0, -1].detach().cpu(), 10)\n",
        "px.bar(x=[str(i) for i in inds], y=vals).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "vocab_dict = model.tokenizer.vocab\n",
        "vocab_dict = {\n",
        "    v: k.replace(\"Ä \", \" \").replace(\"\\n\", \"\\\\n\") for k, v in vocab_dict.items()\n",
        "}\n",
        "\n",
        "vocab_dict_filepath = Path(os.getcwd()) / \"vocab_dict.json\"\n",
        "if not vocab_dict_filepath.exists():\n",
        "    with open(vocab_dict_filepath, \"w\") as f:\n",
        "        json.dump(vocab_dict, f)\n",
        "\n",
        "\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "data = load_dataset(\n",
        "    \"NeelNanda/c4-code-20k\", split=\"train\"\n",
        ")  # currently use this dataset to avoid deal with tokenization while streaming\n",
        "tokenized_data = utils.tokenize_and_concatenate(data, model.tokenizer, max_length=128)\n",
        "tokenized_data = tokenized_data.shuffle(42)\n",
        "all_tokens = tokenized_data[\"tokens\"]\n",
        "\n",
        "\n",
        "# Currently, don't think much more time can be squeezed out of it. Maybe the best saving would be to\n",
        "# make the entire sequence indexing parallelized, but that's possibly not worth it right now.\n",
        "\n",
        "max_batch_size = 512\n",
        "total_batch_size = 4096 * 5\n",
        "feature_idx = list(inds.flatten().cpu().numpy())\n",
        "# max_batch_size = 512\n",
        "# total_batch_size = 16384\n",
        "# feature_idx = list(range(1000))\n",
        "\n",
        "tokens = all_tokens[:total_batch_size]\n",
        "\n",
        "feature_data: Dict[int, FeatureData] = get_feature_data(\n",
        "    encoder=sparse_autoencoder,\n",
        "    # encoder_B=sparse_autoencoder,\n",
        "    model=model,\n",
        "    hook_point=sparse_autoencoder.cfg.hook_point,\n",
        "    hook_point_layer=sparse_autoencoder.cfg.hook_point_layer,\n",
        "    tokens=tokens,\n",
        "    feature_idx=feature_idx,\n",
        "    max_batch_size=max_batch_size,\n",
        "    left_hand_k=3,\n",
        "    buffer=(5, 5),\n",
        "    n_groups=10,\n",
        "    first_group_size=20,\n",
        "    other_groups_size=5,\n",
        "    verbose=True,\n",
        ")\n",
        "\n",
        "\n",
        "for test_idx in list(inds.flatten().cpu().numpy()):\n",
        "    html_str = feature_data[test_idx].get_all_html()\n",
        "    with open(f\"data_{test_idx:04}.html\", \"w\") as f:\n",
        "        f.write(html_str)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This will produce a number of html files which each contain a dashboard showing feature activation on the sample data. It currently doesn't process that much data so it isn't that useful. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "mats_sae_training",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
