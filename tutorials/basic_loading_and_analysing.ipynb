{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and Analysing Pre-Trained Sparse Autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the most part I'll try to import functions and classes near where they are used\n",
    "# to make it clear where they come from.\n",
    "\n",
    "import torch\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading a pretrained Sparse Autoencoder\n",
    "\n",
    "SAE Lens provides utilities for downloading some previously trained Sparse Autoencoders from huggingface. We may improve the functionality here in the future and add more pre-trained SAEs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sae_lens.toolkit.pretrained_saes import get_gpt2_res_jb_saes\n",
    "\n",
    "# let's start with a layer 8 SAE.\n",
    "hook_point = \"blocks.8.hook_resid_pre\"\n",
    "\n",
    "# if the SAEs were stored with precomputed feature sparsities,\n",
    "#  those will be return in a dictionary as well.\n",
    "saes, sparsities = get_gpt2_res_jb_saes(hook_point)\n",
    "\n",
    "print(saes.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to start analysing as quickly as possible, we have a \"session loader\" which will load the transformer_lens model and a dataloading class that can provide both activations and tokens from the the huggingface dataset the SAE was trained with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sae_lens.training.session_loader import LMSparseAutoencoderSessionloader\n",
    "\n",
    "sparse_autoencoder = saes[hook_point]\n",
    "sparse_autoencoder.to(device)\n",
    "sparse_autoencoder.cfg.device = device\n",
    "\n",
    "loader = LMSparseAutoencoderSessionloader(sparse_autoencoder.cfg)\n",
    "\n",
    "# don't overwrite the sparse autoencoder with the loader's sae (newly initialized)\n",
    "model, _, activation_store = loader.load_sae_training_group_session()\n",
    "\n",
    "# TODO: We should have the session loader go straight to huggingface."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Analysis\n",
    "\n",
    "Let's check some basic stats on this SAE in order to see how some basic functionality in the codebase works.\n",
    "\n",
    "We'll calculate:\n",
    "- L0 (the number of features that fire per activation)\n",
    "- The cross entropy loss when the output of the SAE is used in place of the activations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L0 Test and Reconstruction Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "sparse_autoencoder.eval()  # prevents error if we're expecting a dead neuron mask for who grads\n",
    "\n",
    "with torch.no_grad():\n",
    "    # activation store can give us tokens.\n",
    "    batch_tokens = activation_store.get_batch_tokens()\n",
    "    _, cache = model.run_with_cache(batch_tokens, prepend_bos=True)\n",
    "\n",
    "    # Use the SAE\n",
    "    sae_out, feature_acts, loss, mse_loss, l1_loss, _ = sparse_autoencoder(\n",
    "        cache[sparse_autoencoder.cfg.hook_point]\n",
    "    )\n",
    "\n",
    "    # save some room\n",
    "    del cache\n",
    "\n",
    "    # ignore the bos token, get the number of features that activated in each token, averaged accross batch and position\n",
    "    l0 = (feature_acts[:, 1:] > 0).float().sum(-1).detach()\n",
    "    print(\"average l0\", l0.mean().item())\n",
    "    px.histogram(l0.flatten().cpu().numpy()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that while the mean L0 is 64, it varies with the specific activation.\n",
    "\n",
    "To estimate reconstruction performance, we calculate the CE loss of the model with and without the SAE being used in place of the activations. This will vary depending on the tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer_lens import utils\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "# next we want to do a reconstruction test.\n",
    "def reconstr_hook(activation, hook, sae_out):\n",
    "    return sae_out\n",
    "\n",
    "\n",
    "def zero_abl_hook(activation, hook):\n",
    "    return torch.zeros_like(activation)\n",
    "\n",
    "\n",
    "print(\"Orig\", model(batch_tokens, return_type=\"loss\").item())\n",
    "print(\n",
    "    \"reconstr\",\n",
    "    model.run_with_hooks(\n",
    "        batch_tokens,\n",
    "        fwd_hooks=[\n",
    "            (\n",
    "                utils.get_act_name(\"resid_pre\", 10),\n",
    "                partial(reconstr_hook, sae_out=sae_out),\n",
    "            )\n",
    "        ],\n",
    "        return_type=\"loss\",\n",
    "    ).item(),\n",
    ")\n",
    "print(\n",
    "    \"Zero\",\n",
    "    model.run_with_hooks(\n",
    "        batch_tokens,\n",
    "        return_type=\"loss\",\n",
    "        fwd_hooks=[(utils.get_act_name(\"resid_pre\", 10), zero_abl_hook)],\n",
    "    ).item(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specific Capability Test\n",
    "\n",
    "Validating model performance on specific tasks when using the reconstructed activation is quite important when studying specific tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_prompt = \"When John and Mary went to the shops, John gave the bag to\"\n",
    "example_answer = \" Mary\"\n",
    "utils.test_prompt(example_prompt, example_answer, model, prepend_bos=True)\n",
    "\n",
    "logits, cache = model.run_with_cache(example_prompt, prepend_bos=True)\n",
    "tokens = model.to_tokens(example_prompt)\n",
    "sae_out, feature_acts, loss, mse_loss, l1_loss, _ = sparse_autoencoder(\n",
    "    cache[sparse_autoencoder.cfg.hook_point]\n",
    ")\n",
    "\n",
    "\n",
    "def reconstr_hook(activations, hook, sae_out):\n",
    "    return sae_out\n",
    "\n",
    "\n",
    "def zero_abl_hook(mlp_out, hook):\n",
    "    return torch.zeros_like(mlp_out)\n",
    "\n",
    "\n",
    "hook_point = sparse_autoencoder.cfg.hook_point\n",
    "\n",
    "print(\"Orig\", model(tokens, return_type=\"loss\").item())\n",
    "print(\n",
    "    \"reconstr\",\n",
    "    model.run_with_hooks(\n",
    "        tokens,\n",
    "        fwd_hooks=[\n",
    "            (\n",
    "                hook_point,\n",
    "                partial(reconstr_hook, sae_out=sae_out),\n",
    "            )\n",
    "        ],\n",
    "        return_type=\"loss\",\n",
    "    ).item(),\n",
    ")\n",
    "print(\n",
    "    \"Zero\",\n",
    "    model.run_with_hooks(\n",
    "        tokens,\n",
    "        return_type=\"loss\",\n",
    "        fwd_hooks=[(hook_point, zero_abl_hook)],\n",
    "    ).item(),\n",
    ")\n",
    "\n",
    "\n",
    "with model.hooks(\n",
    "    fwd_hooks=[\n",
    "        (\n",
    "            hook_point,\n",
    "            partial(reconstr_hook, sae_out=sae_out),\n",
    "        )\n",
    "    ]\n",
    "):\n",
    "    utils.test_prompt(example_prompt, example_answer, model, prepend_bos=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Feature Interfaces\n",
    "\n",
    "Feature dashboards are an important part of SAE Evaluation. They work by:\n",
    "- 1. Collecting feature activations over a larger number of examples.\n",
    "- 2. Aggregating feature specific statistics (such as max activating examples).\n",
    "- 3. Representing that information in a standardized way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def get_tokens(\n",
    "    activation_store,\n",
    "    n_batches_to_sample_from: int = 2**10,\n",
    "    n_prompts_to_select: int = 4096 * 6,\n",
    "):\n",
    "    all_tokens_list = []\n",
    "    pbar = tqdm(range(n_batches_to_sample_from))\n",
    "    for _ in pbar:\n",
    "        batch_tokens = activation_store.get_batch_tokens()\n",
    "        batch_tokens = batch_tokens[torch.randperm(batch_tokens.shape[0])][\n",
    "            : batch_tokens.shape[0]\n",
    "        ]\n",
    "        all_tokens_list.append(batch_tokens)\n",
    "\n",
    "    all_tokens = torch.cat(all_tokens_list, dim=0)\n",
    "    all_tokens = all_tokens[torch.randperm(all_tokens.shape[0])]\n",
    "    return all_tokens[:n_prompts_to_select]\n",
    "\n",
    "\n",
    "all_tokens = get_tokens(activation_store)  # should take a few minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sae_vis.data_config_classes import SaeVisConfig\n",
    "from sae_vis.data_storing_fns import SaeVisData\n",
    "\n",
    "test_feature_idx_gpt = range(10)\n",
    "\n",
    "feature_vis_config_gpt = SaeVisConfig(\n",
    "    hook_point=hook_point,\n",
    "    features=test_feature_idx_gpt,\n",
    "    batch_size=2048,\n",
    "    minibatch_size_tokens=128,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "sae_vis_data_gpt = SaeVisData.create(\n",
    "    encoder=sparse_autoencoder,\n",
    "    model=model,\n",
    "    tokens=all_tokens,  # type: ignore\n",
    "    cfg=feature_vis_config_gpt,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import webbrowser\n",
    "\n",
    "for feature in test_feature_idx_gpt:\n",
    "    filename = f\"{feature}_feature_vis_demo_gpt.html\"\n",
    "    sae_vis_data_gpt.save_feature_centric_vis(filename, feature)\n",
    "    webbrowser.open(\"file://\" + os.path.abspath(filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, since generating feature dashboards can be done once per sparse autoencoder, for pre-trained SAEs in the public domain, everyone can use the same dashboards. Neuronpedia hosts dashboards which we can load via the intergration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "from sae_lens.analysis.neuronpedia_integration import get_neuronpedia_quick_list\n",
    "\n",
    "# this function should open\n",
    "neuronpedia_quick_list = get_neuronpedia_quick_list(\n",
    "    test_feature_idx_gpt,\n",
    "    layer=sparse_autoencoder.cfg.hook_point_layer,\n",
    "    model=\"gpt2-small\",\n",
    "    dataset=\"res-jb\",\n",
    "    name=\"A quick list we made\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mats_sae_training",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
