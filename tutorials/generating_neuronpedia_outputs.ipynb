{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Outputs for Neuronpedia Upload\n",
    "\n",
    "We use Callum McDougall's `sae_vis` library for generating JSON data to upload to Neuronpedia.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from huggingface_hub import hf_hub_download\n",
    "\n",
    "# MODEL = \"gpt2-small\"\n",
    "# LAYER = 0\n",
    "# SOURCE = \"res-jb\"\n",
    "# REPO_ID = \"jbloom/GPT2-Small-SAEs\"\n",
    "# FILENAME = f\"final_sparse_autoencoder_gpt2-small_blocks.{LAYER}.hook_resid_pre_24576.pt\"\n",
    "# SAE_PATH = hf_hub_download(repo_id=REPO_ID, filename=FILENAME)\n",
    "\n",
    "MODEL = \"pythia-70m-deduped\"\n",
    "LAYER = 0\n",
    "SOURCE = \"res-sm\"\n",
    "SAE_PATH = \"../data/res-sm/sae_\" + str(LAYER) + \"_resid.pt\"\n",
    "FEATURE_SPARSITY_PATH = \"../data/res-sm/feature_sparsity_\" + str(LAYER) + \"_resid.pt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save JSON to neuronpedia_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config_tokenizer_class: GPTNeoXTokenizer\n",
      "tokenizer_auto_map: None\n",
      "tokenizer_class_candidate fast\n",
      "tokenizer_class <class 'transformers.models.gpt_neox.tokenization_gpt_neox_fast.GPTNeoXTokenizerFast'>\n",
      "pretrained_model_name_or_path EleutherAI/pythia-70m-deduped\n",
      "vocab filenames {'vocab_file': 'vocab.json', 'merges_file': 'merges.txt', 'tokenizer_file': 'tokenizer.json'}\n",
      "vocab.json\n",
      "merges.txt\n",
      "tokenizer.json\n",
      "added_tokens.json\n",
      "special_tokens_map.json\n",
      "tokenizer_config.json\n",
      "Loaded pretrained model pythia-70m-deduped into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd38e4ea33b444ac9c201dd0bac9bbd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/1650 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run name: 32768-L1-0.001-LR-0.0003-Tokens-2.000e+06\n",
      "n_tokens_per_buffer (millions): 0.04096\n",
      "Lower bound: n_contexts_per_buffer (millions): 0.00032\n",
      "Total training steps: 488\n",
      "Total wandb updates: 48\n",
      "n_tokens_per_feature_sampling_window (millions): 1048.576\n",
      "n_tokens_per_dead_feature_window (millions): 524.288\n",
      "We will reset the sparsity calculation 0 times.\n",
      "Number tokens in sparsity calculation window: 8.19e+06\n",
      "Run name: 32768-L1-0.001-LR-0.0003-Tokens-2.000e+06\n",
      "n_tokens_per_buffer (millions): 0.04096\n",
      "Lower bound: n_contexts_per_buffer (millions): 0.00032\n",
      "Total training steps: 488\n",
      "Total wandb updates: 48\n",
      "n_tokens_per_feature_sampling_window (millions): 1048.576\n",
      "n_tokens_per_dead_feature_window (millions): 524.288\n",
      "We will reset the sparsity calculation 0 times.\n",
      "Number tokens in sparsity calculation window: 8.19e+06\n",
      "Total features to run: 19578\n",
      "Total skipped: 13190\n",
      "Total batches: 153\n",
      "Hook Point Layer: 0\n",
      "Hook Point: blocks.0.hook_resid_post\n",
      "Writing files to: ../neuronpedia_outputs/pythia-70m-deduped_blocks.0.hook_resid_post_32768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4096/4096 [03:25<00:00, 19.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to get tokens: 209.5396511554718\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/153 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing batch: 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42988279bb5a42d8bd1bffd485baf5d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Forward passes to gather data:   0%|          | 0/384 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c35a36b54d34bc3bef0955dda5c245f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Getting sequence data:   0%|          | 0/128 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sae_analysis.neuronpedia_runner import NeuronpediaRunner\n",
    "\n",
    "NP_OUTPUT_FOLDER = \"../neuronpedia_outputs\"\n",
    "\n",
    "runner = NeuronpediaRunner(\n",
    "    sae_path=SAE_PATH,\n",
    "    feature_sparsity_path=FEATURE_SPARSITY_PATH,\n",
    "    neuronpedia_parent_folder=NP_OUTPUT_FOLDER,\n",
    "    init_session=True,\n",
    "    n_batches_to_sample_from=2 ** 12,\n",
    "    n_prompts_to_select=4096 * 6,\n",
    "    n_features_at_a_time=128,\n",
    "    buffer_tokens=8,\n",
    ")\n",
    "runner.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload to Neuronpedia\n",
    "#### This currently only works if you have admin access to the Neuronpedia database via localhost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpers that fix weird NaN stuff\n",
    "from decimal import Decimal\n",
    "from typing import Any\n",
    "import math\n",
    "import json\n",
    "import os\n",
    "import requests\n",
    "\n",
    "folder_path = runner.neuronpedia_folder\n",
    "\n",
    "def nanToNeg999(obj: Any) -> Any:\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: nanToNeg999(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [nanToNeg999(v) for v in obj]\n",
    "    elif (isinstance(obj, float) or isinstance(obj, Decimal)) and math.isnan(obj):\n",
    "        return -999\n",
    "    return obj\n",
    "\n",
    "\n",
    "class NanConverter(json.JSONEncoder):\n",
    "    def encode(self, o: Any, *args: Any, **kwargs: Any):\n",
    "        return super().encode(nanToNeg999(o), *args, **kwargs)\n",
    "\n",
    "# Server info\n",
    "host = \"http://localhost:3000\"\n",
    "sourceName = str(LAYER) + \"-\" + SOURCE\n",
    "\n",
    "# Upload alive features\n",
    "for file_name in os.listdir(folder_path):\n",
    "    if file_name.startswith(\"batch-\") and file_name.endswith(\".json\"):\n",
    "        print(\"Uploading file: \" + file_name)\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        f = open(file_path, \"r\")\n",
    "        data = json.load(f)\n",
    "\n",
    "        # Replace NaNs\n",
    "        data_fixed = json.dumps(data, cls=NanConverter)\n",
    "        data = json.loads(data_fixed)\n",
    "\n",
    "        url = host + \"/api/internal/upload-features\"\n",
    "        resp = requests.post(\n",
    "            url,\n",
    "            json={\n",
    "                \"modelId\": MODEL,\n",
    "                \"layer\": sourceName,\n",
    "                \"features\": data,\n",
    "            },\n",
    "        )\n",
    "\n",
    "# Upload dead features (just makes blanks features)\n",
    "# We want this for completeness\n",
    "# skipped_path = os.path.join(folder_path, \"skipped_indexes.json\")\n",
    "# f = open(skipped_path, \"r\")\n",
    "# data = json.load(f)\n",
    "# skipped_indexes = data[\"skipped_indexes\"]\n",
    "# url = host + \"/api/internal/upload-dead-features\"\n",
    "# resp = requests.post(\n",
    "#     url,\n",
    "#     json={\n",
    "#         \"modelId\": MODEL,\n",
    "#         \"layer\": sourceName,\n",
    "#         \"deadIndexes\": skipped_indexes,\n",
    "#     },\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Automatically validate the uploaded data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mats_sae_training",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
