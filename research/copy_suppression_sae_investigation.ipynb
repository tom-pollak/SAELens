{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for Generating Investigative Datasets for SAE's + Copy Suppression\n",
    "\n",
    "\n",
    "Components:\n",
    "- Generation <- get data + model and create token df (Neel Style)\n",
    "- Calculation of non-SAE intervention (eg: Ablation)\n",
    "- Calculation of SAE based interventions (eg: Reconstruction of Query)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer_lens import HookedTransformer, utils\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from typing import List, Tuple\n",
    "from transformer_lens.components import HookPoint\n",
    "import pandas as pd\n",
    "\n",
    "from jaxtyping import Float\n",
    "from torch import Tensor\n",
    "import warnings\n",
    "\n",
    "from tqdm import tqdm\n",
    "from functools import partial\n",
    "import os \n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import plotly.express as px\n",
    "import sys \n",
    "sys.path.append(\"..\")\n",
    "from sae_training.config import LanguageModelSAERunnerConfig\n",
    "# from sae_training.sparse_autoencoder import CPU_Unpickler\n",
    "from sae_training.sparse_autoencoder import SparseAutoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils\n",
    "\n",
    "Thanks Neel!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_flatten(nested_list):\n",
    "    return [x for y in nested_list for x in y]\n",
    "\n",
    "def make_token_df(tokens, len_prefix=5, len_suffix=1, model=None):\n",
    "\n",
    "    str_tokens = [model.to_str_tokens(t) for t in tokens]\n",
    "    unique_token = [[f\"{s}/{i}\" for i, s in enumerate(str_tok)] for str_tok in str_tokens]\n",
    "    \n",
    "    context = []\n",
    "    batch = []\n",
    "    pos = []\n",
    "    label = []\n",
    "    for b in range(tokens.shape[0]):\n",
    "        # context.append([])\n",
    "        # batch.append([])\n",
    "        # pos.append([])\n",
    "        # label.append([])\n",
    "        for p in range(tokens.shape[1]):\n",
    "            prefix = \"\".join(str_tokens[b][max(0, p-len_prefix):p])\n",
    "            if p==tokens.shape[1]-1:\n",
    "                suffix = \"\"\n",
    "            else:\n",
    "                suffix = \"\".join(str_tokens[b][p+1:min(tokens.shape[1]-1, p+1+len_suffix)])\n",
    "            current = str_tokens[b][p]\n",
    "            context.append(f\"{prefix}|{current}|{suffix}\")\n",
    "            batch.append(b)\n",
    "            pos.append(p)\n",
    "            label.append(f\"{b}/{p}\")\n",
    "    # print(len(batch), len(pos), len(context), len(label))\n",
    "    return pd.DataFrame(dict(\n",
    "        str_tokens=list_flatten(str_tokens),\n",
    "        unique_token=list_flatten(unique_token),\n",
    "        context=context,\n",
    "        batch=batch,\n",
    "        pos=pos,\n",
    "        label=label,\n",
    "    ))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = HookedTransformer.from_pretrained(\n",
    "    \"gpt2-small\",\n",
    "    center_unembed=True,\n",
    "    center_writing_weights=True,\n",
    "    fold_ln=True,\n",
    "    # refactor_factored_attn_matrices=True,\n",
    ")\n",
    "model.set_use_split_qkv_input(True)\n",
    "model.set_use_attn_result(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LENGTH_RANDOM_TOKS = 4\n",
    "TOKEN_OF_INTEREST = \" John\"\n",
    "N_REPEAT_TOKENS = 3\n",
    "\n",
    "def generate_random_token_prompt(n_random_tokens = 10, n_repeat_tokens = 3, token_of_interest: str = \" John\"):\n",
    "    \n",
    "    random_tokens = torch.randint(0, model.tokenizer.vocab_size, (n_random_tokens,)).to(model.cfg.device)\n",
    "    # append the token id for \" John\"\n",
    "    if token_of_interest is not None:\n",
    "        john_token = torch.tensor(model.to_single_token(token_of_interest)).unsqueeze(0).to(model.cfg.device)\n",
    "        random_tokens = torch.cat([john_token, random_tokens], dim=0)\n",
    "    \n",
    "    # repeat the tokens \n",
    "    random_tokens = random_tokens.repeat(n_repeat_tokens)\n",
    "    \n",
    "    # generate an index for each group of tokens\n",
    "    random_token_groups = torch.arange(0, n_repeat_tokens).unsqueeze(-1).repeat(1, LENGTH_RANDOM_TOKS+1).flatten()\n",
    "    \n",
    "    return random_tokens, random_token_groups\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_webtext(seed: int = 420, dataset=\"stas/openwebtext-10k\") -> List[str]:\n",
    "    \"\"\"Get 10,000 sentences from the OpenWebText dataset\"\"\"\n",
    "\n",
    "    # Let's see some WEBTEXT\n",
    "    raw_dataset = load_dataset(dataset)\n",
    "    train_dataset = raw_dataset[\"train\"]\n",
    "    dataset = [train_dataset[i][\"text\"] for i in range(len(train_dataset))]\n",
    "\n",
    "    # Shuffle the dataset (I don't want the Hitler thing being first so use a seeded shuffle)\n",
    "    np.random.seed(seed)\n",
    "    np.random.shuffle(dataset)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "data = get_webtext()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Sparse AutoEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import wandb\n",
    "# run = wandb.init()\n",
    "# artifact = run.use_artifact('jbloom/mats_sae_training_gpt2_small_hook_q/sparse_autoencoder_gpt2-small_blocks.10.attn.hook_q_4096:v13', type='model')\n",
    "# artifact_dir = artifact.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Ours in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sae_training.utils import LMSparseAutoencoderSessionloader\n",
    "from sae_training.sparse_autoencoder import SparseAutoencoder\n",
    "# path = \"checkpoints/ikig1wjm/final_sparse_autoencoder_gpt2-small_blocks.10.attn.hook_q_32768.pkl\"\n",
    "\n",
    "# path=\"../artifacts/sparse_autoencoder_gpt2-small_blocks.10.attn.hook_q_4096:v15/final_sparse_autoencoder_gpt2-small_blocks.10.attn.hook_q_4096.pkl\"#\n",
    "# path=\"../artifacts/sparse_autoencoder_gpt2-small_blocks.10.attn.hook_q_4096:v16/final_sparse_autoencoder_gpt2-small_blocks.10.attn.hook_q_4096.pkl\"\n",
    "# path=\"../artifacts/sparse_autoencoder_gpt2-small_blocks.10.hook_resid_pre_24576:v56/final_sparse_autoencoder_gpt2-small_blocks.10.hook_resid_pre_24576.pkl\"\n",
    "\n",
    "# sparse_autoencoder = SparseAutoencoder.load_from_pretrained(path)\n",
    "# hacky solution to saved with cuda load on mps:\n",
    "\n",
    "\n",
    "path = \"artifacts/sparse_autoencoder_gpt2-small_blocks.10.attn.hook_q_4096:v13/final_sparse_autoencoder_gpt2-small_blocks.10.attn.hook_q_4096.pkl\"\n",
    "\n",
    "import io\n",
    "import pickle\n",
    "\n",
    "path = \"artifacts/sparse_autoencoder_gpt2-small_blocks.10.attn.hook_q_65536:v28/1076002816_sparse_autoencoder_gpt2-small_blocks.10.attn.hook_q_65536.pkl\" \n",
    "# path = \"checkpoints/peu1onjp/132669440_sparse_autoencoder_gpt2-small_blocks.10.attn.hook_q_8192.pkl\"\n",
    "# path = \"checkpoints/g2zrx9ho/final_sparse_autoencoder_gpt2-small_blocks.10.attn.hook_q_8192.pkl\"\n",
    "\n",
    "class CPU_Unpickler(pickle.Unpickler):\n",
    "    def find_class(self, module, name):\n",
    "        if module == 'torch.storage' and name == '_load_from_bytes':\n",
    "            return lambda b: torch.load(io.BytesIO(b), map_location='cpu')\n",
    "        else: return super().find_class(module, name)\n",
    "\n",
    "with open(path, 'rb') as file:\n",
    "    state_dict = CPU_Unpickler(file).load()\n",
    "\n",
    "cfg = state_dict[\"cfg\"].__dict__\n",
    "cfg[\"device\"] = \"mps\"\n",
    "del cfg[\"d_sae\"]\n",
    "del cfg[\"tokens_per_buffer\"]\n",
    "cfg = LanguageModelSAERunnerConfig(**cfg)\n",
    "sparse_autoencoder = SparseAutoencoder(cfg)\n",
    "sparse_autoencoder.load_state_dict(state_dict[\"state_dict\"])\n",
    "del state_dict\n",
    "del cfg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Jacob's SAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformer_lens.utils import download_file_from_hf\n",
    "# from dataclasses import dataclass\n",
    "# point, layer = \"resid_pre\", 10\n",
    "# dic = utils.download_file_from_hf(\"jacobcd52/gpt2-small-sparse-autoencoders\", f\"gpt2-small_6144_{point}_{layer}.pt\", force_is_torch=True)\n",
    "# # sparse_autoencoder.load_state_dict(dic)\n",
    "# dic.keys()\n",
    "\n",
    "\n",
    "# @dataclass\n",
    "# class SparseAutoencoderConfig:\n",
    "#     d_sae: int\n",
    "#     d_in: int\n",
    "#     l1_coefficient: float\n",
    "#     dtype: str\n",
    "#     seed: int\n",
    "#     device: str\n",
    "#     model_batch_size: int\n",
    "#     hook_point: str = \"blocks.10.hook_resid_pre\"\n",
    "#     hook_point_layer: int = 10\n",
    "    \n",
    "# cfg = {\n",
    "#     \"d_sae\": 6144,\n",
    "#     \"d_in\": 768,\n",
    "#     \"l1_coefficient\": 0.001,\n",
    "#     \"dtype\": torch.float32,\n",
    "#     \"seed\": 0,\n",
    "#     \"device\": \"mps\",\n",
    "#     \"model_batch_size\": 1028,\n",
    "# }\n",
    "\n",
    "# sparse_autoencoder_cfg = SparseAutoencoderConfig(**cfg)\n",
    "# sparse_autoencoder = SparseAutoencoder(sparse_autoencoder_cfg)\n",
    "\n",
    "\n",
    "# point, layer = \"resid_pre\", 10\n",
    "# dic = download_file_from_hf(\"jacobcd52/gpt2-small-sparse-autoencoders\", f\"gpt2-small_6144_{point}_{layer}.pt\", force_is_torch=True)\n",
    "# sparse_autoencoder.load_state_dict(dic)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Hooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAYER_IDX, HEAD_IDX = (10, 7)\n",
    "W_U = model.W_U.clone()\n",
    "HEAD_HOOK_RESULT_NAME = utils.get_act_name(\"result\", LAYER_IDX)\n",
    "HEAD_HOOK_QUERY_NAME = utils.get_act_name(\"q\", LAYER_IDX)\n",
    "HEAD_HOOK_RESID_NAME = utils.get_act_name(\"resid_pre\", LAYER_IDX)\n",
    "# BATCH_SIZE = 10\n",
    "\n",
    "def hook_to_ablate_head(head_output: Float[Tensor, \"batch seq_len head_idx d_head\"], hook: HookPoint, head = (LAYER_IDX, HEAD_IDX)):\n",
    "    assert head[0] == hook.layer()\n",
    "    assert \"result\" in hook.name\n",
    "    head_output[:, :, head[1], :] = 0\n",
    "    return head_output\n",
    "\n",
    "def hook_to_reconstruct_query(\n",
    "    head_input: Float[Tensor, \"batch seq_len head_idx d_head\"], \n",
    "    hook: HookPoint, \n",
    "    head = (LAYER_IDX, HEAD_IDX),\n",
    "    reconstructed_query: Float[Tensor, \"batch seq_len d_model\"] = None,):\n",
    "    assert head[0] == hook.layer()\n",
    "    head_input[:, :, head[1], :] = reconstructed_query[:, :]\n",
    "    return head_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_attn(patterns, token_df, title=\"\", facet_col_labels = [\"Original\", \"Reconstructed\"]):\n",
    "    '''\n",
    "    # patterns_original = cache[utils.get_act_name(\"pattern\", LAYER_IDX)][0,HEAD_IDX].detach().cpu()\n",
    "    # patterns_reconstructed = cache_reconstructed_query[utils.get_act_name(\"pattern\", LAYER_IDX)][0,HEAD_IDX].detach().cpu()\n",
    "    patterns_original = cache[utils.get_act_name(\"attn_scores\", LAYER_IDX)][0,HEAD_IDX].detach().cpu()\n",
    "    patterns_reconstructed = cache_reconstructed_query[utils.get_act_name(\"attn_scores\", LAYER_IDX)][0,HEAD_IDX].detach().cpu()\n",
    "    both_patterns = torch.stack([patterns_original, patterns_reconstructed])\n",
    "    plot_attn(both_patterns.detach().cpu(), token_df, title=\"Original and Reconstructed Attention Distribution\")\n",
    "    \n",
    "    '''\n",
    "    fig = px.imshow(patterns, text_auto=\".2f\", title=title,\n",
    "                    facet_col=0,\n",
    "                    color_continuous_midpoint=0,\n",
    "                    color_continuous_scale=\"RdBu\",\n",
    "                    )\n",
    "    \n",
    "    tickvals = 1+ np.arange(patterns.shape[2])\n",
    "    ticktext = token_df[\"unique_token\"].tolist()\n",
    "    \n",
    "    # add tokens as x-ticks and y-ticks, for each facet\n",
    "    # Update x-ticks and y-ticks for each facet\n",
    "    for i in range(len(facet_col_labels)):\n",
    "        fig.update_xaxes(\n",
    "            dict(tickmode='array', tickvals=tickvals, ticktext=ticktext),\n",
    "            row=1, col=i+1\n",
    "        )\n",
    "        fig.update_yaxes(\n",
    "            dict(tickmode='array', tickvals=tickvals, ticktext=ticktext),\n",
    "            row=1, col=i+1\n",
    "        )\n",
    "    \n",
    "    \n",
    "    # add facet col labels:\n",
    "    for i, label in enumerate(facet_col_labels):\n",
    "        fig.layout.annotations[i].text = label\n",
    "        fig.layout.annotations[i].font.size = 20\n",
    "        \n",
    "    fig.update_layout(\n",
    "        width=1200,\n",
    "        height=800,\n",
    "    )\n",
    "    fig.show()\n",
    "    \n",
    "\n",
    "# plot_attn(patterns_original.detach().cpu(), token_df, title=\"Original Attention Distribution\")\n",
    "# plot_attn(patterns_reconstructed.detach().cpu(), token_df, title=\"Reconstructed Attention Distribution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Dataset Generation Func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get ATTN results\n",
    "\n",
    "def get_max_attn_token(tokens, cache, LAYER_IDX, HEAD_IDX):\n",
    "    tokens = tokens.to(\"cpu\")\n",
    "    pattern_name = utils.get_act_name(\"pattern\", LAYER_IDX)\n",
    "    pattern = cache[pattern_name][0,HEAD_IDX].detach().cpu()\n",
    "    max_idx_pos = pattern.argmax(dim=-1)\n",
    "    max_idx_token_id = torch.gather(tokens, dim=-1, index=max_idx_pos.unsqueeze(-1).T)\n",
    "    max_idx_tok = model.to_string(max_idx_token_id.T)\n",
    "    max_idx_tok_value = pattern.max(dim=1).values\n",
    "    return max_idx_pos[1:], max_idx_tok[1:], max_idx_tok_value[1:]\n",
    "\n",
    "\n",
    "def kl_divergence_attention(y_true, y_pred):\n",
    "\n",
    "    # Compute log probabilities for KL divergence\n",
    "    log_y_true = torch.log(y_true + 1e-10)\n",
    "    log_y_pred = torch.log(y_pred + 1e-10)\n",
    "\n",
    "    return y_true * (log_y_true - log_y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def eval_prompt(prompt: List) -> pd.DataFrame:\n",
    "    '''\n",
    "    Takes a list of strings as input.\n",
    "    '''\n",
    "    tokens = model.to_tokens(prompt)\n",
    "    # tokens = tokens[:, :MAX_PROMPT_LEN]\n",
    "    token_df = make_token_df(tokens[:,1:], model=model)\n",
    "    \n",
    "    # tokens = t.stack(tokens).to(device)\n",
    "    \n",
    "    # Basic Forward Pass\n",
    "    loss, original_cache = model.run_with_cache(tokens, return_type=\"loss\", loss_per_token=True)\n",
    "    token_df['loss'] = loss.flatten().tolist()\n",
    "    \n",
    "    ## Collect ATTN Results\n",
    "    max_idx_pos, max_idx_tok, max_idx_tok_value = get_max_attn_token(tokens, original_cache, LAYER_IDX, HEAD_IDX)\n",
    "    token_df['max_idx_pos'] = max_idx_pos.flatten().tolist()\n",
    "    token_df['max_idx_tok'] = max_idx_tok\n",
    "    token_df['max_idx_tok_value'] = max_idx_tok_value.flatten().tolist()\n",
    "    \n",
    "    \n",
    "    # Full Head Ablation\n",
    "    ablated_loss = model.run_with_hooks(tokens, return_type=\"loss\", loss_per_token=True, fwd_hooks=[(HEAD_HOOK_RESULT_NAME, hook_to_ablate_head)])\n",
    "    token_df['ablated_loss'] = ablated_loss.flatten().tolist()\n",
    "    token_df[\"loss_diff\"] = token_df[\"ablated_loss\"] - token_df[\"loss\"]\n",
    "    \n",
    "    # Reconstruction of Query with SAE\n",
    "    if \"resid_pre\" in sparse_autoencoder.cfg.hook_point:\n",
    "        original_act = original_cache[sparse_autoencoder.cfg.hook_point]\n",
    "        # token_df[\"q_norm\"] = torch.norm(original_act, dim=-1)[:,1:].flatten().tolist()\n",
    "        sae_out, feature_acts, _, mse_loss, _ = sparse_autoencoder(original_act)\n",
    "        # token_df[\"rec_q_norm\"] = torch.norm(sae_out, dim=-1)[:,1:].flatten().tolist()\n",
    "\n",
    "        # need to generate query\n",
    "        def replacement_hook(resid_pre, hook, new_resid_pre=sae_out):\n",
    "            return new_resid_pre\n",
    "        \n",
    "        with model.hooks(fwd_hooks=[(HEAD_HOOK_RESID_NAME, replacement_hook)]):\n",
    "            _, resid_pre_cache = model.run_with_cache(tokens, return_type=\"loss\", loss_per_token=True)\n",
    "            sae_out = resid_pre_cache[HEAD_HOOK_QUERY_NAME][:,:,HEAD_IDX]\n",
    "        \n",
    "        original_act = original_cache[HEAD_HOOK_QUERY_NAME][:,:,HEAD_IDX]\n",
    "        mse_loss = (sae_out.float() - original_act.float()).pow(2).sum(-1)\n",
    "        total_variance = original_act.pow(2).sum(-1)\n",
    "        explained_variance = mse_loss/total_variance\n",
    "        \n",
    "    else:\n",
    "        original_act = original_cache[sparse_autoencoder.cfg.hook_point][:,:,HEAD_IDX]\n",
    "        token_df[\"q_norm\"] = torch.norm(original_act, dim=-1)[:,1:].flatten().tolist()\n",
    "        sae_out, feature_acts, _, mse_loss, _ = sparse_autoencoder(original_cache[sparse_autoencoder.cfg.hook_point][:,:,HEAD_IDX])\n",
    "        token_df[\"rec_q_norm\"] = torch.norm(sae_out, dim=-1)[:,1:].flatten().tolist()\n",
    "        # norm_ratio = torch.norm(original_act, dim=-1)/ torch.norm(sae_out, dim=-1)\n",
    "        total_variance = original_act.pow(2).sum(-1)\n",
    "        explained_variance = mse_loss/total_variance\n",
    "        \n",
    "    num_active_features = (feature_acts > 0).sum(dim=-1)\n",
    "    \n",
    "    hook_fn = partial(hook_to_reconstruct_query, reconstructed_query=sae_out)\n",
    "    with model.hooks(fwd_hooks=[(HEAD_HOOK_QUERY_NAME, hook_fn)]):\n",
    "        _, cache_reconstructed_query = model.run_with_cache(tokens, return_type=\"loss\", loss_per_token=True)\n",
    "        max_idx_pos, max_idx_tok, max_idx_tok_value = get_max_attn_token(tokens, cache_reconstructed_query, LAYER_IDX, HEAD_IDX)\n",
    "        \n",
    "        # Get the KL Divergence of the attention distributions\n",
    "        patterns_original = original_cache[utils.get_act_name(\"pattern\", LAYER_IDX)][0,HEAD_IDX].detach().cpu()\n",
    "        patterns_reconstructed = cache_reconstructed_query[utils.get_act_name(\"pattern\", LAYER_IDX)][0,HEAD_IDX].detach().cpu()\n",
    "        kl_result = kl_divergence_attention(patterns_original, patterns_reconstructed)\n",
    "        kl_result = kl_result.sum(dim=-1)[1:].numpy()\n",
    "    \n",
    "    token_df['rec_q_max_idx_pos'] = max_idx_pos.flatten().tolist()\n",
    "    token_df['rec_q_max_idx_tok'] = max_idx_tok\n",
    "    token_df['rec_q_max_idx_tok_value'] = max_idx_tok_value.flatten().tolist()\n",
    "    token_df['kl_divergence'] = kl_result.flatten().tolist()\n",
    "\n",
    "    # add results to dataframe\n",
    "\n",
    "    # SAE Metrics\n",
    "    token_df['mse_loss'] = mse_loss.flatten()[1:].tolist()\n",
    "    token_df['explained_variance'] = explained_variance.flatten()[1:].tolist()\n",
    "    token_df['num_active_features'] = num_active_features.flatten()[1:].tolist()\n",
    "    # print(feature_acts.shape)\n",
    "    # token_df[\"ids_active_features\"] = (feature_acts[0,1:] > 0)\n",
    "    \n",
    "    return token_df, original_cache, cache_reconstructed_query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Test on individual prompt (random repeating tokens)\n",
    "random_tokens, random_token_groups = generate_random_token_prompt(n_random_tokens=LENGTH_RANDOM_TOKS, n_repeat_tokens=3, token_of_interest=\" Mary\")\n",
    "prompt = model.to_string(random_tokens)\n",
    "print(prompt)\n",
    "token_df, original_cache, cache_reconstructed_query = eval_prompt([prompt])\n",
    "print(token_df.columns)\n",
    "filter_cols = [\"str_tokens\", \"unique_token\", \"context\", \"batch\", \"pos\", \"label\", \"loss\", \"loss_diff\", \"mse_loss\", \"num_active_features\", \"explained_variance\", \"kl_divergence\"]\n",
    "token_df[filter_cols].style.background_gradient(\n",
    "    subset=[\"loss_diff\", \"mse_loss\",\"explained_variance\", \"num_active_features\", \"kl_divergence\"],\n",
    "    cmap=\"coolwarm\",\n",
    ")\n",
    "\n",
    "# patterns_original = original_cache[utils.get_act_name(\"attn_scores\", LAYER_IDX)][0,HEAD_IDX].detach().cpu()\n",
    "# patterns_reconstructed = cache_reconstructed_query[utils.get_act_name(\"attn_scores\", LAYER_IDX)][0,HEAD_IDX].detach().cpu()\n",
    "patterns_original = original_cache[utils.get_act_name(\"pattern\", LAYER_IDX)][0,HEAD_IDX].detach().cpu()\n",
    "patterns_reconstructed = cache_reconstructed_query[utils.get_act_name(\"pattern\", LAYER_IDX)][0,HEAD_IDX].detach().cpu()\n",
    "both_patterns = torch.stack([patterns_original, patterns_reconstructed])\n",
    "plot_attn(both_patterns.detach().cpu(), token_df, title=\"Original and Reconstructed Attention Distribution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not \"resid_pre\" in sparse_autoencoder.cfg.hook_point:\n",
    "    original_act = original_cache[sparse_autoencoder.cfg.hook_point][:,:,HEAD_IDX]\n",
    "    sae_out, feature_acts, _, mse_loss, _ = sparse_autoencoder(original_cache[sparse_autoencoder.cfg.hook_point][:,:,HEAD_IDX])\n",
    "else:\n",
    "    original_act = original_cache[sparse_autoencoder.cfg.hook_point]\n",
    "    sae_out, feature_acts, _, mse_loss, _ = sparse_autoencoder(original_act)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_acts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n",
    "def plot_line_with_top_10_labels(tensor, title=\"\", n = 10):\n",
    "    \"\"\"\n",
    "    Plots a line chart of the given tensor with the top 10 values annotated.\n",
    "\n",
    "    :param tensor: A PyTorch tensor to be plotted.\n",
    "    \"\"\"\n",
    "    # Convert the tensor to a Pandas series for easier processing\n",
    "    data = pd.Series(tensor.detach().cpu().numpy())\n",
    "\n",
    "    # Create the line chart\n",
    "    fig = px.line(data, labels={'value': 'Activation Value', 'index': 'Feature Id'},\n",
    "                    title=title)\n",
    "\n",
    "    # Identify the top 10 values\n",
    "    top_10_indices = data.nlargest(n).index\n",
    "\n",
    "    # Annotate the top 10 values\n",
    "    for idx in top_10_indices:\n",
    "        fig.add_annotation(x=idx, y=data[idx],\n",
    "                           text=f\"{idx}\",\n",
    "                           showarrow=False, arrowhead=1,\n",
    "                           ax=0, ay=-40)  # Adjust ax, ay as needed\n",
    "    # Remove the legend\n",
    "    fig.update_layout(showlegend=False)\n",
    "\n",
    "    # Show the plot\n",
    "    fig.show()\n",
    "\n",
    "# Example usage\n",
    "POS_INTEREST = feature_acts.shape[1] - 1\n",
    "plot_line_with_top_10_labels(feature_acts[0, POS_INTEREST], \"\", 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_attn_score_by_feature(feature_ids, cache, token_df, title=\"\", vals = None):\n",
    "    '''\n",
    "    '''\n",
    "\n",
    "    k = cache[f\"blocks.{LAYER_IDX}.attn.hook_k\"][0,:(1+POS_INTEREST),HEAD_IDX]\n",
    "    # score_contributions = sparse_autoencoder.W_enc[:,inds].T @ k.T\n",
    "    score_contributions = sparse_autoencoder.W_dec[feature_ids] @ k.T\n",
    "    if vals is not None:\n",
    "        score_contributions = score_contributions * vals.unsqueeze(1)\n",
    "    fig = px.imshow(score_contributions.detach().cpu(), \n",
    "                    color_continuous_scale=\"RdBu\",\n",
    "                    color_continuous_midpoint=0,\n",
    "                    labels = dict(y=\"Feature\", x=\"Token\"),\n",
    "                    text_auto=\".2f\", title=\"\")\n",
    "    # add xticks and y ticks\n",
    "    fig.update_layout(\n",
    "        xaxis=dict(\n",
    "            tickmode='array',\n",
    "            tickvals=1+np.arange(score_contributions.shape[1]),\n",
    "            ticktext=token_df[\"str_tokens\"].tolist(),\n",
    "        ),\n",
    "        yaxis=dict(\n",
    "            tickmode='array',\n",
    "            tickvals=np.arange(score_contributions.shape[0]),\n",
    "            ticktext=list(feature_ids.detach().cpu().numpy()),\n",
    "        ),\n",
    "    )\n",
    "    fig.update_layout(\n",
    "        width=800,\n",
    "        height=800,\n",
    "    )\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "feature_name = \"All features\"\n",
    "vals, inds = torch.topk(feature_acts[0,POS_INTEREST],30)\n",
    "\n",
    "print(inds)\n",
    "plot_attn_score_by_feature(inds, original_cache, token_df, title=\"\")#, vals=vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_unembed_score_by_feature(feature_ids, token_df, title=\"\", vals = None):\n",
    "\n",
    "    token_ids = model.to_tokens(token_df.str_tokens.to_list(), prepend_bos=False).squeeze()\n",
    "    # W_U_normed = W_U / W_U.norm(dim=-1).unsqueeze(-1)\n",
    "    resid_stream_projection =  sparse_autoencoder.W_dec[feature_ids] @ model.W_Q[LAYER_IDX, HEAD_IDX].T @ W_U[:,token_ids]\n",
    "    \n",
    "    if vals is not None:\n",
    "        resid_stream_projection = resid_stream_projection * vals.unsqueeze(1)\n",
    "    fig = px.imshow(resid_stream_projection.detach().cpu(), \n",
    "                    color_continuous_scale=\"RdBu\",\n",
    "                    color_continuous_midpoint=0,\n",
    "                    labels = dict(y=\"Feature\", x=\"Token\"),\n",
    "                    text_auto=\".2f\", title=\"\")\n",
    "    # add xticks and y ticks\n",
    "    fig.update_layout(\n",
    "        xaxis=dict(\n",
    "            tickmode='array',\n",
    "            tickvals=np.arange(resid_stream_projection.shape[1]),\n",
    "            ticktext=token_df[\"str_tokens\"].tolist(),\n",
    "        ),\n",
    "        yaxis=dict(\n",
    "            tickmode='array',\n",
    "            tickvals=np.arange(resid_stream_projection.shape[0]),\n",
    "            ticktext=list(feature_ids.detach().cpu().numpy()),\n",
    "        ),\n",
    "    )\n",
    "    fig.update_layout(\n",
    "        width=800,\n",
    "        height=800,\n",
    "    )\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "feature_name = \"All features\"\n",
    "vals, inds = torch.topk(feature_acts[0,POS_INTEREST],10)\n",
    "\n",
    "print(inds)\n",
    "plot_unembed_score_by_feature(inds, token_df, vals=vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_unembed_bar(feature_id, sparse_autoencoder, feature_name = \"\"):\n",
    "    \n",
    "    norm_unembed = model.W_U / model.W_U.norm(dim=0)[None: None]\n",
    "    # feature_unembed = sparse_autoencoder.W_dec[feature_id] @ norm_unembed\n",
    "    feature_unembed = sparse_autoencoder.W_dec[feature_id] @ model.W_Q[10,7].T @  model.W_U\n",
    "    # feature_unembed = sparse_autoencoder.W_dec[feature_id] @  model.W_U\n",
    "    # torch.topk(unembed_4795,10)\n",
    "\n",
    "    feature_unembed_df = pd.DataFrame(\n",
    "        feature_unembed.detach().cpu().numpy(),\n",
    "        columns = [feature_name],\n",
    "        index = [model.tokenizer.decode(i) for i in list(range(50257))]\n",
    "    )\n",
    "\n",
    "    feature_unembed_df = feature_unembed_df.sort_values(feature_name, ascending=False).reset_index().rename(columns={'index': 'token'})\n",
    "    fig = px.bar(feature_unembed_df.head(20).sort_values(feature_name, ascending=True),\n",
    "                 color_continuous_midpoint=0,\n",
    "                 color_continuous_scale=\"RdBu\",\n",
    "            y = 'token', x = feature_name, orientation='h', color = feature_name, hover_data=[feature_name])\n",
    "\n",
    "    fig.update_layout(\n",
    "        width=800,\n",
    "        height=600,\n",
    "    )\n",
    "\n",
    "    # fig.write_image(f\"figures/{str(feature_id)}_{feature_name}.png\")\n",
    "    fig.show()\n",
    "    \n",
    "plot_feature_unembed_bar(46076, sparse_autoencoder, feature_name = \"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_qk_via_feature(feature_id, sparse_autoencoder, feature_name = \"\", highlight_tokens = []):\n",
    "    eff_embed = model.W_E + model.blocks[0].mlp(model.blocks[0].ln2(model.W_E[None] + model.blocks[0].attn.b_O))\n",
    "    eff_emb_in_key_space =  eff_embed @ model.W_K[LAYER_IDX,HEAD_IDX] @ sparse_autoencoder.W_dec[feature_id]\n",
    "    # feature_unembed = sparse_autoencoder.W_dec[feature_id] @ model.W_Q[LAYER_IDX,HEAD_IDX].T @  model.W_U\n",
    "    feature_unembed = sparse_autoencoder.W_enc[:,feature_id] @ model.W_Q[LAYER_IDX,HEAD_IDX].T @  model.W_U\n",
    "    \n",
    "    df = pd.DataFrame(dict(\n",
    "        eff_emb_in_key_space=eff_emb_in_key_space[0].detach().cpu().numpy(),\n",
    "        feature_unembed = feature_unembed.detach().cpu().numpy(),\n",
    "        token = [model.tokenizer.decode(i) for i in range(50257)],\n",
    "    ))\n",
    "    \n",
    "    df[\"token_of_interest\"] = df[\"token\"].isin(highlight_tokens)\n",
    "    \n",
    "    # add a column to df with text for the largest 10 values (positive and negative) \n",
    "    # that we can use to label these points\n",
    "    top_10_key = df.sort_values(\"eff_emb_in_key_space\", ascending=False).head(6)\n",
    "    top_10_proj = df.sort_values(\"feature_unembed\", ascending=False).head(6)\n",
    "    \n",
    "    top_10_key[\"text\"] = top_10_key.apply(lambda x: f\"{x['token']}\", axis=1)\n",
    "    top_10_proj[\"text\"] = top_10_proj.apply(lambda x: f\"{x['token']}\", axis=1)\n",
    "    \n",
    "    # Merging the top and bottom points for annotation\n",
    "    points_to_annotate = pd.concat([top_10_key, top_10_proj])\n",
    "\n",
    "    fig = px.scatter(\n",
    "        df,\n",
    "        x=\"eff_emb_in_key_space\",\n",
    "        y = \"feature_unembed\",\n",
    "        color=\"token_of_interest\",\n",
    "        color_continuous_scale=\"RdBu\",\n",
    "        # color=\"score_contributions\",\n",
    "        # text=\"text\",\n",
    "        # opacity=0.3,\n",
    "        hover_data=[\"token\"],\n",
    "        labels=dict(eff_emb_in_key_space=\"Token to Feature Virtual Weight\", feature_unembed=\"Unembed to Feature Virtual Weight\"),\n",
    "        title=f\"Feature {feature_id} {feature_name}\",\n",
    "        template=\"plotly\",\n",
    "        marginal_x=\"histogram\",\n",
    "        marginal_y=\"histogram\",\n",
    "    )\n",
    "    \n",
    "\n",
    "    for _, row in points_to_annotate.iterrows():\n",
    "        fig.add_annotation(x=row['eff_emb_in_key_space'], y=row['feature_unembed'],\n",
    "                           text=row['text'], showarrow=False, arrowhead=1,\n",
    "                           ax=20, ay=-40)\n",
    "\n",
    "    \n",
    "    fig.update_layout(\n",
    "        width=1200,\n",
    "        height=1200,\n",
    "    )\n",
    "    fig.show()\n",
    "\n",
    "plot_qk_via_feature(46037, sparse_autoencoder, feature_name = \"\", highlight_tokens=token_df.str_tokens.tolist())\n",
    "# plot_qk_via_feature(23056, sparse_autoencoder, feature_name = \"\", highlight_tokens=token_df.str_tokens.tolist())\n",
    "# plot_qk_via_feature(17088, sparse_autoencoder, feature_name = \"\", highlight_tokens=token_df.str_tokens.tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Realistic Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = \"When John and Mary went to the shops, John gave the bag to\"\n",
    "# answer = \" Mary\"\n",
    "# prompt = \"All's fair in love and\"\n",
    "# answer = \" war\"\n",
    "prompt = \" The cat is cute. The dog is\"\n",
    "# prompt = \" Alice, with her keen intelligence and artistic talent, discussed philosophy with Bob, who shared her intellect and also possessed remarkable culinary skills, while\"\n",
    "answer = \" cute\"\n",
    "model.reset_hooks()\n",
    "utils.test_prompt(prompt, answer, model)\n",
    "\n",
    "with model.hooks(fwd_hooks=[(HEAD_HOOK_RESULT_NAME, hook_to_ablate_head)]):\n",
    "    utils.test_prompt(prompt, answer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_df, original_cache, cache_reconstructed_query = eval_prompt([prompt + answer])\n",
    "print(token_df.columns)\n",
    "filter_cols = [\"str_tokens\", \"unique_token\", \"context\", \"batch\", \"pos\", \"label\", \"loss\", \"loss_diff\", \"mse_loss\", \"num_active_features\", \"explained_variance\", \"kl_divergence\"]\n",
    "token_df[filter_cols].style.background_gradient(\n",
    "    subset=[\"loss_diff\", \"mse_loss\",\"explained_variance\", \"num_active_features\", \"kl_divergence\"],\n",
    "    cmap=\"coolwarm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns_original = original_cache[utils.get_act_name(\"attn_scores\", LAYER_IDX)][0,HEAD_IDX].detach().cpu()\n",
    "patterns_reconstructed = cache_reconstructed_query[utils.get_act_name(\"attn_scores\", LAYER_IDX)][0,HEAD_IDX].detach().cpu()\n",
    "patterns_original = original_cache[utils.get_act_name(\"pattern\", LAYER_IDX)][0,HEAD_IDX].detach().cpu()\n",
    "patterns_reconstructed = cache_reconstructed_query[utils.get_act_name(\"pattern\", LAYER_IDX)][0,HEAD_IDX].detach().cpu()\n",
    "both_patterns = torch.stack([patterns_original, patterns_reconstructed])\n",
    "plot_attn(both_patterns.detach().cpu(), token_df, title=\"Original and Reconstructed Attention Distribution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_act = original_cache[sparse_autoencoder.cfg.hook_point][:,:,HEAD_IDX]\n",
    "sae_out, feature_acts, _, mse_loss, _ = sparse_autoencoder(original_cache[sparse_autoencoder.cfg.hook_point][:,:,HEAD_IDX])\n",
    "POS_INTEREST = feature_acts.shape[1]-2\n",
    "print(POS_INTEREST)\n",
    "plot_line_with_top_10_labels(feature_acts[0, POS_INTEREST], \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_name = \"All features\"\n",
    "vals, inds = torch.topk(feature_acts[0,POS_INTEREST],10)\n",
    "print(inds)\n",
    "plot_attn_score_by_feature(inds, original_cache, token_df, title=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_unembed_score_by_feature(inds, token_df, vals=vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_qk_via_feature(58215, sparse_autoencoder, feature_name = \"\", highlight_tokens=token_df.str_tokens.tolist())\n",
    "# plot_qk_via_feature(57972, sparse_autoencoder, feature_name = \"\", highlight_tokens=token_df.str_tokens.tolist())\n",
    "plot_qk_via_feature(23287, sparse_autoencoder, feature_name = \"\", highlight_tokens=token_df.str_tokens.tolist())\n",
    "\n",
    "# plot_qk_via_feature(1704, sparse_autoencoder, feature_name = \"\", highlight_tokens=token_df.str_tokens.tolist())\n",
    "# plot_qk_via_feature(14106, sparse_autoencoder, feature_name = \"\", highlight_tokens=token_df.str_tokens.tolist())\n",
    "# plot_qk_via_feature(4092, sparse_autoencoder, feature_name = \"\", highlight_tokens=token_df.str_tokens.tolist())\n",
    "# plot_qk_via_feature(14106, sparse_autoencoder, feature_name = \"\", highlight_tokens=token_df.str_tokens.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "from typing import  Dict\n",
    "from sae_analysis.visualizer import data_fns, html_fns\n",
    "from sae_analysis.visualizer.data_fns import get_feature_data, FeatureData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vocab_dict = model.tokenizer.vocab\n",
    "vocab_dict = {v: k.replace(\"Ġ\", \" \").replace(\"\\n\", \"\\\\n\") for k, v in vocab_dict.items()}\n",
    "\n",
    "vocab_dict_filepath = Path(os.getcwd()) / \"vocab_dict.json\"\n",
    "if not vocab_dict_filepath.exists():\n",
    "    with open(vocab_dict_filepath, \"w\") as f:\n",
    "        json.dump(vocab_dict, f)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "from sae_analysis.visualizer import data_fns\n",
    "reload(data_fns)\n",
    "\n",
    "dataset=\"stas/openwebtext-10k\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "# data = get_webtext()\n",
    "raw_dataset = load_dataset(dataset)\n",
    "train_dataset = raw_dataset[\"train\"]\n",
    "tokenized_data = utils.tokenize_and_concatenate(train_dataset, model.tokenizer, max_length=128)\n",
    "tokenized_data = tokenized_data.shuffle(42)\n",
    "all_tokens = tokenized_data[\"tokens\"]\n",
    "\n",
    "\n",
    "# Currently, don't think much more time can be squeezed out of it. Maybe the best saving would be to\n",
    "# make the entire sequence indexing parallelized, but that's possibly not worth it right now.\n",
    "\n",
    "max_batch_size = 32\n",
    "total_batch_size = 512 * 10\n",
    "feature_idx = list(inds.flatten().cpu().numpy())\n",
    "# max_batch_size = 512\n",
    "# total_batch_size = 16384\n",
    "# feature_idx = list(range(1000))\n",
    "\n",
    "tokens = all_tokens[:total_batch_size]\n",
    "\n",
    "feature_data: Dict[int, FeatureData] = data_fns.get_feature_data(\n",
    "    encoder=sparse_autoencoder,\n",
    "    # encoder_B=sparse_autoencoder,\n",
    "    model=model,\n",
    "    hook_point=sparse_autoencoder.cfg.hook_point,\n",
    "    hook_point_layer=sparse_autoencoder.cfg.hook_point_layer - 1,\n",
    "    hook_point_head_index=sparse_autoencoder.cfg.hook_point_head_index,\n",
    "    tokens=tokens,\n",
    "    feature_idx=feature_idx,\n",
    "    max_batch_size=max_batch_size,\n",
    "    left_hand_k = 3,\n",
    "    buffer = (5, 5),\n",
    "    n_groups = 10,\n",
    "    first_group_size = 20,\n",
    "    other_groups_size = 5,\n",
    "    verbose = True,\n",
    ")\n",
    "\n",
    "\n",
    "for test_idx in list(inds.flatten().cpu().numpy()):\n",
    "    html_str = feature_data[test_idx].get_all_html()\n",
    "    with open(f\"data_{test_idx:04}.html\", \"w\") as f:\n",
    "        f.write(html_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run on whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.mps.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_token_list = []\n",
    "loss_list = []\n",
    "ablated_loss_list = []\n",
    "\n",
    "NUM_PROMPTS = 10\n",
    "MAX_PROMPT_LEN = 100\n",
    "# BATCH_SIZE = 10\n",
    "dataframe_list = []\n",
    "with torch.no_grad():\n",
    "    for i in tqdm(range(NUM_PROMPTS)):\n",
    "        \n",
    "        # Get Token Data\n",
    "        prompt = data[i]\n",
    "        # new_str = data[BATCH_SIZE * i: BATCH_SIZE * (i + 1)]\n",
    "        \n",
    "\n",
    "        token_df, _, _= eval_prompt(prompt)\n",
    "        dataframe_list.append(token_df)\n",
    "        \n",
    "df = pd.concat(dataframe_list)\n",
    "\n",
    "print(df.shape)\n",
    "print(df.columns)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(by=\"loss_diff\", ascending=True).head(10).style.background_gradient(cmap='viridis', subset=[\"loss_diff\", \"mse_loss\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = df.query(\"max_idx_tok == rec_q_max_idx_tok\").query(\"max_idx_tok != '<|endoftext|>'\")\n",
    "print(df.shape)\n",
    "print(tmp.shape[0])\n",
    "tmp = tmp.sort_values(\"num_active_features\", ascending=True).head(50)\n",
    "tmp#.style.background_gradient(cmap='viridis', subset=[\"loss_diff\", \"num_active_features\", \"mse_loss\", \"kl_divergence\", \"q_norm\", \"rec_q_norm\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(tmp, x=\"num_active_features\", y=\"loss_diff\", hover_data=[\"max_idx_tok\", \"max_idx_tok_value\"], marginal_x=\"histogram\", marginal_y=\"histogram\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(tmp, x=\"max_idx_tok_value\", y=\"rec_q_max_idx_tok_value\", hover_data=[\"max_idx_tok\", \"max_idx_tok_value\"], marginal_x=\"histogram\", marginal_y=\"histogram\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape)\n",
    "px.histogram(df, x=\"loss_diff\", nbins=100, log_y=False, title=\"Loss Difference (Ablated - Original)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.histogram(df, x=\"num_active_features\", nbins=100, title=\"Loss Difference (Ablated - Original)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(df,\n",
    "           marginal_x=\"histogram\",\n",
    "           marginal_y=\"histogram\",\n",
    "           x=\"num_active_features\", y=\"mse_loss\", \n",
    "           log_y=True,\n",
    "           log_x=True,\n",
    "           title=\"Query Reconstuction Loss (MSE) vs. Number of Active Features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drill Down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals, inds = torch.topk(feature_acts[0,POS_INTEREST],10)\n",
    "tok_of_interest = token_df[\"str_tokens\"][POS_INTEREST+1]\n",
    "print(tok_of_interest)\n",
    "tok_id = model.tokenizer.encode(tok_of_interest)[0]\n",
    "projection_love = sparse_autoencoder.W_dec[inds] @ model.W_Q[10,7].T @  model.W_U[:,tok_id]\n",
    "# projection_love = sparse_autoencoder.W_enc[:,inds].T @ model.W_Q[10,7].T @  model.W_U[:,love_id]\n",
    "inds = inds.tolist()\n",
    "\n",
    "print(projection_love.shape)\n",
    "df = pd.DataFrame(dict(\n",
    "    projection_love=projection_love.detach().cpu().numpy(),\n",
    "    score_contributions = score_contributions[:,POS_INTEREST].detach().cpu().numpy() - score_contributions[:,0].detach().cpu().numpy(),\n",
    "    feature_id=inds,\n",
    "    activation=vals.detach().cpu().numpy(),\n",
    "    rank=range(len(inds)),\n",
    "))\n",
    "\n",
    "fig = px.scatter(\n",
    "    df,\n",
    "    x=\"projection_love\",\n",
    "    y = \"score_contributions\",\n",
    "    # color=\"feature_id\",\n",
    "    # color_continuous_scale=\"RdBu\",\n",
    "    color_continuous_midpoint=0,\n",
    "    color=\"activation\",\n",
    "    hover_data=[\"feature_id\"],\n",
    "    labels=dict(projection_love=\"Feature-Token Unembed Proj\", y=\"Activation\", score_contributions=\"Attention Score Contribution\"),\n",
    "    template=\"plotly\",\n",
    ")\n",
    "\n",
    "# add a black border around all points\n",
    "fig.update_traces(marker=dict(line=dict(width=1, color=\"Black\")))\n",
    "# make all points slightly larger\n",
    "fig.update_traces(marker=dict(size=12))\n",
    "# increase font size\n",
    "fig.update_layout(font=dict(size=18))\n",
    "fig.update_layout(\n",
    "    width=800,\n",
    "    height=800,\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_qk_via_feature(feature_id, sparse_autoencoder, feature_name = \"\", highlight_tokens = []):\n",
    "    eff_embed = model.W_E + model.blocks[0].mlp(model.blocks[0].ln2(model.W_E[None] + model.blocks[0].attn.b_O))\n",
    "    eff_emb_in_key_space =  eff_embed @ model.W_K[LAYER_IDX,HEAD_IDX] @ sparse_autoencoder.W_dec[feature_id]\n",
    "    feature_unembed = sparse_autoencoder.W_dec[feature_id] @ model.W_Q[LAYER_IDX,HEAD_IDX].T @  model.W_U\n",
    "    # feature_unembed = sparse_autoencoder.W_enc[:,feature_id] @ model.W_Q[LAYER_IDX,HEAD_IDX].T @  model.W_U\n",
    "    \n",
    "    df = pd.DataFrame(dict(\n",
    "        eff_emb_in_key_space=eff_emb_in_key_space[0].detach().cpu().numpy(),\n",
    "        feature_unembed = feature_unembed.detach().cpu().numpy(),\n",
    "        token = [model.tokenizer.decode(i) for i in range(50257)],\n",
    "    ))\n",
    "    \n",
    "    df[\"token_of_interest\"] = df[\"token\"].isin(highlight_tokens)\n",
    "    \n",
    "    # add a column to df with text for the largest 10 values (positive and negative) \n",
    "    # that we can use to label these points\n",
    "    top_10_key = df.sort_values(\"eff_emb_in_key_space\", ascending=False).head(6)\n",
    "    top_10_proj = df.sort_values(\"feature_unembed\", ascending=False).head(6)\n",
    "    \n",
    "    top_10_key[\"text\"] = top_10_key.apply(lambda x: f\"{x['token']}\", axis=1)\n",
    "    top_10_proj[\"text\"] = top_10_proj.apply(lambda x: f\"{x['token']}\", axis=1)\n",
    "    \n",
    "    # Merging the top and bottom points for annotation\n",
    "    points_to_annotate = pd.concat([top_10_key, top_10_proj])\n",
    "\n",
    "    fig = px.scatter(\n",
    "        df,\n",
    "        x=\"eff_emb_in_key_space\",\n",
    "        y = \"feature_unembed\",\n",
    "        color=\"token_of_interest\",\n",
    "        color_continuous_scale=\"RdBu\",\n",
    "        # color=\"score_contributions\",\n",
    "        # text=\"text\",\n",
    "        # opacity=0.3,\n",
    "        hover_data=[\"token\"],\n",
    "        labels=dict(eff_emb_in_key_space=\"Token to Feature Virtual Weight\", feature_unembed=\"Unembed to Feature Virtual Weight\"),\n",
    "        title=f\"Feature {feature_id} {feature_name}\",\n",
    "        template=\"plotly\",\n",
    "        marginal_x=\"histogram\",\n",
    "        marginal_y=\"histogram\",\n",
    "    )\n",
    "    \n",
    "\n",
    "    for _, row in points_to_annotate.iterrows():\n",
    "        fig.add_annotation(x=row['eff_emb_in_key_space'], y=row['feature_unembed'],\n",
    "                           text=row['text'], showarrow=False, arrowhead=1,\n",
    "                           ax=20, ay=-40)\n",
    "\n",
    "    \n",
    "    fig.update_layout(\n",
    "        width=1200,\n",
    "        height=1200,\n",
    "    )\n",
    "    fig.show()\n",
    "    \n",
    "# plot_qk_via_feature(inds[0], sparse_autoencoder, feature_name = \"What's not to suppress here?\")\n",
    "plot_qk_via_feature(3985, sparse_autoencoder, feature_name = \"\", highlight_tokens=model.to_str_tokens(tokens[0,1:]))\n",
    "# plot_qk_via_feature(1102, sparse_autoencoder, feature_name = \"\", highlight_tokens=model.to_str_tokens(tokens[0,1:]))\n",
    "# plot_qk_via_feature(1664, sparse_autoencoder, feature_name = \"\", highlight_tokens=model.to_str_tokens(tokens[0,1:]))\n",
    "# plot_qk_via_feature(3017, sparse_autoencoder, feature_name = \"\", highlight_tokens=model.to_str_tokens(tokens[0,1:]))\n",
    "# plot_qk_via_feature(2282, sparse_autoencoder, feature_name = \"\", highlight_tokens=model.to_str_tokens(tokens[0,1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_qk_via_feature(inds[1], sparse_autoencoder, feature_name = \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_qk_via_feature(2433, sparse_autoencoder, feature_name = \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_qk_via_feature(2688, sparse_autoencoder, feature_name = \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the key proj via the\n",
    "\n",
    "eff_embed.shape\n",
    "\n",
    "\n",
    "def plot_proj_onto_embed_via_key(feature_id, sparse_autoencoder, feature_name = \"\"):\n",
    "    \n",
    "    eff_embed = model.W_E + model.blocks[0].mlp(model.blocks[0].ln2(model.W_E[None] + model.blocks[0].attn.b_O))\n",
    "    \n",
    "    eff_emb_in_key_space =  eff_embed @ model.W_K[LAYER_IDX,HEAD_IDX] @ sparse_autoencoder.W_dec[feature_id]\n",
    "\n",
    "    feature_unembed_df = pd.DataFrame(\n",
    "        eff_emb_in_key_space.T.detach().cpu().numpy(),\n",
    "        columns = [feature_name],\n",
    "        index = [model.tokenizer.decode(i) for i in list(range(50257))]\n",
    "    )\n",
    "\n",
    "    feature_unembed_df = feature_unembed_df.sort_values(feature_name, ascending=False).reset_index().rename(columns={'index': 'token'})\n",
    "    fig = px.bar(feature_unembed_df.head(20).sort_values(feature_name, ascending=True),\n",
    "                 color_continuous_midpoint=0,\n",
    "                 color_continuous_scale=\"RdBu\",\n",
    "            y = 'token', x = feature_name, orientation='h', color = feature_name, hover_data=[feature_name])\n",
    "\n",
    "    fig.update_layout(\n",
    "        width=500,\n",
    "        height=600,\n",
    "    )\n",
    "\n",
    "    # fig.write_image(f\"figures/{str(feature_id)}_{feature_name}.png\")\n",
    "    fig.show()\n",
    "plot_proj_onto_embed_via_key(2688, sparse_autoencoder, feature_name = \"Famous People you Love\")\n",
    "plot_feature_unembed_bar(2688, sparse_autoencoder, feature_name = \"Famous People you Love\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mats_sae_training",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
