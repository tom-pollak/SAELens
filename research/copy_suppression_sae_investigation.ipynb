{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for Generating Investigative Datasets for SAE's + Copy Suppression\n",
    "\n",
    "\n",
    "Components:\n",
    "- Generation <- get data + model and create token df (Neel Style)\n",
    "- Calculation of non-SAE intervention (eg: Ablation)\n",
    "- Calculation of SAE based interventions (eg: Reconstruction of Query)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from importlib import reload\n",
    "\n",
    "import joseph\n",
    "from joseph.analysis import *\n",
    "from joseph.visualisation import *\n",
    "from joseph.utils import *\n",
    "from joseph.data import *\n",
    "\n",
    "\n",
    "reload(joseph.analysis)\n",
    "reload(joseph.visualisation)\n",
    "reload(joseph.utils)\n",
    "reload(joseph.data)\n",
    "\n",
    "from joseph.analysis import *\n",
    "from joseph.visualisation import *\n",
    "from joseph.utils import *\n",
    "from joseph.data import *\n",
    "\n",
    "# turn torch grad tracking off\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "LAYER_IDX, HEAD_IDX = (10, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = HookedTransformer.from_pretrained(\n",
    "    \"gpt2-small\",\n",
    "    center_unembed=True,\n",
    "    center_writing_weights=True,\n",
    "    fold_ln=True,\n",
    "    # refactor_factored_attn_matrices=True,\n",
    ")\n",
    "model.set_use_split_qkv_input(True)\n",
    "model.set_use_attn_result(True)\n",
    "\n",
    "\n",
    "\n",
    "# path = \"checkpoints/ikig1wjm/final_sparse_autoencoder_gpt2-small_blocks.10.attn.hook_q_32768.pkl\"\n",
    "# path=\"../artifacts/sparse_autoencoder_gpt2-small_blocks.10.attn.hook_q_4096:v15/final_sparse_autoencoder_gpt2-small_blocks.10.attn.hook_q_4096.pkl\"#\n",
    "# path=\"../artifacts/sparse_autoencoder_gpt2-small_blocks.10.attn.hook_q_4096:v16/final_sparse_autoencoder_gpt2-small_blocks.10.attn.hook_q_4096.pkl\"\n",
    "# path=\"../artifacts/sparse_autoencoder_gpt2-small_blocks.10.hook_resid_pre_24576:v56/final_sparse_autoencoder_gpt2-small_blocks.10.hook_resid_pre_24576.pkl\"\n",
    "# hacky solution to saved with cuda load on mps:\n",
    "# path = \"artifacts/sparse_autoencoder_gpt2-small_blocks.10.attn.hook_q_4096:v13/final_sparse_autoencoder_gpt2-small_blocks.10.attn.hook_q_4096.pkl\"\n",
    "# sparse_autoencoder = SparseAutoencoder.load_from_pretrained(path)\n",
    "\n",
    "\n",
    "from sae_training.sparse_autoencoder import SparseAutoencoder\n",
    "from sae_training.config import LanguageModelSAERunnerConfig\n",
    "\n",
    "# path = \"checkpoints/peu1onjp/132669440_sparse_autoencoder_gpt2-small_blocks.10.attn.hook_q_8192.pkl\"\n",
    "# path = \"checkpoints/g2zrx9ho/final_sparse_autoencoder_gpt2-small_blocks.10.attn.hook_q_8192.pkl\"\n",
    "path = \"artifacts/sparse_autoencoder_gpt2-small_blocks.10.attn.hook_q_65536:v28/1076002816_sparse_autoencoder_gpt2-small_blocks.10.attn.hook_q_65536.pkl\" \n",
    "\n",
    "\n",
    "with open(path, 'rb') as file:\n",
    "    state_dict = CPU_Unpickler(file).load()\n",
    "\n",
    "cfg = state_dict[\"cfg\"].__dict__\n",
    "cfg[\"device\"] = \"mps\"\n",
    "cfg[\"hook_point_layer\"] = 10\n",
    "del cfg[\"d_sae\"]\n",
    "del cfg[\"tokens_per_buffer\"]\n",
    "cfg = LanguageModelSAERunnerConfig(**cfg)\n",
    "sparse_autoencoder = SparseAutoencoder(cfg)\n",
    "sparse_autoencoder.load_state_dict(state_dict[\"state_dict\"])\n",
    "del state_dict\n",
    "del cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Jacob's SAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformer_lens.utils import download_file_from_hf\n",
    "# from dataclasses import dataclass\n",
    "# point, layer = \"resid_pre\", 10\n",
    "# dic = utils.download_file_from_hf(\"jacobcd52/gpt2-small-sparse-autoencoders\", f\"gpt2-small_6144_{point}_{layer}.pt\", force_is_torch=True)\n",
    "# # sparse_autoencoder.load_state_dict(dic)\n",
    "# dic.keys()\n",
    "\n",
    "\n",
    "# @dataclass\n",
    "# class SparseAutoencoderConfig:\n",
    "#     d_sae: int\n",
    "#     d_in: int\n",
    "#     l1_coefficient: float\n",
    "#     dtype: str\n",
    "#     seed: int\n",
    "#     device: str\n",
    "#     model_batch_size: int\n",
    "#     hook_point: str = \"blocks.10.hook_resid_pre\"\n",
    "#     hook_point_layer: int = 10\n",
    "    \n",
    "# cfg = {\n",
    "#     \"d_sae\": 6144,\n",
    "#     \"d_in\": 768,\n",
    "#     \"l1_coefficient\": 0.001,\n",
    "#     \"dtype\": torch.float32,\n",
    "#     \"seed\": 0,\n",
    "#     \"device\": \"mps\",\n",
    "#     \"model_batch_size\": 1028,\n",
    "# }\n",
    "\n",
    "# sparse_autoencoder_cfg = SparseAutoencoderConfig(**cfg)\n",
    "# sparse_autoencoder = SparseAutoencoder(sparse_autoencoder_cfg)\n",
    "\n",
    "\n",
    "# point, layer = \"resid_pre\", 10\n",
    "# dic = download_file_from_hf(\"jacobcd52/gpt2-small-sparse-autoencoders\", f\"gpt2-small_6144_{point}_{layer}.pt\", force_is_torch=True)\n",
    "# sparse_autoencoder.load_state_dict(dic)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Hooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Dataset Generation Func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Test on individual prompt (random repeating tokens)\n",
    "random_tokens, random_token_groups = generate_random_token_prompt(model, n_random_tokens=LENGTH_RANDOM_TOKS, n_repeat_tokens=3, token_of_interest=\" Mary\")\n",
    "prompt = model.to_string(random_tokens)\n",
    "print(prompt)\n",
    "token_df, original_cache, cache_reconstructed_query = eval_prompt([prompt], model, sparse_autoencoder)\n",
    "print(token_df.columns)\n",
    "filter_cols = [\"str_tokens\", \"unique_token\", \"context\", \"batch\", \"pos\", \"label\", \"loss\", \"loss_diff\", \"mse_loss\", \"num_active_features\", \"explained_variance\", \"kl_divergence\"]\n",
    "token_df[filter_cols].style.background_gradient(\n",
    "    subset=[\"loss_diff\", \"mse_loss\",\"explained_variance\", \"num_active_features\", \"kl_divergence\"],\n",
    "    cmap=\"coolwarm\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns_original = original_cache[utils.get_act_name(\"attn_scores\", LAYER_IDX)][0,HEAD_IDX].detach().cpu()\n",
    "patterns_reconstructed = cache_reconstructed_query[utils.get_act_name(\"attn_scores\", LAYER_IDX)][0,HEAD_IDX].detach().cpu()\n",
    "both_patterns = torch.stack([patterns_original, patterns_reconstructed])\n",
    "plot_attn(both_patterns.detach().cpu(), token_df, title=\"Original and Reconstructed Attention Distribution\")\n",
    "patterns_original = original_cache[utils.get_act_name(\"pattern\", LAYER_IDX)][0,HEAD_IDX].detach().cpu()\n",
    "patterns_reconstructed = cache_reconstructed_query[utils.get_act_name(\"pattern\", LAYER_IDX)][0,HEAD_IDX].detach().cpu()\n",
    "both_patterns = torch.stack([patterns_original, patterns_reconstructed])\n",
    "plot_attn(both_patterns.detach().cpu(), token_df, title=\"Original and Reconstructed Attention Distribution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not \"resid_pre\" in sparse_autoencoder.cfg.hook_point:\n",
    "    original_act = original_cache[sparse_autoencoder.cfg.hook_point][:,:,HEAD_IDX]\n",
    "    sae_out, feature_acts, _, mse_loss, _ = sparse_autoencoder(original_cache[sparse_autoencoder.cfg.hook_point][:,:,HEAD_IDX])\n",
    "else:\n",
    "    original_act = original_cache[sparse_autoencoder.cfg.hook_point]\n",
    "    sae_out, feature_acts, _, mse_loss, _ = sparse_autoencoder(original_act)\n",
    "\n",
    "POS_INTEREST = feature_acts.shape[1] - 1\n",
    "plot_line_with_top_10_labels(feature_acts[0, POS_INTEREST], \"\", 30)\n",
    "\n",
    "vals, inds = torch.topk(feature_acts[0,POS_INTEREST],10)\n",
    "\n",
    "print(inds)\n",
    "plot_attn_score_by_feature(model, sparse_autoencoder, inds, original_cache, token_df, pos_interest=POS_INTEREST, vals=vals)\n",
    "plot_unembed_score_by_feature(model, sparse_autoencoder, inds, token_df, vals=vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_feature_unembed_bar(46076, sparse_autoencoder, feature_name = \"\")\n",
    "plot_qk_via_feature(model, 49077, sparse_autoencoder, feature_name = \"\", highlight_tokens=token_df.str_tokens.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Realistic Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"When John and Mary went to the shops, John gave the bag to\"\n",
    "answer = \" Mary\"\n",
    "# prompt = \"All's fair in love and\"\n",
    "# answer = \" war\"\n",
    "# prompt = \" The cat is cute. The dog is\"\n",
    "# prompt = \" Alice, with her keen intelligence and artistic talent, discussed philosophy with Bob, who shared her intellect and also possessed remarkable culinary skills, while\"\n",
    "# answer = \" cute\"\n",
    "model.reset_hooks()\n",
    "utils.test_prompt(prompt, answer, model)\n",
    "\n",
    "with model.hooks(fwd_hooks=[(HEAD_HOOK_RESULT_NAME, hook_to_ablate_head)]):\n",
    "    utils.test_prompt(prompt, answer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_df, original_cache, cache_reconstructed_query = eval_prompt([prompt + answer], model, sparse_autoencoder)\n",
    "print(token_df.columns)\n",
    "filter_cols = [\"str_tokens\", \"unique_token\", \"context\", \"batch\", \"pos\", \"label\", \"loss\", \"loss_diff\", \"mse_loss\", \"num_active_features\", \"explained_variance\", \"kl_divergence\"]\n",
    "token_df[filter_cols].style.background_gradient(\n",
    "    subset=[\"loss_diff\", \"mse_loss\",\"explained_variance\", \"num_active_features\", \"kl_divergence\"],\n",
    "    cmap=\"coolwarm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns_original = original_cache[utils.get_act_name(\"attn_scores\", LAYER_IDX)][0,HEAD_IDX].detach().cpu()\n",
    "patterns_reconstructed = cache_reconstructed_query[utils.get_act_name(\"attn_scores\", LAYER_IDX)][0,HEAD_IDX].detach().cpu()\n",
    "plot_attn(both_patterns.detach().cpu(), token_df, title=\"Original and Reconstructed Attention Distribution\")\n",
    "patterns_original = original_cache[utils.get_act_name(\"pattern\", LAYER_IDX)][0,HEAD_IDX].detach().cpu()\n",
    "patterns_reconstructed = cache_reconstructed_query[utils.get_act_name(\"pattern\", LAYER_IDX)][0,HEAD_IDX].detach().cpu()\n",
    "both_patterns = torch.stack([patterns_original, patterns_reconstructed])\n",
    "plot_attn(both_patterns.detach().cpu(), token_df, title=\"Original and Reconstructed Attention Distribution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_act = original_cache[sparse_autoencoder.cfg.hook_point][:,:,HEAD_IDX]\n",
    "sae_out, feature_acts, _, mse_loss, _ = sparse_autoencoder(original_cache[sparse_autoencoder.cfg.hook_point][:,:,HEAD_IDX])\n",
    "POS_INTEREST = feature_acts.shape[1]-2\n",
    "print(POS_INTEREST)\n",
    "plot_line_with_top_10_labels(feature_acts[0, POS_INTEREST], \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_name = \"All features\"\n",
    "vals, inds = torch.topk(feature_acts[0,POS_INTEREST],10)\n",
    "print(inds)\n",
    "plot_attn_score_by_feature(model, sparse_autoencoder, inds, original_cache, token_df, POS_INTEREST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_unembed_score_by_feature(model, sparse_autoencoder, inds, token_df, vals=vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_qk_via_feature(model, 23287, sparse_autoencoder, feature_name = \"\", highlight_tokens=token_df.str_tokens.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "from typing import  Dict\n",
    "from sae_analysis.visualizer import data_fns, html_fns\n",
    "from sae_analysis.visualizer.data_fns import get_feature_data, FeatureData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vocab_dict = model.tokenizer.vocab\n",
    "vocab_dict = {v: k.replace(\"Ġ\", \" \").replace(\"\\n\", \"\\\\n\") for k, v in vocab_dict.items()}\n",
    "\n",
    "vocab_dict_filepath = Path(os.getcwd()) / \"vocab_dict.json\"\n",
    "if not vocab_dict_filepath.exists():\n",
    "    with open(vocab_dict_filepath, \"w\") as f:\n",
    "        json.dump(vocab_dict, f)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "from sae_analysis.visualizer import data_fns\n",
    "reload(data_fns)\n",
    "\n",
    "dataset=\"stas/openwebtext-10k\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "# data = get_webtext()\n",
    "raw_dataset = load_dataset(dataset)\n",
    "train_dataset = raw_dataset[\"train\"]\n",
    "tokenized_data = utils.tokenize_and_concatenate(train_dataset, model.tokenizer, max_length=128)\n",
    "tokenized_data = tokenized_data.shuffle(42)\n",
    "all_tokens = tokenized_data[\"tokens\"]\n",
    "\n",
    "\n",
    "# Currently, don't think much more time can be squeezed out of it. Maybe the best saving would be to\n",
    "# make the entire sequence indexing parallelized, but that's possibly not worth it right now.\n",
    "\n",
    "max_batch_size = 32\n",
    "total_batch_size = 512 * 10\n",
    "feature_idx = list(inds.flatten().cpu().numpy())\n",
    "# max_batch_size = 512\n",
    "# total_batch_size = 16384\n",
    "# feature_idx = list(range(1000))\n",
    "\n",
    "tokens = all_tokens[:total_batch_size]\n",
    "\n",
    "feature_data: Dict[int, FeatureData] = data_fns.get_feature_data(\n",
    "    encoder=sparse_autoencoder,\n",
    "    # encoder_B=sparse_autoencoder,\n",
    "    model=model,\n",
    "    hook_point=sparse_autoencoder.cfg.hook_point,\n",
    "    hook_point_layer=sparse_autoencoder.cfg.hook_point_layer - 1,\n",
    "    hook_point_head_index=sparse_autoencoder.cfg.hook_point_head_index,\n",
    "    tokens=tokens,\n",
    "    feature_idx=feature_idx,\n",
    "    max_batch_size=max_batch_size,\n",
    "    left_hand_k = 3,\n",
    "    buffer = (5, 5),\n",
    "    n_groups = 10,\n",
    "    first_group_size = 20,\n",
    "    other_groups_size = 5,\n",
    "    verbose = True,\n",
    ")\n",
    "\n",
    "\n",
    "for test_idx in list(inds.flatten().cpu().numpy()):\n",
    "    html_str = feature_data[test_idx].get_all_html()\n",
    "    with open(f\"data_{test_idx:04}.html\", \"w\") as f:\n",
    "        f.write(html_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run on whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.mps.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_token_list = []\n",
    "loss_list = []\n",
    "ablated_loss_list = []\n",
    "\n",
    "NUM_PROMPTS = 10\n",
    "MAX_PROMPT_LEN = 100\n",
    "# BATCH_SIZE = 10\n",
    "dataframe_list = []\n",
    "with torch.no_grad():\n",
    "    for i in tqdm(range(NUM_PROMPTS)):\n",
    "        \n",
    "        # Get Token Data\n",
    "        prompt = data[i]\n",
    "        # new_str = data[BATCH_SIZE * i: BATCH_SIZE * (i + 1)]\n",
    "        \n",
    "\n",
    "        token_df, _, _= eval_prompt(prompt)\n",
    "        dataframe_list.append(token_df)\n",
    "        \n",
    "df = pd.concat(dataframe_list)\n",
    "\n",
    "print(df.shape)\n",
    "print(df.columns)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(by=\"loss_diff\", ascending=True).head(10).style.background_gradient(cmap='viridis', subset=[\"loss_diff\", \"mse_loss\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = df.query(\"max_idx_tok == rec_q_max_idx_tok\").query(\"max_idx_tok != '<|endoftext|>'\")\n",
    "print(df.shape)\n",
    "print(tmp.shape[0])\n",
    "tmp = tmp.sort_values(\"num_active_features\", ascending=True).head(50)\n",
    "tmp#.style.background_gradient(cmap='viridis', subset=[\"loss_diff\", \"num_active_features\", \"mse_loss\", \"kl_divergence\", \"q_norm\", \"rec_q_norm\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(tmp, x=\"num_active_features\", y=\"loss_diff\", hover_data=[\"max_idx_tok\", \"max_idx_tok_value\"], marginal_x=\"histogram\", marginal_y=\"histogram\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(tmp, x=\"max_idx_tok_value\", y=\"rec_q_max_idx_tok_value\", hover_data=[\"max_idx_tok\", \"max_idx_tok_value\"], marginal_x=\"histogram\", marginal_y=\"histogram\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape)\n",
    "px.histogram(df, x=\"loss_diff\", nbins=100, log_y=False, title=\"Loss Difference (Ablated - Original)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.histogram(df, x=\"num_active_features\", nbins=100, title=\"Loss Difference (Ablated - Original)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(df,\n",
    "           marginal_x=\"histogram\",\n",
    "           marginal_y=\"histogram\",\n",
    "           x=\"num_active_features\", y=\"mse_loss\", \n",
    "           log_y=True,\n",
    "           log_x=True,\n",
    "           title=\"Query Reconstuction Loss (MSE) vs. Number of Active Features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drill Down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals, inds = torch.topk(feature_acts[0,POS_INTEREST],10)\n",
    "tok_of_interest = token_df[\"str_tokens\"][POS_INTEREST+1]\n",
    "print(tok_of_interest)\n",
    "tok_id = model.tokenizer.encode(tok_of_interest)[0]\n",
    "projection_love = sparse_autoencoder.W_dec[inds] @ model.W_Q[10,7].T @  model.W_U[:,tok_id]\n",
    "# projection_love = sparse_autoencoder.W_enc[:,inds].T @ model.W_Q[10,7].T @  model.W_U[:,love_id]\n",
    "inds = inds.tolist()\n",
    "\n",
    "print(projection_love.shape)\n",
    "df = pd.DataFrame(dict(\n",
    "    projection_love=projection_love.detach().cpu().numpy(),\n",
    "    score_contributions = score_contributions[:,POS_INTEREST].detach().cpu().numpy() - score_contributions[:,0].detach().cpu().numpy(),\n",
    "    feature_id=inds,\n",
    "    activation=vals.detach().cpu().numpy(),\n",
    "    rank=range(len(inds)),\n",
    "))\n",
    "\n",
    "fig = px.scatter(\n",
    "    df,\n",
    "    x=\"projection_love\",\n",
    "    y = \"score_contributions\",\n",
    "    # color=\"feature_id\",\n",
    "    # color_continuous_scale=\"RdBu\",\n",
    "    color_continuous_midpoint=0,\n",
    "    color=\"activation\",\n",
    "    hover_data=[\"feature_id\"],\n",
    "    labels=dict(projection_love=\"Feature-Token Unembed Proj\", y=\"Activation\", score_contributions=\"Attention Score Contribution\"),\n",
    "    template=\"plotly\",\n",
    ")\n",
    "\n",
    "# add a black border around all points\n",
    "fig.update_traces(marker=dict(line=dict(width=1, color=\"Black\")))\n",
    "# make all points slightly larger\n",
    "fig.update_traces(marker=dict(size=12))\n",
    "# increase font size\n",
    "fig.update_layout(font=dict(size=18))\n",
    "fig.update_layout(\n",
    "    width=800,\n",
    "    height=800,\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_qk_via_feature(feature_id, sparse_autoencoder, feature_name = \"\", highlight_tokens = []):\n",
    "    eff_embed = model.W_E + model.blocks[0].mlp(model.blocks[0].ln2(model.W_E[None] + model.blocks[0].attn.b_O))\n",
    "    eff_emb_in_key_space =  eff_embed @ model.W_K[LAYER_IDX,HEAD_IDX] @ sparse_autoencoder.W_dec[feature_id]\n",
    "    feature_unembed = sparse_autoencoder.W_dec[feature_id] @ model.W_Q[LAYER_IDX,HEAD_IDX].T @  model.W_U\n",
    "    # feature_unembed = sparse_autoencoder.W_enc[:,feature_id] @ model.W_Q[LAYER_IDX,HEAD_IDX].T @  model.W_U\n",
    "    \n",
    "    df = pd.DataFrame(dict(\n",
    "        eff_emb_in_key_space=eff_emb_in_key_space[0].detach().cpu().numpy(),\n",
    "        feature_unembed = feature_unembed.detach().cpu().numpy(),\n",
    "        token = [model.tokenizer.decode(i) for i in range(50257)],\n",
    "    ))\n",
    "    \n",
    "    df[\"token_of_interest\"] = df[\"token\"].isin(highlight_tokens)\n",
    "    \n",
    "    # add a column to df with text for the largest 10 values (positive and negative) \n",
    "    # that we can use to label these points\n",
    "    top_10_key = df.sort_values(\"eff_emb_in_key_space\", ascending=False).head(6)\n",
    "    top_10_proj = df.sort_values(\"feature_unembed\", ascending=False).head(6)\n",
    "    \n",
    "    top_10_key[\"text\"] = top_10_key.apply(lambda x: f\"{x['token']}\", axis=1)\n",
    "    top_10_proj[\"text\"] = top_10_proj.apply(lambda x: f\"{x['token']}\", axis=1)\n",
    "    \n",
    "    # Merging the top and bottom points for annotation\n",
    "    points_to_annotate = pd.concat([top_10_key, top_10_proj])\n",
    "\n",
    "    fig = px.scatter(\n",
    "        df,\n",
    "        x=\"eff_emb_in_key_space\",\n",
    "        y = \"feature_unembed\",\n",
    "        color=\"token_of_interest\",\n",
    "        color_continuous_scale=\"RdBu\",\n",
    "        # color=\"score_contributions\",\n",
    "        # text=\"text\",\n",
    "        # opacity=0.3,\n",
    "        hover_data=[\"token\"],\n",
    "        labels=dict(eff_emb_in_key_space=\"Token to Feature Virtual Weight\", feature_unembed=\"Unembed to Feature Virtual Weight\"),\n",
    "        title=f\"Feature {feature_id} {feature_name}\",\n",
    "        template=\"plotly\",\n",
    "        marginal_x=\"histogram\",\n",
    "        marginal_y=\"histogram\",\n",
    "    )\n",
    "    \n",
    "\n",
    "    for _, row in points_to_annotate.iterrows():\n",
    "        fig.add_annotation(x=row['eff_emb_in_key_space'], y=row['feature_unembed'],\n",
    "                           text=row['text'], showarrow=False, arrowhead=1,\n",
    "                           ax=20, ay=-40)\n",
    "\n",
    "    \n",
    "    fig.update_layout(\n",
    "        width=1200,\n",
    "        height=1200,\n",
    "    )\n",
    "    fig.show()\n",
    "    \n",
    "# plot_qk_via_feature(inds[0], sparse_autoencoder, feature_name = \"What's not to suppress here?\")\n",
    "plot_qk_via_feature(3985, sparse_autoencoder, feature_name = \"\", highlight_tokens=model.to_str_tokens(tokens[0,1:]))\n",
    "# plot_qk_via_feature(1102, sparse_autoencoder, feature_name = \"\", highlight_tokens=model.to_str_tokens(tokens[0,1:]))\n",
    "# plot_qk_via_feature(1664, sparse_autoencoder, feature_name = \"\", highlight_tokens=model.to_str_tokens(tokens[0,1:]))\n",
    "# plot_qk_via_feature(3017, sparse_autoencoder, feature_name = \"\", highlight_tokens=model.to_str_tokens(tokens[0,1:]))\n",
    "# plot_qk_via_feature(2282, sparse_autoencoder, feature_name = \"\", highlight_tokens=model.to_str_tokens(tokens[0,1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_qk_via_feature(inds[1], sparse_autoencoder, feature_name = \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_qk_via_feature(2433, sparse_autoencoder, feature_name = \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_qk_via_feature(2688, sparse_autoencoder, feature_name = \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the key proj via the\n",
    "\n",
    "eff_embed.shape\n",
    "\n",
    "\n",
    "def plot_proj_onto_embed_via_key(feature_id, sparse_autoencoder, feature_name = \"\"):\n",
    "    \n",
    "    eff_embed = model.W_E + model.blocks[0].mlp(model.blocks[0].ln2(model.W_E[None] + model.blocks[0].attn.b_O))\n",
    "    \n",
    "    eff_emb_in_key_space =  eff_embed @ model.W_K[LAYER_IDX,HEAD_IDX] @ sparse_autoencoder.W_dec[feature_id]\n",
    "\n",
    "    feature_unembed_df = pd.DataFrame(\n",
    "        eff_emb_in_key_space.T.detach().cpu().numpy(),\n",
    "        columns = [feature_name],\n",
    "        index = [model.tokenizer.decode(i) for i in list(range(50257))]\n",
    "    )\n",
    "\n",
    "    feature_unembed_df = feature_unembed_df.sort_values(feature_name, ascending=False).reset_index().rename(columns={'index': 'token'})\n",
    "    fig = px.bar(feature_unembed_df.head(20).sort_values(feature_name, ascending=True),\n",
    "                 color_continuous_midpoint=0,\n",
    "                 color_continuous_scale=\"RdBu\",\n",
    "            y = 'token', x = feature_name, orientation='h', color = feature_name, hover_data=[feature_name])\n",
    "\n",
    "    fig.update_layout(\n",
    "        width=500,\n",
    "        height=600,\n",
    "    )\n",
    "\n",
    "    # fig.write_image(f\"figures/{str(feature_id)}_{feature_name}.png\")\n",
    "    fig.show()\n",
    "plot_proj_onto_embed_via_key(2688, sparse_autoencoder, feature_name = \"Famous People you Love\")\n",
    "plot_feature_unembed_bar(2688, sparse_autoencoder, feature_name = \"Famous People you Love\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mats_sae_training",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
