{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append(\"../..\")\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from importlib import reload\n",
    "\n",
    "import joseph\n",
    "from joseph.analysis import *\n",
    "from joseph.visualisation import *\n",
    "from joseph.utils import *\n",
    "from joseph.data import *\n",
    "\n",
    "\n",
    "reload(joseph.analysis)\n",
    "reload(joseph.visualisation)\n",
    "reload(joseph.utils)\n",
    "reload(joseph.data)\n",
    "\n",
    "from joseph.analysis import *\n",
    "from joseph.visualisation import *\n",
    "from joseph.utils import *\n",
    "from joseph.data import *\n",
    "\n",
    "# turn torch grad tracking off\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "\n",
    "model = HookedTransformer.from_pretrained(\n",
    "    \"gpt2-small\",\n",
    "    # \"tiny-stories-2L-33M\",\n",
    "    # \"attn-only-2l\",\n",
    "    # center_unembed=True,\n",
    "    # center_writing_weights=True,\n",
    "    # fold_ln=True,\n",
    "    # refactor_factored_attn_matrices=True,\n",
    ")\n",
    "model.set_use_split_qkv_input(True)\n",
    "model.set_use_attn_result(True)\n",
    "\n",
    "\n",
    "# path = \"./artifacts/sparse_autoencoder_gpt2-small_blocks.10.hook_resid_pre_49152:v28/1100001280_sparse_autoencoder_gpt2-small_blocks.10.hook_resid_pre_49152.pt\"\n",
    "# path = \"/Users/josephbloom/GithubRepositories/mats_sae_training/artifacts/sparse_autoencoder_gelu-2l_blocks.0.hook_mlp_out_32768:v40/final_sparse_autoencoder_gelu-2l_blocks.0.hook_mlp_out_32768.pt\"\n",
    "path = \"./artifacts/sparse_autoencoder_gelu-2l_blocks.0.hook_mlp_out_65536:v11/final_sparse_autoencoder_gelu-2l_blocks.0.hook_mlp_out_65536.pt\" # zero init\n",
    "path2 = \"./artifacts/sparse_autoencoder_gelu-2l_blocks.0.hook_mlp_out_65536:v11/final_sparse_autoencoder_gelu-2l_blocks.0.hook_mlp_out_65536.pt\" # geom median init\n",
    "path3 = \"./artifacts/sparse_autoencoder_gelu-2l_blocks.0.hook_mlp_out_65536:v11/final_sparse_autoencoder_gelu-2l_blocks.0.hook_mlp_out_65536.pt\" # mean init\n",
    " \n",
    "sparse_autoencoder = SparseAutoencoder.load_from_pretrained(path)\n",
    "sparse_autoencoder_2 = SparseAutoencoder.load_from_pretrained(path2)\n",
    "sparse_autoencoder_3 = SparseAutoencoder.load_from_pretrained(path3)\n",
    "\n",
    "print(sparse_autoencoder.cfg)\n",
    "\n",
    "\n",
    "# sanity check\n",
    "text = \"Many important transition points in the history of science have been moments when science 'zoomed in.' At these points, we develop a visualization or tool that allows us to see the world in a new level of detail, and a new field of science develops to study the world through this lens.\"\n",
    "model(text, return_type=\"loss\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "run = wandb.init()\n",
    "artifact = run.use_artifact('jbloom/mats_sae_training_language_models_gelu_2l_test/sparse_autoencoder_gelu-2l_blocks.0.hook_mlp_out_65536:v31', type='model')\n",
    "artifact_dir = artifact.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sae_training.utils import LMSparseAutoencoderSessionloader\n",
    "model, sparse_autoencoder, activation_store = LMSparseAutoencoderSessionloader.load_session_from_pretrained(\n",
    "    path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "from sae_training.geom_median.src import geom_median\n",
    "reload(geom_median)\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    \n",
    "    # all_activations = activation_store.storage_buffer.detach()\n",
    "\n",
    "    # out = geom_median.torch.compute_geometric_median(\n",
    "    #     activation_store.storage_buffer.detach().cpu(), skip_typechecks=True, maxiter=100, per_component=False,\n",
    "    #     ).median\n",
    "    \n",
    "    # get the distance between the median and each activation\n",
    "    dataset_mean = all_activations.mean(dim=0).detach().cpu()\n",
    "    dataset_mean_distances = (all_activations.detach().cpu() - dataset_mean).median(0).values\n",
    "    dataset_mean_distances_mean = dataset_mean_distances.mean().item()\n",
    "    \n",
    "    \n",
    "    med_distances = (all_activations.detach().cpu() -  out.to(\"mps\").detach().cpu()).median(0).values\n",
    "    med_distances_mean = med_distances.mean().item()\n",
    "    \n",
    "    dec_distance = (all_activations.detach().cpu() -  sparse_autoencoder.b_dec.detach().cpu()).median(0).values\n",
    "    dec_distance_mean = dec_distance.mean().item()\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    print(med_distances_mean, dec_distance_mean, dataset_mean_distances_mean)\n",
    "\n",
    "    tmp = pd.DataFrame(\n",
    "        {\n",
    "            \"dataset_mean\": dataset_mean_distances,\n",
    "            \"median\": med_distances,\n",
    "            \"decoder\": dec_distance,\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "    fig = px.line(\n",
    "        tmp,\n",
    "        title = \"Median Distances in the Canonical Basis\",\n",
    "    ).show()\n",
    "         \n",
    "         \n",
    "    # get the distance between the median and each activation\n",
    "    dataset_mean = all_activations.mean(dim=0).detach().cpu()\n",
    "    dataset_mean_distances = (all_activations.detach().cpu() - dataset_mean).mean(0)\n",
    "    dataset_mean_distances_mean = dataset_mean_distances.mean().item()\n",
    "    \n",
    "    \n",
    "    med_distances = (all_activations.detach().cpu() -  out.to(\"mps\").detach().cpu()).mean(0)\n",
    "    med_distances_mean = med_distances.mean().item()\n",
    "    \n",
    "    dec_distance = (all_activations.detach().cpu() -  sparse_autoencoder.b_dec.detach().cpu()).mean(0)\n",
    "    dec_distance_mean = dec_distance.mean().item()\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    print(med_distances_mean, dec_distance_mean, dataset_mean_distances_mean)\n",
    "\n",
    "    tmp = pd.DataFrame(\n",
    "        {\n",
    "            \"dataset_mean\": dataset_mean_distances,\n",
    "            \"median\": med_distances,\n",
    "            \"decoder\": dec_distance,\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "    fig = px.line(\n",
    "        tmp,\n",
    "        title = \"Mean Distances in the Canonical Basis\",\n",
    "    ).show()\n",
    "         \n",
    "    tmp = pd.DataFrame(\n",
    "        {\n",
    "            \"dataset_mean\": dataset_mean,\n",
    "            \"median\": out.detach().cpu(),\n",
    "            \"decoder\": sparse_autoencoder.b_dec.detach().cpu(),\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "    fig = px.line(\n",
    "        tmp,\n",
    "        title = \"Mean, Median, and Decoder in the Canonical Basis\",\n",
    "    ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_activation_mean = all_activations.median(0).values.abs().detach().cpu()\n",
    "outlier_dims = (abs_activation_mean > 0.8).nonzero().squeeze().tolist()\n",
    "fig = px.line(abs_activation_mean, title = \"median activations\")\n",
    "# add vertical lines for outlier dims\n",
    "for dim in outlier_dims:\n",
    "    fig.add_vline(dim, line_width=1, line_dash=\"dash\", line_color=\"red\")\n",
    "    \n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.line(activations[0].detach().cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.strip(all_activations[:, outlier_dims].detach().cpu(), \n",
    "         color=\"variable\",\n",
    "         orientation=\"h\",\n",
    "         title = \"Outlier Dimension Activations\").show()\n",
    "for dim in outlier_dims:\n",
    "    print((all_activations[:, dim].abs() > 2).float().mean().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from umap import UMAP\n",
    "import plotly.graph_objects as go\n",
    "# umap = UMAP(n_components=2, n_neighbors=10, min_dist=0.1, metric=\"cosine\")\n",
    "\n",
    "# umap.fit(all_activations.detach().cpu().numpy())\n",
    "\n",
    "umap_embedding = umap.embedding_\n",
    "\n",
    "# project the median and decoder onto the umap embedding\n",
    "median_embedding = umap.transform(out.unsqueeze(0).to(\"cpu\").detach().numpy())\n",
    "decoder_embedding_zero_init = umap.transform(sparse_autoencoder.b_dec.unsqueeze(0).to(\"cpu\").detach().numpy())\n",
    "decoder_embedding_median_init = umap.transform(sparse_autoencoder_2.b_dec.unsqueeze(0).to(\"cpu\").detach().numpy())\n",
    "decoder_embedding_mean_init = umap.transform(sparse_autoencoder_3.b_dec.unsqueeze(0).to(\"cpu\").detach().numpy())\n",
    "mean_embedding = umap.transform(dataset_mean.unsqueeze(0).detach().cpu().numpy())\n",
    "\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "\n",
    "fig.add_scatter(\n",
    "    x=umap_embedding[:, 0],\n",
    "    y=umap_embedding[:, 1],\n",
    "    mode=\"markers\",\n",
    "    name=\"activations\",\n",
    "    marker=dict(color=\"grey\"),\n",
    "    opacity=0.3,\n",
    ")\n",
    "\n",
    "fig.layout.showlegend = True\n",
    "fig.add_scatter(\n",
    "    x=median_embedding[:, 0],\n",
    "    y=median_embedding[:, 1],\n",
    "    mode=\"markers\",\n",
    "    name=\"median\",\n",
    "    marker=dict(color=\"red\"),\n",
    "    \n",
    ")\n",
    "\n",
    "fig.add_scatter(\n",
    "    x=decoder_embedding_zero_init[:, 0],\n",
    "    y=decoder_embedding_zero_init[:, 1],\n",
    "    mode=\"markers\",\n",
    "    name=\"decoder (0 init run)\",\n",
    "    marker=dict(color=\"yellow\"),\n",
    ")\n",
    "\n",
    "fig.add_scatter(\n",
    "    x=decoder_embedding_median_init[:, 0],\n",
    "    y=decoder_embedding_median_init[:, 1],\n",
    "    mode=\"markers\",\n",
    "    name=\"decoder (median init run)\",\n",
    "    marker=dict(color=\"green\"),\n",
    ")\n",
    "\n",
    "fig.add_scatter(\n",
    "    x=decoder_embedding_mean_init[:, 0],\n",
    "    y=decoder_embedding_mean_init[:, 1],\n",
    "    mode=\"markers\",\n",
    "    name=\"decoder (mean init run)\",\n",
    "    marker=dict(color=\"brown\"),\n",
    ")\n",
    "\n",
    "fig.add_scatter(\n",
    "    x=mean_embedding[:, 0],\n",
    "    y=mean_embedding[:, 1],\n",
    "    mode=\"markers\",\n",
    "    name=\"mean\",\n",
    "    marker=dict(color=\"blue\"),\n",
    ")\n",
    "fig.update_layout(\n",
    "    width=1200,\n",
    "    height=800)\n",
    "fig.show()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mats_sae_training",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
