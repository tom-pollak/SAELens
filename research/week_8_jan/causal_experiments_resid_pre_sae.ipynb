{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Causal Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append(\"../..\")\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from importlib import reload\n",
    "from tqdm import tqdm\n",
    "\n",
    "import joseph\n",
    "from joseph.analysis import *\n",
    "from joseph.visualisation import *\n",
    "from joseph.utils import *\n",
    "from joseph.data import *\n",
    "\n",
    "\n",
    "reload(joseph.analysis)\n",
    "reload(joseph.visualisation)\n",
    "reload(joseph.utils)\n",
    "reload(joseph.data)\n",
    "\n",
    "from joseph.analysis import *\n",
    "from joseph.visualisation import *\n",
    "from joseph.utils import *\n",
    "from joseph.data import *\n",
    "\n",
    "# turn torch grad tracking off\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "\n",
    "model = HookedTransformer.from_pretrained(\n",
    "    \"gpt2-small\",\n",
    "    # \"tiny-stories-2L-33M\",\n",
    "    # \"attn-only-2l\",\n",
    "    # center_unembed=True,\n",
    "    # center_writing_weights=True,\n",
    "    # fold_ln=True,\n",
    "    # refactor_factored_attn_matrices=True,\n",
    ")\n",
    "model.set_use_split_qkv_input(True)\n",
    "model.set_use_attn_result(True)\n",
    "\n",
    "\n",
    "path = \"./artifacts/sparse_autoencoder_gpt2-small_blocks.10.hook_resid_pre_49152:v28/1100001280_sparse_autoencoder_gpt2-small_blocks.10.hook_resid_pre_49152.pt\"\n",
    "sparse_autoencoder = SparseAutoencoder.load_from_pretrained(path)\n",
    "\n",
    "print(sparse_autoencoder.cfg)\n",
    "\n",
    "\n",
    "# sanity check\n",
    "text = \"Many important transition points in the history of science have been moments when science 'zoomed in.' At these points, we develop a visualization or tool that allows us to see the world in a new level of detail, and a new field of science develops to study the world through this lens.\"\n",
    "model(text, return_type=\"loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dev / Single Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"When John and Mary went to the shops, John gave the shopping to\"\n",
    "answer = \" Mary\"\n",
    "# prompt = \"All's fair in love and\"\n",
    "# answer = \" war\"\n",
    "# prompt = \" The cat is cute. The dog is\"\n",
    "# prompt = \" Alice, with her keen intelligence and artistic talent, discussed philosophy with Bob, who shared her intellect and also possessed remarkable culinary skills, while\"\n",
    "# answer = \" cute\"\n",
    "model.reset_hooks()\n",
    "utils.test_prompt(prompt, answer, model)\n",
    "\n",
    "HEAD_HOOK_RESULT_NAME = \"blocks.10.attn.hook_z\"\n",
    "LAYER_IDX = sparse_autoencoder.cfg.hook_point_layer\n",
    "HEAD_IDX = 7\n",
    "def hook_to_ablate_head(head_output: Float[Tensor, \"batch seq_len head_idx d_head\"], hook: HookPoint, head = (LAYER_IDX, HEAD_IDX)):\n",
    "    print(hook.layer(), hook.name)\n",
    "    assert head[0] == hook.layer(), f\"{head[0]} != {hook.layer()}\"\n",
    "    assert (\"result\" in hook.name) or (\"q\" in hook.name) or (\"z\" in hook.name)\n",
    "    head_output[:, :, head[1], :] = 0\n",
    "    return head_output\n",
    "\n",
    "with model.hooks(fwd_hooks=[(HEAD_HOOK_RESULT_NAME, hook_to_ablate_head)]):\n",
    "    utils.test_prompt(prompt, answer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joseph\n",
    "reload(joseph.analysis)\n",
    "from joseph.analysis import *\n",
    "\n",
    "\n",
    "token_df, original_cache, cache_reconstructed_query, feature_acts = eval_prompt([prompt + answer], model, sparse_autoencoder, head_idx_override=7)\n",
    "print(token_df.columns)\n",
    "filter_cols = [\"str_tokens\", \"unique_token\", \"context\", \"batch\", \"pos\", \"label\", \"loss\", \"loss_diff\", \"mse_loss\", \"num_active_features\", \"explained_variance\", \"kl_divergence\",\n",
    "               \"top_k_features\"]\n",
    "token_df[filter_cols].tail().style.background_gradient(\n",
    "    subset=[\"loss_diff\", \"mse_loss\",\"explained_variance\", \"num_active_features\", \"kl_divergence\"],\n",
    "    cmap=\"coolwarm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_attn_key(cache, token_df, layer_idx, head_idx):\n",
    "    '''\n",
    "    Given some cache and token_df, return a tensor with the key vectors\n",
    "    which were most attended to by the head.\n",
    "    \n",
    "    '''\n",
    "    keys = cache[utils.get_act_name(\"k\",layer_idx)][:, :, head_idx, :].cpu()\n",
    "    pos = torch.tensor(token_df[\"max_idx_pos\"].values, dtype=torch.long)\n",
    "    keys = keys[0, pos, :]\n",
    "    return keys\n",
    "\n",
    "keys = get_max_attn_key(original_cache, token_df, 10, 7)\n",
    "print(keys.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_attn(patterns, token_df, title=\"\", facet_col_labels = [\"Original\", \"Reconstructed\"]):\n",
    "    '''\n",
    "    # patterns_original = cache[utils.get_act_name(\"pattern\", LAYER_IDX)][0,HEAD_IDX].detach().cpu()\n",
    "    # patterns_reconstructed = cache_reconstructed_query[utils.get_act_name(\"pattern\", LAYER_IDX)][0,HEAD_IDX].detach().cpu()\n",
    "    patterns_original = cache[utils.get_act_name(\"attn_scores\", LAYER_IDX)][0,HEAD_IDX].detach().cpu()\n",
    "    patterns_reconstructed = cache_reconstructed_query[utils.get_act_name(\"attn_scores\", LAYER_IDX)][0,HEAD_IDX].detach().cpu()\n",
    "    both_patterns = torch.stack([patterns_original, patterns_reconstructed])\n",
    "    plot_attn(both_patterns.detach().cpu(), token_df, title=\"Original and Reconstructed Attention Distribution\")\n",
    "    \n",
    "    '''\n",
    "    fig = px.imshow(patterns, text_auto=\".2f\", title=title,\n",
    "                    facet_col=0,\n",
    "                    color_continuous_midpoint=0,\n",
    "                    color_continuous_scale=\"RdBu\",\n",
    "                    )\n",
    "    \n",
    "    tickvals = np.arange(patterns.shape[2])\n",
    "    ticktext = token_df[\"unique_token\"].tolist()\n",
    "    \n",
    "    # add tokens as x-ticks and y-ticks, for each facet\n",
    "    # Update x-ticks and y-ticks for each facet\n",
    "    for i in range(len(facet_col_labels)):\n",
    "        fig.update_xaxes(\n",
    "            dict(tickmode='array', tickvals=tickvals, ticktext=ticktext),\n",
    "            row=1, col=i+1\n",
    "        )\n",
    "        fig.update_yaxes(\n",
    "            dict(tickmode='array', tickvals=tickvals, ticktext=ticktext),\n",
    "            row=1, col=i+1\n",
    "        )\n",
    "    \n",
    "    \n",
    "    # add facet col labels:\n",
    "    for i, label in enumerate(facet_col_labels):\n",
    "        fig.layout.annotations[i].text = label\n",
    "        fig.layout.annotations[i].font.size = 20\n",
    "        \n",
    "    fig.update_layout(\n",
    "        width=1200,\n",
    "        height=800,\n",
    "    )\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "LAYER_IDX = sparse_autoencoder.cfg.hook_point_layer\n",
    "HEAD_IDX = 7\n",
    "patterns_original = original_cache[utils.get_act_name(\"attn_scores\", LAYER_IDX)][0,HEAD_IDX].detach().cpu()\n",
    "patterns_reconstructed = cache_reconstructed_query[utils.get_act_name(\"attn_scores\", LAYER_IDX)][0,HEAD_IDX].detach().cpu()\n",
    "both_patterns = torch.stack([patterns_original, patterns_reconstructed])\n",
    "plot_attn(both_patterns.detach().cpu(), token_df, title=\"Original and Reconstructed Attention Distribution\")\n",
    "# patterns_original = original_cache[utils.get_act_name(\"pattern\", LAYER_IDX)][0,HEAD_IDX].detach().cpu()\n",
    "# patterns_reconstructed = cache_reconstructed_query[utils.get_act_name(\"pattern\", LAYER_IDX)][0,HEAD_IDX].detach().cpu()\n",
    "# both_patterns = torch.stack([patterns_original, patterns_reconstructed])\n",
    "# plot_attn(both_patterns.detach().cpu(), token_df, title=\"Original and Reconstructed Attention Distribution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "layer_idx = 10\n",
    "head_idx = 7\n",
    "tokens =model.to_tokens([prompt + answer])\n",
    "\n",
    "original_act = original_cache[sparse_autoencoder.cfg.hook_point]\n",
    "# token_df[\"q_norm\"] = torch.norm(original_act, dim=-1)[:,1:].flatten().tolist()\n",
    "sae_out, feature_acts, _, mse_loss, _ = sparse_autoencoder(original_act)\n",
    "head_hook_query_name = utils.get_act_name(\"q\", layer_idx)\n",
    "head_hook_resid_name = utils.get_act_name(\"resid_pre\", layer_idx)\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_top_k_sae_approximation(sparse_autoencoder, feature_acts, top_k):\n",
    "    top_k_features = torch.topk(feature_acts, top_k, dim=2, sorted=False)\n",
    "    feature_acts_top_k = torch.zeros_like(feature_acts)\n",
    "    feature_acts_top_k[:, :, top_k_features.indices[0]] = feature_acts[:, :, top_k_features.indices[0]]\n",
    "    new_sae_out = (feature_acts_top_k @ sparse_autoencoder.W_dec) + sparse_autoencoder.b_dec\n",
    "    return new_sae_out\n",
    "\n",
    "# get the top k features by activation, and construct a new sae out \n",
    "for top_k in tqdm([1,3,5,10,30,50,60,100]):\n",
    "    new_sae_out = get_top_k_sae_approximation(sparse_autoencoder, feature_acts, top_k)\n",
    "    print((sae_out - new_sae_out).norm().item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# need to generate query\n",
    "def replacement_hook(resid_pre, hook, new_resid_pre=new_sae_out):\n",
    "    return new_resid_pre\n",
    "new_sae_out = get_top_k_sae_approximation(sparse_autoencoder, feature_acts, top_k=66)\n",
    "\n",
    "model.reset_hooks()\n",
    "with model.hooks(fwd_hooks=[(head_hook_resid_name, replacement_hook)]):\n",
    "    _, top_k_sae_out_cache = model.run_with_cache(tokens, return_type=\"loss\", loss_per_token=True)\n",
    "    top_k_acts_queries = top_k_sae_out_cache[head_hook_query_name][:,:,head_idx]\n",
    "\n",
    "print(top_k_acts_queries.shape)\n",
    "print((sae_out - new_sae_out).norm().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAYER_IDX =10\n",
    "HEAD_IDX = 7\n",
    "patterns_original = original_cache[utils.get_act_name(\"attn_scores\", LAYER_IDX)][0,HEAD_IDX].detach().cpu()\n",
    "patterns_reconstructed = cache_reconstructed_query[utils.get_act_name(\"attn_scores\", LAYER_IDX)][0,HEAD_IDX].detach().cpu()\n",
    "both_patterns = torch.stack([patterns_original, patterns_reconstructed])\n",
    "plot_attn(both_patterns.detach().cpu(), token_df, title=\"Original and Reconstructed Attention Distribution\")\n",
    "\n",
    "\n",
    "patterns_original = cache_reconstructed_query[utils.get_act_name(\"attn_scores\", LAYER_IDX)][0,HEAD_IDX].detach().cpu()\n",
    "patterns_reconstructed = top_k_sae_out_cache[utils.get_act_name(\"attn_scores\", LAYER_IDX)][0,HEAD_IDX].detach().cpu()\n",
    "both_patterns = torch.stack([patterns_original, patterns_reconstructed])\n",
    "plot_attn(both_patterns.detach().cpu(), token_df, title=\"Original and Reconstructed Attention Distribution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_webtext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_token_list = []\n",
    "loss_list = []\n",
    "ablated_loss_list = []\n",
    "# data = get_webtext()\n",
    "\n",
    "NUM_PROMPTS = 200\n",
    "# MAX_PROMPT_LEN = 100\n",
    "# BATCH_SIZE = 10\n",
    "dataframe_list = []\n",
    "feature_acts_list = []\n",
    "with torch.no_grad():\n",
    "    for i in tqdm(range(NUM_PROMPTS)):\n",
    "        \n",
    "        # Get Token Data\n",
    "        prompt = model.to_string(model.to_tokens(data[i])[0,:128])\n",
    "        token_df, _, _, feature_acts = eval_prompt(prompt, model, sparse_autoencoder, head_idx_override=7)\n",
    "        feature_acts_list.append(feature_acts)\n",
    "        dataframe_list.append(token_df)\n",
    "        \n",
    "all_token_df = pd.concat(dataframe_list)\n",
    "all_token_df.reset_index(drop=True)\n",
    "all_token_features = torch.cat(feature_acts_list)\n",
    "\n",
    "print(all_token_df.shape)\n",
    "print(all_token_df.columns)\n",
    "all_token_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mats_sae_training",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
