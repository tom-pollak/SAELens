{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini-Sprint: SAEs for Semantics of Copy-Suppression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import wandb\n",
    "\n",
    "from sae_training.config import LanguageModelSAERunnerConfig\n",
    "from sae_training.lm_runner import language_model_sae_runner\n",
    "from sae_training.utils import LMSparseAutoencoderSessionloader\n",
    "\n",
    "# run = wandb.init()\n",
    "# artifact = run.use_artifact('jbloom/mats_sae_training_gpt2_small/sparse_autoencoder_gpt2-small_blocks.10.hook_resid_pre_24576:v36', type='model')\n",
    "# artifact_dir = artifact.download()\n",
    "\n",
    "\n",
    "path =\"artifacts/sparse_autoencoder_gpt2-small_blocks.10.hook_resid_pre_24576:v36/final_sparse_autoencoder_gpt2-small_blocks.10.hook_resid_pre_24576.pt\"\n",
    "model, sparse_autoencoder, activations_loader = LMSparseAutoencoderSessionloader.load_session_from_pretrained(\n",
    "    path\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sae_training.train_sae_on_language_model import get_recons_loss\n",
    "\n",
    "score, loss, recons_loss, zero_abl_loss = get_recons_loss(\n",
    "    sparse_autoencoder, model, activations_loader, num_batches=10,\n",
    ")\n",
    "\n",
    "print(score, loss, recons_loss, zero_abl_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer_lens import utils\n",
    "import torch\n",
    "import einops\n",
    "import plotly.express as px\n",
    "\n",
    "import pandas as pd\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "from jaxtyping import Float\n",
    "from transformer_lens import ActivationCache\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "from transformer_lens import utils\n",
    "\n",
    "example_prompt = \"After John and Mary went to the store, John gave a bottle of milk to\"\n",
    "example_answer = \" Mary\"\n",
    "utils.test_prompt(example_prompt, example_answer, model, prepend_bos=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"mps\"\n",
    "prompt_format = [\n",
    "    \"When John and Mary went to the shops,{} gave the bag to\",\n",
    "    \"When Tom and James went to the park,{} gave the ball to\",\n",
    "    \"When Dan and Sid went to the shops,{} gave an apple to\",\n",
    "    \"After Martin and Amy went to the park,{} gave a drink to\",\n",
    "]\n",
    "names = [\n",
    "    (\" Mary\", \" John\"),\n",
    "    (\" Tom\", \" James\"),\n",
    "    (\" Dan\", \" Sid\"),\n",
    "    (\" Martin\", \" Amy\"),\n",
    "]\n",
    "# List of prompts\n",
    "prompts = []\n",
    "# List of answers, in the format (correct, incorrect)\n",
    "answers = []\n",
    "# List of the token (ie an integer) corresponding to each answer, in the format (correct_token, incorrect_token)\n",
    "answer_tokens = []\n",
    "for i in range(len(prompt_format)):\n",
    "    for j in range(2):\n",
    "        answers.append((names[i][j], names[i][1 - j]))\n",
    "        answer_tokens.append(\n",
    "            (\n",
    "                model.to_single_token(answers[-1][0]),\n",
    "                model.to_single_token(answers[-1][1]),\n",
    "            )\n",
    "        )\n",
    "        # Insert the *incorrect* answer to the prompt, making the correct answer the indirect object.\n",
    "        prompts.append(prompt_format[i].format(answers[-1][1]))\n",
    "answer_tokens = torch.tensor(answer_tokens).to(device)\n",
    "print(prompts)\n",
    "print(answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = model.to_tokens(prompts, prepend_bos=True)\n",
    "original_logits, cache = model.run_with_cache(tokens)\n",
    "\n",
    "\n",
    "def logits_to_ave_logit_diff(logits, answer_tokens, per_prompt=False):\n",
    "    # Only the final logits are relevant for the answer\n",
    "    final_logits = logits[:, -1, :]\n",
    "    answer_logits = final_logits.gather(dim=-1, index=answer_tokens)\n",
    "    answer_logit_diff = answer_logits[:, 0] - answer_logits[:, 1]\n",
    "    if per_prompt:\n",
    "        return answer_logit_diff\n",
    "    else:\n",
    "        return answer_logit_diff.mean()\n",
    "\n",
    "per_prompt_original_logit_diff = logits_to_ave_logit_diff(original_logits, answer_tokens, per_prompt=True).detach()\n",
    "\n",
    "\n",
    "def replacement_hook(resid, hook, pos, encoder):\n",
    "    resid[:,pos] = encoder(resid[:,pos])[0]\n",
    "    return resid\n",
    "\n",
    "\n",
    "model.reset_hooks()\n",
    "hook_fn = partial(replacement_hook, encoder=sparse_autoencoder, pos = -1)\n",
    "model.add_hook(utils.get_act_name(\"resid_pre\", 10), hook_fn, prepend=True)\n",
    "hook_fn = partial(replacement_hook, encoder=sparse_autoencoder, pos = 4)\n",
    "model.add_hook(utils.get_act_name(\"resid_pre\", 10), hook_fn, prepend=True)\n",
    "tokens = model.to_tokens(prompts, prepend_bos=True)\n",
    "new_logits, new_cache = model.run_with_cache(tokens)\n",
    "model.reset_hooks()\n",
    "\n",
    "per_prompt_new_logit_diff = logits_to_ave_logit_diff(new_logits, answer_tokens, per_prompt=True).detach()\n",
    "\n",
    "px.bar(\n",
    "    y = per_prompt_original_logit_diff.cpu().numpy() - per_prompt_new_logit_diff.cpu().numpy(),\n",
    "    labels={\"x\": \"Original Logit Difference\", \"y\": \"New Logit Difference\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So basically the reconstructi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from circuitsvis.attention import attention_patterns\n",
    "\n",
    "patterns = cache[\"blocks.10.attn.hook_pattern\"][0]\n",
    "attention_patterns(tokens=model.to_str_tokens(prompts[0]), attention=patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = new_cache[\"blocks.10.attn.hook_pattern\"][0]\n",
    "attention_patterns(tokens=model.to_str_tokens(prompts[0]), attention=patterns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drill down on this sentence \"When John and Mary went to the shops, John gave the bag to\".\n",
    "- The correct answer is Mary. \n",
    "- L9H9 contributes directly to this. \n",
    "- L10H7 suppresses Mary (via copy-suppression)\n",
    "  - L10H7 Attends to the previous instance of Mary.\n",
    "  - And inhibits mary.\n",
    "\n",
    "\n",
    "L10H7 attends to Mary from -1 to 4 before inhibiting Mary. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "answer_residual_directions = model.tokens_to_residual_directions(answer_tokens)\n",
    "logit_diff_directions = (\n",
    "    answer_residual_directions[:, 0] - answer_residual_directions[:, 1]\n",
    ")\n",
    "\n",
    "# cache syntax - resid_post is the residual stream at the end of the layer, -1 gets the final layer. The general syntax is [activation_name, layer_index, sub_layer_type].\n",
    "final_residual_stream = cache[\"resid_post\", -1]\n",
    "print(\"Final residual stream shape:\", final_residual_stream.shape)\n",
    "final_token_residual_stream = final_residual_stream[:, -1, :]\n",
    "# Apply LayerNorm scaling\n",
    "# pos_slice is the subset of the positions we take - here the final token of each prompt\n",
    "scaled_final_token_residual_stream = cache.apply_ln_to_stack(\n",
    "    final_token_residual_stream, layer=-1, pos_slice=-1\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "def residual_stack_to_logit_diff(\n",
    "    residual_stack: Float[torch.Tensor, \"components batch d_model\"],\n",
    "    cache: ActivationCache,\n",
    ") -> float:\n",
    "    scaled_residual_stack = cache.apply_ln_to_stack(\n",
    "        residual_stack, layer=-1, pos_slice=-1\n",
    "    )\n",
    "    return einsum(\n",
    "        \"... batch d_model, batch d_model -> ...\",\n",
    "        scaled_residual_stack,\n",
    "        logit_diff_directions,\n",
    "    ) / len(prompts)\n",
    "\n",
    "\n",
    "from fancy_einsum import einsum\n",
    "\n",
    "accumulated_residual, labels = cache.accumulated_resid(\n",
    "    layer=-1, incl_mid=True, pos_slice=-1, return_labels=True\n",
    ")\n",
    "\n",
    "\n",
    "logit_lens_logit_diffs = residual_stack_to_logit_diff(accumulated_residual, cache)\n",
    "fig = px.line(\n",
    "    y=logit_lens_logit_diffs.detach().cpu().numpy(),\n",
    "    x=np.arange(model.cfg.n_layers * 2 + 1) / 2,\n",
    "    hover_name=labels,\n",
    "    title=\"Logit Difference From Accumulate Residual Stream\",\n",
    ")\n",
    "fig.update_layout(width=800, height=400)\n",
    "fig.show()\n",
    "\n",
    "per_head_residual, labels = cache.stack_head_results(\n",
    "    layer=-1, pos_slice=-1, return_labels=True\n",
    ")\n",
    "per_head_logit_diffs = residual_stack_to_logit_diff(per_head_residual, cache)\n",
    "per_head_logit_diffs = einops.rearrange(\n",
    "    per_head_logit_diffs,\n",
    "    \"(layer head_index) -> layer head_index\",\n",
    "    layer=model.cfg.n_layers,\n",
    "    head_index=model.cfg.n_heads,\n",
    ")\n",
    "fig = px.imshow(\n",
    "    per_head_logit_diffs.detach().cpu().numpy(),\n",
    "    labels={\"x\": \"Head\", \"y\": \"Layer\"},\n",
    "    title=\"Logit Difference From Each Head\",\n",
    "    color_continuous_scale=\"RdBu\",\n",
    "    color_continuous_midpoint=0,\n",
    ")\n",
    "fig.update_layout(width=800, height=400)\n",
    "fig.show()\n",
    "\n",
    "\n",
    "accumulated_residual, labels = new_cache.accumulated_resid(\n",
    "    layer=-1, incl_mid=True, pos_slice=-1, return_labels=True\n",
    ")\n",
    "\n",
    "\n",
    "logit_lens_logit_diffs = residual_stack_to_logit_diff(accumulated_residual, new_cache)\n",
    "fig = px.line(\n",
    "    y=logit_lens_logit_diffs.detach().cpu().numpy(),\n",
    "    x=np.arange(model.cfg.n_layers * 2 + 1) / 2,\n",
    "    hover_name=labels,\n",
    "    title=\"Logit Difference From Accumulate Residual Stream\",\n",
    ")\n",
    "fig.update_layout(width=800, height=400)\n",
    "fig.show()\n",
    "\n",
    "per_head_residual, labels = cache.stack_head_results(\n",
    "    layer=-1, pos_slice=-1, return_labels=True\n",
    ")\n",
    "per_head_logit_diffs = residual_stack_to_logit_diff(per_head_residual, new_cache)\n",
    "per_head_logit_diffs = einops.rearrange(\n",
    "    per_head_logit_diffs,\n",
    "    \"(layer head_index) -> layer head_index\",\n",
    "    layer=model.cfg.n_layers,\n",
    "    head_index=model.cfg.n_heads,\n",
    ")\n",
    "fig = px.imshow(\n",
    "    per_head_logit_diffs.detach().cpu().numpy(),\n",
    "    labels={\"x\": \"Head\", \"y\": \"Layer\"},\n",
    "    title=\"Logit Difference From Each Head\",\n",
    "    color_continuous_scale=\"RdBu\",\n",
    "    color_continuous_midpoint=0,\n",
    ")\n",
    "fig.update_layout(width=800, height=400)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which features causes L10H7 to attend to Mary?\n",
    "\n",
    "Hypothesis 1: Features in resid_pre 10 that unembed as Mary will also be causally responsible for attention to Mary.\n",
    "Experiment: Rank Features by how much the decoder direction activates the Mary token and compare these to the contribution they make to the attn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_logits, cache = model.run_with_cache(tokens)\n",
    "sae_out, feature_acts, loss, mse_loss, l1_loss = sparse_autoencoder(cache[\"blocks.10.hook_resid_pre\"][0])\n",
    "print(mse_loss)\n",
    "\n",
    "(feature_acts > 0).float().sum(1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sae_qk_circuit = sparse_autoencoder.W_dec @ model.blocks[10].attn.QK[7] @ sparse_autoencoder.W_dec.T\n",
    "sae_qk_circuit = sae_qk_circuit.A @ sae_qk_circuit.B\n",
    "sae_qk_circuit = sae_qk_circuit.detach().cpu().numpy()\n",
    "print(sae_qk_circuit.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.histogram(sae_qk_circuit.flatten()[:10000]) # not very sparse!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_autoencoder.W_enc.norm(dim=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_autoencoder.W_dec.norm(dim=1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = 0\n",
    "key_resid_pre_feature_acts = sparse_autoencoder(cache[\"blocks.10.hook_resid_pre\"][example,4])[1]\n",
    "query_resid_pre_feature_acts = sparse_autoencoder(cache[\"blocks.10.hook_resid_pre\"][example,-1])[1]\n",
    "sae_qk_circuit_instance = (key_resid_pre_feature_acts)[:, None] * sparse_autoencoder.W_dec * sparse_autoencoder.W_dec @ model.blocks[10].attn.QK[7] @ ((query_resid_pre_feature_acts)[:, None] * sparse_autoencoder.W_dec).T\n",
    "(sae_qk_circuit_instance.AB > 0).sum()\n",
    "\n",
    "values, indices = torch.topk(sae_qk_circuit_instance.AB.detach().flatten(),25)\n",
    "d_enc = sparse_autoencoder.cfg.d_sae\n",
    "start_topk_ind = (indices // d_enc)\n",
    "end_topk_ind = (indices % d_enc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"start\": start_topk_ind.detach().cpu(), \"end\": end_topk_ind.detach().cpu(), \"value\": values.detach().cpu()})\n",
    "df.start = df.start.map(lambda x: f\"start_{x}\")\n",
    "df.end = df.end.map(lambda x: f\"end_{x}\")\n",
    "px.bar(df, y=\"value\", color=\"value\", hover_data=[\"start\", \"end\"], color_continuous_midpoint=0, color_continuous_scale=\"RdBu\").show()\n",
    "\n",
    "# Create the pivoted DataFrame\n",
    "pivot_df = df.pivot(index=\"start\", columns=\"end\", values=\"value\").fillna(0)\n",
    "\n",
    "# Use the index and columns of the pivoted DataFrame for tick labels\n",
    "px.imshow(pivot_df.values,\n",
    "          labels={\"x\": \"End\", \"y\": \"Start\"},\n",
    "          x=pivot_df.columns,\n",
    "          y=pivot_df.index,\n",
    "          title=\"SAE QK Circuit\",\n",
    "          color_continuous_scale=\"RdBu\",\n",
    "          color_continuous_midpoint=0\n",
    "         ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_embed_bar(feature_id, sparse_autoencoder, feature_name = \"\"):\n",
    "\n",
    "    norm_embed = model.W_E / model.W_E.norm(dim=1)[:, None]\n",
    "    feature_unembed = sparse_autoencoder.W_dec[feature_id] @ norm_embed.T\n",
    "    # torch.topk(unembed_4795,10)\n",
    "\n",
    "    feature_unembed_df = pd.DataFrame(\n",
    "        feature_unembed.detach().cpu().numpy(),\n",
    "        columns = [feature_name],\n",
    "        index = [model.tokenizer.decode(i) for i in list(range(50257))]\n",
    "    )\n",
    "\n",
    "    feature_unembed_df = feature_unembed_df.sort_values(feature_name, ascending=False).reset_index().rename(columns={'index': 'token'})\n",
    "    fig = px.bar(feature_unembed_df.head(20).sort_values(feature_name, ascending=True),\n",
    "            y = 'token', x = feature_name, orientation='h', color = feature_name, hover_data=[feature_name],\n",
    "                color_continuous_midpoint=0,\n",
    "                 color_continuous_scale=\"RdBu\",)\n",
    "\n",
    "    fig.update_layout(\n",
    "        width=500,\n",
    "        height=600,\n",
    "    )\n",
    "\n",
    "    fig.show()\n",
    "    \n",
    "def plot_feature_unembed_bar(feature_id, sparse_autoencoder, feature_name = \"\"):\n",
    "    \n",
    "    norm_unembed = model.W_U / model.W_U.norm(dim=0)[None: None]\n",
    "    feature_unembed = sparse_autoencoder.W_dec[feature_id] @ norm_unembed\n",
    "    # torch.topk(unembed_4795,10)\n",
    "\n",
    "    feature_unembed_df = pd.DataFrame(\n",
    "        feature_unembed.detach().cpu().numpy(),\n",
    "        columns = [feature_name],\n",
    "        index = [model.tokenizer.decode(i) for i in list(range(50257))]\n",
    "    )\n",
    "\n",
    "    feature_unembed_df = feature_unembed_df.sort_values(feature_name, ascending=False).reset_index().rename(columns={'index': 'token'})\n",
    "    fig = px.bar(feature_unembed_df.head(20).sort_values(feature_name, ascending=True),\n",
    "                 color_continuous_midpoint=0,\n",
    "                 color_continuous_scale=\"RdBu\",\n",
    "            y = 'token', x = feature_name, orientation='h', color = feature_name, hover_data=[feature_name])\n",
    "\n",
    "    fig.update_layout(\n",
    "        width=500,\n",
    "        height=600,\n",
    "    )\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "plot_feature_unembed_bar(18032, sparse_autoencoder, \"feature_4795_embed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "plot_feature_embed_bar(1, sparse_autoencoder, feature_name = \"resid stream change\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# topk by change in resid stream (downward)\n",
    "plot_feature_unembed_bar(1, sparse_autoencoder, feature_name = \"resid stream change\")\n",
    "plot_feature_unembed_bar(2, sparse_autoencoder, feature_name = \"resid stream change\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache.apply_ln_to_stack(x_reconstruct[0],layer=10).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "attn_scores_original = cache[\"blocks.10.attn.hook_attn_scores\"][0,7].detach().cpu()\n",
    "attn_scores_new = new_cache[\"blocks.10.attn.hook_attn_scores\"][0,7].detach().cpu()\n",
    "px.imshow(\n",
    "    torch.stack([attn_scores_original, attn_scores_new]),\n",
    "    facet_col = 0,\n",
    "    color_continuous_midpoint=0, \n",
    "    color_continuous_scale=\"RdBu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(x = attn_scores_original.flatten(), y = attn_scores_new.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.imshow(sae_qk_circuit[:1000,:1000], title=\"SAE QK Circuit\", color_continuous_midpoint=0, color_continuous_scale=\"RdBu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.imshow(sae_qk_circuit.cpu().numpy(), title=\"SAE QK Circuit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L10H7_W_K = model.W_K[10,7]\n",
    "L10H7_W_Q = model.W_K[10,7]\n",
    "print(L10H7_W_K.shape)\n",
    "print(L10H7_W_Q.shape)\n",
    "reconstructed_keys = x_reconstruct @ L10H7_W_K\n",
    "reconstructed_queries = x_reconstruct @ L10H7_W_Q\n",
    "print(reconstructed_keys.shape)\n",
    "print(reconstructed_queries.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, x_reconstruct, acts, l2_loss, l1_loss = encoder_resid_pre_10(cache[\"blocks.10.hook_resid_pre\"])\n",
    "_, _, acts_mid, _, _ = encoder_resid_pre_10(cache[\"blocks.10.hook_resid_mid\"])\n",
    "acts = acts.detach().cpu()\n",
    "acts_mid = acts_mid.detach().cpu()\n",
    "fired_in_acts_and_not_in_acts_mid = (acts[0,-1] > 0) & (acts_mid[0,-1] == 0)\n",
    "fired_in_acts_and_not_in_acts_mid.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.imshow((acts[:,1:]>0).sum(-1).float().detach().cpu(), color_continuous_midpoint=0, color_continuous_scale=\"RdBu\").show()\n",
    "px.imshow((acts_mid[:,1:]>0).sum(-1).float().detach().cpu(), color_continuous_midpoint=0, color_continuous_scale=\"RdBu\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_john_features = (acts[0,-1] > 0).nonzero()\n",
    "pred_mary_features = (acts[1,-1] > 0).nonzero()\n",
    "common_features = set(pred_john_features.flatten().tolist()).intersection(set(pred_mary_features.flatten().tolist()))\n",
    "print(len(common_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "girls_names = [\" Mary\", \" Magdalene\", \" Maria\", \" Marie\", \" Marian\", \" Marianne\", \" Mara\", \" Maura\", \" Marla\", \" Marta\", \" Maire\", \" Maree\"]\n",
    "boys_names = [\" John\", \" Joseph\", \" Jon\", \" Jonathan\", \" Johan\", \" Johannes\", \" Sean\", \" Shaun\", \" Shane\", \" Ivan\", \" Jan\"]\n",
    "\n",
    "\n",
    "boys_name_tokens = []\n",
    "for name in boys_names:\n",
    "    # get the token without bost\n",
    "    john_tokens = model.to_tokens(name, prepend_bos=False)[0]\n",
    "    print(name, john_tokens, john_tokens.shape)\n",
    "    boys_name_tokens.append(john_tokens[0])\n",
    "boys_name_tokens = torch.stack(boys_name_tokens, dim=0).to(device)\n",
    "print(boys_name_tokens)\n",
    "\n",
    "girls_names_tokens = []\n",
    "for name in girls_names:\n",
    "    # get the token without bost\n",
    "    mary_tokens = model.to_tokens(name, prepend_bos=False)[0]\n",
    "    print(name, mary_tokens, mary_tokens.shape)\n",
    "    girls_names_tokens.append(mary_tokens[0])\n",
    "girls_names_tokens = torch.stack(girls_names_tokens, dim=0).to(device)\n",
    "print(girls_names_tokens)\n",
    "print(model.tokenizer.decode(boys_name_tokens))\n",
    "print(model.tokenizer.decode(girls_names_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_proj = encoder_resid_pre_10.W_dec @ model.W_U[:, boys_name_tokens]\n",
    "feature_proj = feature_proj.detach().cpu().numpy()\n",
    "\n",
    "feature_proj_df = pd.DataFrame(feature_proj, columns = boys_name_tokens.detach().cpu().numpy())\n",
    "feature_proj_df_long = feature_proj_df.reset_index().melt(id_vars='index')\n",
    "feature_proj_df_long.columns = ['feature', 'name', 'value']\n",
    "# make feature categorical\n",
    "feature_proj_df_long.feature = feature_proj_df_long.feature.astype(str)\n",
    "feature_proj_df_long.name = feature_proj_df_long.name.apply(lambda x: model.tokenizer.decode(x))\n",
    "# px.strip(feature_proj_df_long, x= \"name\", y = \"value\", color = \"name\", hover_data=[\"feature\"]).show()\n",
    "\n",
    "# most negative change resid pre to resid mid 4795, 3501, 5337, 920  \n",
    "# most positive change resid pre to resid mid 5680, 4470, 1057, 369\n",
    "# px.bar(feature_proj_df_long[feature_proj_df_long.feature.map(int).isin([4795, 3501, 5337, 920])], y = \"value\", facet_col= \"feature\", x = \"name\", hover_data=[\"feature\"]).show()\n",
    "# px.bar(feature_proj_df_long[feature_proj_df_long.feature.map(int).isin([5680, 4470, 1057, 369])], y = \"value\", facet_col= \"feature\", x = \"name\", hover_data=[\"feature\"]).show()\n",
    "\n",
    "resid_stream_change = pd.Series(acts[0,-1] - acts_mid[0,-1])\n",
    "resid_stream_change.index = resid_stream_change.index.astype(str)\n",
    "feature_proj_df_long = feature_proj_df_long.merge(resid_stream_change.rename('change'), left_on='feature', right_index=True)\n",
    "\n",
    "\n",
    "fig = px.scatter(\n",
    "    feature_proj_df_long,\n",
    "    x = \"value\",\n",
    "    y  = \"change\",\n",
    "    facet_col = \"name\",\n",
    "    labels = {'value': 'feature projection onto {name}',\n",
    "              'change': 'Residual stream change'},\n",
    "    hover_data=[\"feature\"])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_proj = encoder_resid_pre_10.W_dec @ model.W_U[:, girls_names_tokens]\n",
    "feature_proj = feature_proj.detach().cpu().numpy()\n",
    "\n",
    "feature_proj_df = pd.DataFrame(feature_proj, columns = girls_names_tokens.detach().cpu().numpy())\n",
    "feature_proj_df_long = feature_proj_df.reset_index().melt(id_vars='index')\n",
    "feature_proj_df_long.columns = ['feature', 'name', 'value']\n",
    "# make feature categorical\n",
    "feature_proj_df_long.feature = feature_proj_df_long.feature.astype(str)\n",
    "feature_proj_df_long.name = feature_proj_df_long.name.apply(lambda x: model.tokenizer.decode(x))\n",
    "# px.strip(feature_proj_df_long, x= \"name\", y = \"value\", color = \"name\", hover_data=[\"feature\"]).show()\n",
    "\n",
    "# most negative change resid pre to resid mid 4795, 3501, 5337, 920  \n",
    "# most positive change resid pre to resid mid 5680, 4470, 1057, 369\n",
    "# px.bar(feature_proj_df_long[feature_proj_df_long.feature.map(int).isin([4795, 3501, 5337, 920])], y = \"value\", facet_col= \"feature\", x = \"name\", hover_data=[\"feature\"]).show()\n",
    "# px.bar(feature_proj_df_long[feature_proj_df_long.feature.map(int).isin([5680, 4470, 1057, 369])], y = \"value\", facet_col= \"feature\", x = \"name\", hover_data=[\"feature\"]).show()\n",
    "\n",
    "resid_stream_change = pd.Series(acts[0,-1] - acts_mid[0,-1])\n",
    "resid_stream_change.index = resid_stream_change.index.astype(str)\n",
    "feature_proj_df_long = feature_proj_df_long.merge(resid_stream_change.rename('change'), left_on='feature', right_index=True)\n",
    "\n",
    "\n",
    "fig = px.scatter(\n",
    "    feature_proj_df_long,\n",
    "    x = \"value\",\n",
    "    y  = \"change\",\n",
    "    facet_col = \"name\",\n",
    "    labels = {'value': 'feature projection onto {name}',\n",
    "              'change': 'Residual stream change'},\n",
    "    hover_data=[\"feature\"])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = torch.topk(acts[0,-1] - acts_mid[0,-1], k = 10, largest=False)\n",
    "indices = values.indices\n",
    "\n",
    "px.bar(\n",
    "    x=values.values.detach().cpu().numpy() * -1,\n",
    "    text=indices.detach().cpu().numpy(),\n",
    "    orientation=\"h\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_unembed_bar(feature_id, feature_name):\n",
    "\n",
    "    feature_unembed = encoder_resid_pre_10.W_dec[feature_id] @ model.W_U\n",
    "    # torch.topk(unembed_4795,10)\n",
    "\n",
    "    feature_unembed_df = pd.DataFrame(\n",
    "        feature_unembed.detach().cpu().numpy(),\n",
    "        columns = [feature_name],\n",
    "        index = [model.tokenizer.decode(i) for i in list(range(50257))]\n",
    "    )\n",
    "\n",
    "    feature_unembed_df = feature_unembed_df.sort_values(feature_name, ascending=False).reset_index().rename(columns={'index': 'token'})\n",
    "    fig = px.bar(feature_unembed_df.head(20).sort_values(feature_name, ascending=True),\n",
    "            y = 'token', x = feature_name, orientation='h', color = feature_name, hover_data=[feature_name])\n",
    "\n",
    "    fig.update_layout(\n",
    "        width=500,\n",
    "        height=600,\n",
    "    )\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "# topk by change in resid stream (downward)\n",
    "plot_feature_unembed_bar(4795, 'Bible Feature')\n",
    "# plot_feature_unembed_bar(3501, '??')\n",
    "# plot_feature_unembed_bar(920, \"'their' family feature\")\n",
    "# plot_feature_unembed_bar(5337, \"??\")\n",
    "# plot_feature_unembed_bar(4793, 'American Last names?')\n",
    "# plot_feature_unembed_bar(3213, 'second token in multitoken word')\n",
    "\n",
    "# plot_feature_unembed_bar(2434, 'Boy Names Feature')\n",
    "# plot_feature_unembed_bar(4793, 'American Last names?')\n",
    "# # plot_feature_unembed_bar(3213, 'second token in multitoken word')\n",
    "# plot_feature_unembed_bar(4774, 'second token in multitoken word')\n",
    "# plot_feature_unembed_bar(3732, 'Israel/Palestine Feature')\n",
    "# plot_feature_unembed_bar(358, 'Them Feature')\n",
    "\n",
    "# plot_feature_unembed_bar(2440, 'Names, most often women, capitalized.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache[\"blocks.10.attn.hook_z\"][0, -1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.W_O[10].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "L10_resid_stream_contribution = einsum(\"head d_head,  head d_head d_model -> head d_model\", cache[\"blocks.10.attn.hook_z\"][0, -1], model.W_O[10]).detach()\n",
    "L10_resid_stream_contribution.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L10H7_resid_stream_contribution.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = model.to_tokens(prompts, prepend_bos=True)\n",
    "\n",
    "loss, x_reconstruct, acts, l2_loss, l1_loss = encoder_resid_pre_10(cache[\"blocks.10.hook_resid_pre\"])\n",
    "_, _, acts_mid, _, _ = encoder_resid_pre_10(cache[\"blocks.10.hook_resid_mid\"])\n",
    "acts = acts.detach().cpu()\n",
    "acts_mid = acts_mid.detach().cpu()\n",
    "fired_in_acts_and_not_in_acts_mid = (acts[0,-1] > 0) & (acts_mid[0,-1] == 0)\n",
    "fired_in_acts_and_not_in_acts_mid.squeeze()\n",
    "difference_in_acts_and_acts_mid = acts[0,-1] - acts_mid[0,-1]\n",
    "\n",
    "\n",
    "L10H7_resid_stream_contribution = cache[\"blocks.10.attn.hook_z\"][0,-1,7] @ model.W_O[10,7] @ encoder_resid_pre_10.W_dec.T\n",
    "feature_contributions_L10H7 = einsum(\"head d_head,  head d_head d_model -> head d_model\", cache[\"blocks.10.attn.hook_z\"][0, -1], model.W_O[10]).detach().detach().cpu()\n",
    "mary_unembed_scores = encoder_resid_pre_10.W_dec @ model.W_U[:, 5335]\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "        'mary_unembed_scores': mary_unembed_scores.detach().cpu(),\n",
    "        'fired_in_acts_and_not_in_acts_mid': fired_in_acts_and_not_in_acts_mid.detach().cpu(),\n",
    "        'difference_in_acts_and_acts_mid': difference_in_acts_and_acts_mid.detach().cpu(),\n",
    "        '_L10H7_resid_stream_contribution': L10H7_resid_stream_contribution.detach().cpu()\n",
    "    }\n",
    "\n",
    ")\n",
    "df.reset_index(inplace=True)\n",
    "df['FeatureId']=df['index']\n",
    "\n",
    "px.scatter(\n",
    "    df, x = \"_L10H7_resid_stream_contribution\", y = \"mary_unembed_scores\", color = \"difference_in_acts_and_acts_mid\", hover_data=[\"FeatureId\"],\n",
    "    color_continuous_scale=\"RdBu\", color_continuous_midpoint=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_contributions_to_mary = L10H7_resid_stream_contribution.cpu()* mary_unembed_scores.cpu()\n",
    "torch.topk(feature_contributions_to_mary,10, largest=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_head_proj = feature_contributions_L10H7 @ encoder_resid_pre_10.W_dec.T.cpu()\n",
    "df2 = pd.DataFrame(all_head_proj.detach().T, columns = [f'L10H{h}' for h in range(12)])\n",
    "df = df.merge(df2, left_index=True, right_index=True)\n",
    "long_df = df.melt(id_vars=['FeatureId','mary_unembed_scores', 'fired_in_acts_and_not_in_acts_mid', 'difference_in_acts_and_acts_mid'],\n",
    "                  value_vars=[col for col in df.columns if col.startswith('L10H')],\n",
    "                  var_name='Head',\n",
    "                  value_name='Score')\n",
    "long_df = long_df[long_df.difference_in_acts_and_acts_mid != 0]\n",
    "long_df.head()\n",
    "# df_long = pd.wide_to_long(df,stubnames = 'L10', i=['FeatureId', 'mary_unembed_scores', 'fired_in_acts_and_not_in_acts_mid', 'difference_in_acts_and_acts_mid'],  j='value')\n",
    "# df_long.head()#.head()\n",
    "\n",
    "\n",
    "fig = px.scatter( long_df.sort_values([\"Head\",\"difference_in_acts_and_acts_mid\"]), x = 'Score', y = 'mary_unembed_scores', color ='difference_in_acts_and_acts_mid',\n",
    "                 animation_frame=\"Head\",\n",
    "                #  facet_col=\"Head\",\n",
    "                #  facet_col_wrap=4,\n",
    "                 color_continuous_scale='RdBu', color_continuous_midpoint=0,\n",
    "                 hover_data=['FeatureId'],\n",
    "                #  template='plotly_dark',\n",
    ")\n",
    "fig.update_layout(\n",
    "    width=1000,\n",
    "    height=800,\n",
    ")\n",
    "fig.show()\n",
    "# px.histogram(feature_contributions_L10H7.detach().cpu()).show()\n",
    "# torch.topk(feature_contributions_L10H7, 20, largest=False)\n",
    "# head_contributions = pd.Series(feature_contributions_L10H7.detach().cpu())\n",
    "# head_contributions.index = head_contributions.index.astype(str)\n",
    "# # feature_proj_df_long = feature_proj_df_long.merge(resid_stream_change.rename('change'), left_on='feature', right_index=True)\n",
    "# head_contributions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decomp, labels = cache.get_full_resid_decomposition(expand_neurons=False, return_labels=True) \n",
    "mary_proj = decomp[:,0,-1] @ model.W_U[:, 5335]\n",
    "pd.DataFrame(mary_proj.detach().cpu().numpy(), index = labels).sort_values(0, ascending=True).head(20) # this is where we let the neuron fire, lets replace it. \n",
    "pd.DataFrame(mary_proj.detach().cpu().numpy(), index = labels).sort_values(0, ascending=True).tail(20) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " encoder_resid_pre_10.W_dec[5335].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = 5335\n",
    "tokens = model.to_tokens(prompts, prepend_bos=True)\n",
    "replacement_for_feature = torch.zeros_like(tokens)\n",
    "replacement_for_feature[0, -1] = acts[:, 0, -1, feature]\n",
    "out_diff = replacement_for_feature[:, :, None] * encoder_resid_pre_10.W_dec[5335]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def remove_bible_feature(mlp_out, hook):\n",
    "    mlp_out[:, :] -= mlp_out0_diff\n",
    "    return mlp_out\n",
    "    \n",
    "model.reset_hooks()\n",
    "model.blocks[0].hook_mlp_out.add_hook(remove_bible_feature)\n",
    "_, new_cache = model.run_with_cache(prompts, stop_at_layer=2, names_filter=lambda x: \"mlp_out\" in x)\n",
    "\n",
    "with torch.no_grad():\n",
    "    loss, x_reconstruct, acts, l2_loss, l1_loss = encoder_resid_pre_10(cache[\"blocks.10.hook_resid_pre\"])\n",
    "\n",
    "model.reset_hooks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sae_2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
