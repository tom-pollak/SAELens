{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append(\"../..\")\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from importlib import reload\n",
    "from tqdm import tqdm\n",
    "\n",
    "import joseph\n",
    "from joseph.analysis import *\n",
    "from joseph.visualisation import *\n",
    "from joseph.utils import *\n",
    "from joseph.data import *\n",
    "\n",
    "\n",
    "reload(joseph.analysis)\n",
    "reload(joseph.visualisation)\n",
    "reload(joseph.utils)\n",
    "reload(joseph.data)\n",
    "\n",
    "from joseph.analysis import *\n",
    "from joseph.visualisation import *\n",
    "from joseph.utils import *\n",
    "from joseph.data import *\n",
    "\n",
    "# turn torch grad tracking off\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "import webbrowser\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "path_to_html = \"../week_8_jan/gpt2_small_features_layer_5\"\n",
    "def render_feature_dashboard(feature_id):\n",
    "    \n",
    "    path = f\"{path_to_html}/data_{feature_id:04}.html\"\n",
    "    \n",
    "    print(f\"Feature {feature_id}\")\n",
    "    if os.path.exists(path):\n",
    "        # with open(path, \"r\") as f:\n",
    "        #     html = f.read()\n",
    "        #     display(HTML(html))\n",
    "        webbrowser.open_new_tab(\"file://\" + os.path.abspath(path))\n",
    "    else:\n",
    "        print(\"No HTML file found\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = HookedTransformer.from_pretrained(\n",
    "    \"gpt2-small\",\n",
    "    # \"pythia-2.8b\",\n",
    "    # \"pythia-70m-deduped\",\n",
    "    # \"tiny-stories-2L-33M\",\n",
    "    # \"attn-only-2l\",\n",
    "    # center_unembed=True,\n",
    "    # center_writing_weights=True,\n",
    "    # fold_ln=True,\n",
    "    # refactor_factored_attn_matrices=True,\n",
    "    fold_ln=True,\n",
    ")\n",
    "model.set_use_split_qkv_input(True)\n",
    "model.set_use_attn_result(True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load SAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sae_training.utils import LMSparseAutoencoderSessionloader\n",
    "\n",
    "\n",
    "path = \"../week_8_jan/artifacts/sparse_autoencoder_gpt2-small_blocks.10.hook_resid_pre_49152:v28/1100001280_sparse_autoencoder_gpt2-small_blocks.10.hook_resid_pre_49152.pt\"\n",
    "# sparse_autoencoder_layer_10 = SparseAutoencoder.load_from_pretrained(path)\n",
    "model, sparse_autoencoder_layer_10, activation_store_layer_10 = LMSparseAutoencoderSessionloader.load_session_from_pretrained(\n",
    "    path\n",
    ")\n",
    "\n",
    "path = \"../week_8_jan/artifacts/sparse_autoencoder_gpt2-small_blocks.5.hook_resid_pre_49152:v9/final_sparse_autoencoder_gpt2-small_blocks.5.hook_resid_pre_49152.pt\"\n",
    "# sparse_autoencoder_layer_5 = SparseAutoencoder.load_from_pretrained(path)\n",
    "_, sparse_autoencoder_layer_5, activation_store_layer_5 = LMSparseAutoencoderSessionloader.load_session_from_pretrained(\n",
    "    path\n",
    ")\n",
    "\n",
    "print(sparse_autoencoder_layer_10.cfg)\n",
    "print(sparse_autoencoder_layer_5.cfg)\n",
    "\n",
    "\n",
    "# sanity check\n",
    "text = \"Many important transition points in the history of science have been moments when science 'zoomed in.' At these points, we develop a visualization or tool that allows us to see the world in a new level of detail, and a new field of science develops to study the world through this lens.\"\n",
    "model(text, return_type=\"loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore Sparsity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "def estimate_feature_sparsity_using_n_tokens_per_prompt(\n",
    "    sparse_autoencoder, activation_store, n_batches,\n",
    "    n_tokens_per_prompt=4):\n",
    "    \n",
    "    total_activations = torch.zeros(sparse_autoencoder.cfg.d_sae).to(sparse_autoencoder.cfg.device)\n",
    "    \n",
    "    pbar = tqdm(range(n_batches))\n",
    "    for _ in pbar:\n",
    "        batch_tokens = activation_store.get_batch_tokens()\n",
    "        _, cache = model.run_with_cache(batch_tokens, prepend_bos=False)\n",
    "        original_act = cache[sparse_autoencoder.cfg.hook_point]\n",
    "        _, feature_acts, _, _, _ = sparse_autoencoder(\n",
    "            original_act\n",
    "        )\n",
    "        # for each batch item, pick 4 random tokens and keep only those\n",
    "        # batch_size x n_tokens x d_sae\n",
    "        random_tok_indices = torch.randint(0, feature_acts.shape[1], (feature_acts.shape[0], n_tokens_per_prompt))\n",
    "        feature_acts = feature_acts[torch.arange(feature_acts.shape[0]).unsqueeze(-1), random_tok_indices]\n",
    "        total_activations += feature_acts.flatten(0,1).sum(0)\n",
    "    \n",
    "    total_tokens = (n_batches * feature_acts.shape[0] * n_tokens_per_prompt)\n",
    "    print(\"Total tokens:\", total_tokens)\n",
    "    \n",
    "    return total_activations / total_tokens\n",
    "\n",
    "n_tokens_per_prompt = 128\n",
    "n_batches = 1000\n",
    "feature_sparsity_10_unstratified  = estimate_feature_sparsity_using_n_tokens_per_prompt(sparse_autoencoder_layer_10, activation_store_layer_10, n_batches=n_batches, n_tokens_per_prompt=n_tokens_per_prompt).detach().cpu()\n",
    "log_feature_sparsity_10_unstratified = torch.log10(feature_sparsity_10_unstratified  + 1e-10)\n",
    "torch.save(log_feature_sparsity_10_unstratified, f\"../week_8_jan/artifacts/sparse_autoencoder_gpt2-small_blocks.10.hook_resid_pre_49152:v28/feature_sparsity_{n_batches}_{n_tokens_per_prompt}.pt\")\n",
    "feature_sparsity_5_unstratified = estimate_feature_sparsity_using_n_tokens_per_prompt(sparse_autoencoder_layer_5, activation_store_layer_5, n_batches=100, n_tokens_per_prompt=128).detach().cpu()\n",
    "log_feature_sparsity_5_unstratified = torch.log10(feature_sparsity_5_unstratified  + 1e-10)\n",
    "torch.save(log_feature_sparsity_5_unstratified, f\"../week_8_jan/artifacts/sparse_autoencoder_gpt2-small_blocks.5.hook_resid_pre_49152:v9/feature_sparsity_{n_batches}_{n_tokens_per_prompt}.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_feature_sparsity_10_stratified = torch.load(\n",
    "    \"../week_8_jan/artifacts/sparse_autoencoder_gpt2-small_blocks.10.hook_resid_pre_49152:v28/log_feature_sparsity_5000_4.pt\"\n",
    ")\n",
    "# px.histogram(\n",
    "#     log_feature_sparsity_10_stratified[log_feature_sparsity_10_stratified > -9],\n",
    "#     nbins=1000,\n",
    "#     width = 1000,\n",
    "#     log_x=False,\n",
    "#     title=\"Feature sparsity (log10) (5000 batches, 4 tokens per prompt)\",\n",
    "# ).show()\n",
    "\n",
    "log_feature_sparsity_5_stratified = torch.load(\n",
    "    \"../week_8_jan/artifacts/sparse_autoencoder_gpt2-small_blocks.5.hook_resid_pre_49152:v9/log_feature_sparsity_5000_4.pt\"\n",
    ")\n",
    "# px.histogram(\n",
    "#     log_feature_sparsity_5_stratified[log_feature_sparsity_5_stratified > -9],\n",
    "#     nbins=1000,\n",
    "#     width=1000,\n",
    "#     log_x=False,\n",
    "#     title=\"Feature sparsity (log10) (5000 batches, 4 tokens per prompt)\",\n",
    "# ).show()\n",
    "# px.histogram(log_feature_sparsity, nbins=1000, log_x=False, title=\"Feature sparsity (log10) (5000 batches, 4 tokens per prompt)\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For layer 10, let's compare. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(\n",
    "    x = log_feature_sparsity_10_stratified,\n",
    "    y =  log_feature_sparsity_10_unstratified,\n",
    "    opacity=0.4,\n",
    "    marginal_x=\"histogram\",\n",
    "    marginal_y=\"histogram\",\n",
    "    title=\"Feature sparsity (log10) Stratified vs Unstratified (5000 batches, 4 tokens per prompt)\",\n",
    "    color = (log_feature_sparsity_10_stratified - log_feature_sparsity_10_unstratified).numpy().tolist(),\n",
    "    width = 1500,\n",
    "    height = 1500,\n",
    "    color_continuous_midpoint=0,\n",
    "    hover_data= [ list(range(len(log_feature_sparsity_10_stratified))) ],\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_feature_dashboard(feature_id):\n",
    "    \n",
    "    path_to_html = \"../week_8_jan/gpt2_small_features\"\n",
    "    path = f\"{path_to_html}/data_{feature_id:04}.html\"\n",
    "    \n",
    "    print(f\"Feature {feature_id}\")\n",
    "    if os.path.exists(path):\n",
    "        # with open(path, \"r\") as f:\n",
    "        #     html = f.read()\n",
    "        #     display(HTML(html))\n",
    "        webbrowser.open_new_tab(\"file://\" + os.path.abspath(path))\n",
    "    else:\n",
    "        print(\"No HTML file found\")\n",
    "\n",
    "\n",
    "# dense_features = ((log_feature_sparsity_10_stratified<-1) & (log_feature_sparsity_10_unstratified>-2)).nonzero().squeeze()\n",
    "# dense_features = dense_features[torch.randperm(len(dense_features))[:10]]\n",
    "# for feature in dense_features:\n",
    "#     render_feature_dashboard(feature.item())\n",
    "\n",
    "diff = log_feature_sparsity_10_stratified - log_feature_sparsity_10_unstratified\n",
    "dense_features = ((diff>2) & (log_feature_sparsity_10_stratified > -4)).nonzero().squeeze()\n",
    "print(len(dense_features))\n",
    "dense_features = dense_features[torch.randperm(len(dense_features))[:6]]\n",
    "for feature in dense_features:\n",
    "    render_feature_dashboard(feature.item())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 1102 -> fires on \"erect\" and/or \"ile\" if it follows \"erect\"\n",
    "# 511 -> fires on \"lashed\" and \"out\" if \"out\" follows lashed\n",
    "# 509 -> fires on in and \"the\" if \"the\" follows in\n",
    "# 1289 -> fires on ongoing and investigation if investigation follows ongoing\n",
    "# 10329 -> Fires on Easter, and eggs or bunny if it follows Easter but not on Bunny or Eggs alone (presumably)\n",
    "# 17301 -> Fires on family members and sometimes on phrases (eg: brother-in-law) (a stretch for sure)\n",
    "# 49144 -> What do you think? (fires on you think, sometimes extends to \"of\" \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Dashboard generator util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import webbrowser\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "path_to_html = \"../week_8_jan/gpt2_small_features_layer_5\"\n",
    "def render_feature_dashboard(feature_id):\n",
    "    \n",
    "    path = f\"{path_to_html}/data_{feature_id:04}.html\"\n",
    "    \n",
    "    print(f\"Feature {feature_id}\")\n",
    "    if os.path.exists(path):\n",
    "        # with open(path, \"r\") as f:\n",
    "        #     html = f.read()\n",
    "        #     display(HTML(html))\n",
    "        webbrowser.open_new_tab(\"file://\" + os.path.abspath(path))\n",
    "    else:\n",
    "        print(\"No HTML file found\")\n",
    "    \n",
    "    return\n",
    "\n",
    "# for feature in [100,300,400]:\n",
    "#     render_feature_dashboard(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fun Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt1 = \"The war caused not only destruction and death but also generations of hatred between the two communities.\"\n",
    "prompt2 = \"The car not only is economical but also feels good to drive.\"\n",
    "prompt3 = \"This investigation is not only one that is continuing and worldwide,\"  # but also one that we expect to continue for quite some time.\"\n",
    "prompt = prompt3\n",
    "answer = \"but\"\n",
    "model.reset_hooks()\n",
    "utils.test_prompt(prompt, answer, model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joseph\n",
    "reload(joseph.analysis)\n",
    "from joseph.analysis import *\n",
    "\n",
    "# prompt3 = \"This investigation is not only one that is continuing and worldwide, but also one that we expect to continue for quite some time.\" # Not only ... but\n",
    "# prompt3 = \"The market is evolving rapidly. Either we must adjust our strategy to meet the new market demands, or we risk falling behind our competitors significantly.\" # either or one (dud?)\n",
    "# prompt3 = \"Culinary trends are constantly changing. Either we experiment with new flavors and techniques in our recipes, or we risk losing the interest of our adventurous diners.\" #maybe a dud as well\n",
    "# prompt3 = \"I thought it was a great book. Both the intricate plot twists and the strong character development make this novel exceptionally engaging.\" # both .... and\n",
    "# prompt3 = \"The team, despite facing numerous challenges and unexpected setbacks, remains optimistic about the upcoming project.\" # Noun verb agreement\n",
    "# prompt3 = \"The book on the shelf in the corner needs a new cover.\" # Noun verb agreement\n",
    "\n",
    "# title = \"which way to the beach\"\n",
    "# prompt = \"She asked 'Which way to the beach?', to which I replied,  'It's over there. You can't miss it.'. She thanked me and walked away.\"\n",
    "# POS_INTEREST = 9\n",
    "\n",
    "# title = \"lots of questions\"\n",
    "# prompt = \"The text read \\\"In the realm of deep learning, how do we best quantify the interpretability of neural networks? While considering this, it's important to remember the balance between complexity and clarity in model design. What are the most effective methods for visualizing high-dimensional data? This leads to another crucial aspect: the role of data quality. Can we establish a standard for data that optimally trains these models? Amidst these inquiries, the evolution of AI safety protocols remains a pivotal concern. How are current safety measures adapting to the rapidly advancing AI landscape? Each question marks a stepping stone towards a deeper understanding and more effective utilization of AI technologies.\"\n",
    "# POS_INTEREST = 10\n",
    "\n",
    "title = \"Tiny Stories Dragon\"\n",
    "prompt = \"\"\"Once upon a time, there was a little girl named Lily. She was very\n",
    "excited to go outside and explore. She flew over the trees and saw a big,\n",
    "scary dragon. The dragon was very scary. But Lily knew that things\n",
    "were not real and she would hurt her.\"\"\"\n",
    "POS_INTEREST = 41\n",
    "\n",
    "# title = \"both_and\"\n",
    "# prompt = \"My parents went to both Melbourne, Australia and Auckland, New Zealand on their honeymoon.\"\n",
    "# POS_INTEREST = 8\n",
    "\n",
    "\n",
    "token_df, original_cache, cache_reconstructed_query, feature_acts = eval_prompt([prompt], model, sparse_autoencoder, head_idx_override=5)\n",
    "filter_cols = [\"str_tokens\", \"unique_token\", \"context\", \"batch\", \"pos\", \"label\", \"loss\", \"loss_diff\", \"mse_loss\", \"num_active_features\", \"explained_variance\", \"kl_divergence\",\n",
    "            \"top_k_features\"]\n",
    "display(token_df[filter_cols].style.background_gradient(\n",
    "    subset=[\"loss_diff\", \"mse_loss\",\"explained_variance\", \"num_active_features\", \"kl_divergence\"],\n",
    "    cmap=\"coolwarm\"))\n",
    "\n",
    "\n",
    "\n",
    "UNIQUE_TOKEN_INTEREST = token_df[\"unique_token\"][POS_INTEREST]\n",
    "feature_acts_of_interest = feature_acts[POS_INTEREST]\n",
    "# plot_line_with_top_10_labels(feature_acts_of_interest, \"\", 25)\n",
    "# vals, inds = torch.topk(feature_acts_of_interest,39)\n",
    "\n",
    "top_k_feature_inds = (feature_acts[1:] > 0).sum(dim=0).nonzero().squeeze()\n",
    "\n",
    "features_acts_by_token_df = pd.DataFrame(\n",
    "    feature_acts[:,top_k_feature_inds[:]].detach().cpu().T,\n",
    "    index = [f\"feature_{i}\" for i in top_k_feature_inds.flatten().tolist()],\n",
    "    columns = token_df[\"unique_token\"])\n",
    "\n",
    "# features_acts_by_token_df.sort_values(by=\",/12\", ascending=False).head(10).style.background_gradient(\n",
    "#     cmap=\"coolwarm\", axis=0)\n",
    "\n",
    "# px.imshow(features_acts_by_token_df.sort_values(by=\",/12\", ascending=False).head(10).T.corr(), color_continuous_midpoint=0, color_continuous_scale=\"RdBu\")\n",
    "\n",
    "tmp = features_acts_by_token_df.sort_values(UNIQUE_TOKEN_INTEREST, ascending=False).T\n",
    "# dashboard_features = features_acts_by_token_df.sort_values(UNIQUE_TOKEN_INTEREST, ascending=False).index[:10].to_series().apply(lambda x: x.split(\"_\")[1]).tolist()\n",
    "# for feature in dashboard_features:\n",
    "#     render_feature_dashboard(feature)\n",
    "\n",
    "px.line(tmp, \n",
    "        title=f\"{title}: Features Activation by Token in Prompt\", \n",
    "        color_discrete_sequence=px.colors.qualitative.Plotly,\n",
    "        height=1000).show()\n",
    "\n",
    "tmp = features_acts_by_token_df.head(100).T\n",
    "px.imshow(tmp, \n",
    "            title=f\"{title}: Top k features by activation\", \n",
    "            color_continuous_midpoint=0, \n",
    "            color_continuous_scale=\"RdBu\", \n",
    "            height=800).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def analyse_lcf(prompt, title = \"\", model=model, sparse_autoencoder=sparse_autoencoder, head_idx_override=None):\n",
    "\n",
    "    token_df, original_cache, cache_reconstructed_query, feature_acts = eval_prompt([prompt], model, sparse_autoencoder, head_idx_override=7)\n",
    "    filter_cols = [\"str_tokens\", \"unique_token\", \"context\", \"batch\", \"pos\", \"label\", \"loss\", \"loss_diff\", \"mse_loss\", \"num_active_features\", \"explained_variance\", \"kl_divergence\",\n",
    "                \"top_k_features\"]\n",
    "    display(token_df[filter_cols].style.background_gradient(\n",
    "        subset=[\"loss_diff\", \"mse_loss\",\"explained_variance\", \"num_active_features\", \"kl_divergence\"],\n",
    "        cmap=\"coolwarm\"))\n",
    "    \n",
    "    \n",
    "    POS_INTEREST = token_df.index.max()\n",
    "    UNIQUE_TOKEN_INTEREST = token_df[\"unique_token\"][POS_INTEREST]\n",
    "    feature_acts_of_interest = feature_acts[POS_INTEREST]\n",
    "    # plot_line_with_top_10_labels(feature_acts_of_interest, \"\", 25)\n",
    "    # vals, inds = torch.topk(feature_acts_of_interest,39)\n",
    "\n",
    "    top_k_feature_inds = (feature_acts[1:] > 0).sum(dim=0).nonzero().squeeze()\n",
    "\n",
    "    features_acts_by_token_df = pd.DataFrame(\n",
    "        feature_acts[:,top_k_feature_inds[:]].detach().cpu().T,\n",
    "        index = [f\"feature_{i}\" for i in top_k_feature_inds.flatten().tolist()],\n",
    "        columns = token_df[\"unique_token\"])\n",
    "\n",
    "    # features_acts_by_token_df.sort_values(by=\",/12\", ascending=False).head(10).style.background_gradient(\n",
    "    #     cmap=\"coolwarm\", axis=0)\n",
    "\n",
    "    # px.imshow(features_acts_by_token_df.sort_values(by=\",/12\", ascending=False).head(10).T.corr(), color_continuous_midpoint=0, color_continuous_scale=\"RdBu\")\n",
    "\n",
    "    tmp = features_acts_by_token_df.sort_values(UNIQUE_TOKEN_INTEREST, ascending=False).T\n",
    "    px.line(tmp, \n",
    "            title=f\"{title}: Features Activation by Token in Prompt\", \n",
    "            color_discrete_sequence=px.colors.qualitative.Plotly).show()\n",
    "\n",
    "    tmp = features_acts_by_token_df.T\n",
    "    px.imshow(tmp, \n",
    "              title=f\"{title}: Top k features by activation\", \n",
    "              color_continuous_midpoint=0, \n",
    "              color_continuous_scale=\"RdBu\", \n",
    "              height=800).show()\n",
    "\n",
    "\n",
    "\n",
    "correlative_conjunction_prompts = {\n",
    "    \"Either - or\": {\n",
    "        \"prompt\": \"Either you are with me,\",\n",
    "        \"answer\": \" or\"\n",
    "    },\n",
    "    \"Neither - nor\": {\n",
    "        \"prompt\": \"I wasn't hired at any of the companies I'd applied to. Neither my experience great amount of experience,\",\n",
    "        \"answer\": \" nor\"\n",
    "    },\n",
    "    \"Such - that\": {\n",
    "        \"prompt\": \"Such is the intensity of the pollen outside,\",\n",
    "        \"answer\": \" that\"\n",
    "    },\n",
    "        \"Whether - or\": {\n",
    "        \"prompt\": \"Whether you bike to work and love that\",\n",
    "        \"answer\": \" or\"\n",
    "    },\n",
    "    \"Not only - but\": {\n",
    "        \"prompt\": \"Not only did my boyfriend buy me a Nintendo Switch,\",\n",
    "        \"answer\": \" but\"\n",
    "    },\n",
    "    \"Not only - but also\": {\n",
    "        \"prompt\": \"Not only did my boyfriend buy me a Nintendo Switch, but\",\n",
    "        \"answer\": \" also\"\n",
    "    },\n",
    "    \"Both - And\": {\n",
    "        \"prompt\": \"My parents went to both Hawaii\",\n",
    "        \"answer\": \" and\"\n",
    "    },\n",
    "    \"As many - as\": {\n",
    "        \"prompt\": \"There were as many applicants\",\n",
    "        \"answer\": \" as\"\n",
    "    },\n",
    "    \"No sooner - than\": {\n",
    "        \"prompt\": \"She would no sooner cheat on an exam\",\n",
    "        \"answer\": \" than\"\n",
    "    },\n",
    "    \"Rather - than\": {\n",
    "        \"prompt\": \"They would rather go to the movies\",\n",
    "        \"answer\": \" than\"\n",
    "    },\n",
    "}\n",
    "\n",
    "comparative_phrases_prompts = {\n",
    "    \"The more - the more\": {\n",
    "         \"prompt\": f\"the more you learn, the more you realize how much you don't know\",\n",
    "    },\n",
    "    \"The fewer - the fewer\": {\n",
    "        \"prompt\":f\"The fewer people who know about this, the better\",\n",
    "    },\n",
    "    \"Less on - more on\": {\n",
    "        \"prompt\":f\"I think we should focus less on talking about doing the work and more on doing the work\",\n",
    "    },\n",
    "}\n",
    " \n",
    " \n",
    "random_sentences = {\n",
    "    \"Random 1\": {\n",
    "         \"prompt\": f\"Each of these patterns shares the property of linking elements in language, creating relationships between them that are similar to those established by correlative conjunctions.\"\n",
    "    },\n",
    "    \"Random 2\": {\n",
    "        \"prompt\":f\"I feel that the insight/intuitions/skills I’ve spent trying to make progress on trajectory models, are best utilized by **studying sparse-autoencoders on language models**.\",\n",
    "    },\n",
    "    \"Random 3\": {\n",
    "        \"prompt\":f\"In most cases, professional emails are formal emails. A formal email is an email between professionals or academics that contains information related to their work.\",\n",
    "    },\n",
    "}   \n",
    "\n",
    "\n",
    "for title, prompt_dict in correlative_conjunction_prompts.items():\n",
    "    print(title)\n",
    "    analyse_lcf(prompt_dict[\"prompt\"], title=title)\n",
    "    print(\"\\n\\n\") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mech interp on a few examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"I have to say, not only is this a great book, but also the author is a great person.\"\n",
    "prompt = \"'Well, Ted,' said the weatherman, 'I don't know about that, but it's not only the owls that have been acting oddly today\"\n",
    "prompt = \" Not only was Hagrid twice as tall as anyone else, he kept pointing at perfectly ordinary things\"\n",
    "prompt = \"Soon he had not only Dumbledore and Morgana, but Hengist of Woodcraft, Alberic Grunnion, Circe, Paracelsus and Merlin.\"\n",
    "prompt = \"She asked 'Which way to the beach?', to which I replied,  'It's over there. You can't miss it.'. She thanked me and walked away.\"\n",
    "prompt = \"The team, despite facing numerous challenges and unexpected setbacks, remains optimistic about the upcoming project.\"\n",
    "prompt = \"\"\"\n",
    "correlative_conjunction_prompts = {\n",
    "    \"Either - or\": {\n",
    "        \"prompt\": \"Either you are with me,\",\n",
    "        \"answer\": \" or\"\n",
    "    },\n",
    "    \"Neither - nor\": {\n",
    "        \"prompt\": \"I wasn't hired at any of the companies I'd applied to. Neither my experience great amount of experience,\",\n",
    "        \"answer\": \" nor\"\n",
    "    },\n",
    "\"\"\"\n",
    "token_df, original_cache, cache_reconstructed_query, feature_acts = eval_prompt([prompt], model, sparse_autoencoder, head_idx_override=5)\n",
    "filter_cols = [\"str_tokens\", \"unique_token\", \"context\", \"batch\", \"pos\", \"label\", \"loss\", \"loss_diff\", \"mse_loss\", \"num_active_features\", \"explained_variance\", \"kl_divergence\",\n",
    "            \"top_k_features\"]\n",
    "# display(token_df[filter_cols].style.background_gradient(\n",
    "#     subset=[\"loss_diff\", \"mse_loss\",\"explained_variance\", \"num_active_features\", \"kl_divergence\"],\n",
    "#     cmap=\"coolwarm\"))\n",
    "\n",
    "\n",
    "# POS_INTEREST = token_df.index.max()\n",
    "# UNIQUE_TOKEN_INTEREST = token_df[\"unique_token\"][POS_INTEREST]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "POS_INTEREST = 6\n",
    "UNIQUE_TOKEN_INTEREST = token_df[\"unique_token\"][POS_INTEREST]\n",
    "feature_acts_of_interest = feature_acts[POS_INTEREST]\n",
    "# plot_line_with_top_10_labels(feature_acts_of_interest, \"\", 25)\n",
    "# vals, inds = torch.topk(feature_acts_of_interest,39)\n",
    "\n",
    "\n",
    "top_k_feature_inds = (feature_acts[1:] > 0).sum(dim=0).nonzero().squeeze()\n",
    "\n",
    "features_acts_by_token_df = pd.DataFrame(\n",
    "    feature_acts[:,top_k_feature_inds[:]].detach().cpu().T,\n",
    "    index = [f\"feature_{i}\" for i in top_k_feature_inds.flatten().tolist()],\n",
    "    columns = token_df[\"unique_token\"])\n",
    "\n",
    "# features_acts_by_token_df.sort_values(by=\",/12\", ascending=False).head(10).style.background_gradient(\n",
    "#     cmap=\"coolwarm\", axis=0)\n",
    "\n",
    "# px.imshow(features_acts_by_token_df.sort_values(by=\",/12\", ascending=False).head(10).T.corr(), color_continuous_midpoint=0, color_continuous_scale=\"RdBu\")\n",
    "\n",
    "tmp = features_acts_by_token_df.sort_values(UNIQUE_TOKEN_INTEREST, ascending=False).T\n",
    "# dashboard_features = features_acts_by_token_df.sort_values(UNIQUE_TOKEN_INTEREST, ascending=False).index[:10].to_series().apply(lambda x: x.split(\"_\")[1]).tolist()\n",
    "# for feature in dashboard_features:\n",
    "#     render_feature_dashboard(feature)\n",
    "\n",
    "px.line(tmp, \n",
    "        title=f\"{title}: Features Activation by Token in Prompt\", \n",
    "        color_discrete_sequence=px.colors.qualitative.Plotly,\n",
    "        height=1000).show()\n",
    "\n",
    "# px.parallel_coordinates(\n",
    "#     tmp.T,\n",
    "#     dimensions = tmp.index,\n",
    "#     # color=UNIQUE_TOKEN_INTEREST,\n",
    "#     color_continuous_scale=px.colors.sequential.Plasma,\n",
    "#     color_continuous_midpoint=0,\n",
    "#     title=f\"{title}: Features Activation by Token in Prompt\",\n",
    "#     height=500,\n",
    "# ).show()\n",
    "# tmp = features_acts_by_token_df.T\n",
    "# px.imshow(tmp, \n",
    "#             title=f\"{title}: Top k features by activation\", \n",
    "#             color_continuous_midpoint=0, \n",
    "#             color_continuous_scale=\"RdBu\", \n",
    "#             height=800).show()\n",
    "\n",
    "px.imshow(tmp.corr(), color_continuous_midpoint=0, color_continuous_scale=\"RdBu\", height=800).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = original_cache.apply_ln_to_stack(original_cache['blocks.11.hook_resid_post']) @ model.W_U\n",
    "print(logits.shape)\n",
    "vals, inds =torch.topk(logits[:,1:], 10, dim=-1)\n",
    "topk_predicted_token_inds = list(set(inds.flatten().tolist()))\n",
    "topk_predicted_token_strs =model.tokenizer.convert_ids_to_tokens(topk_predicted_token_inds)\n",
    "\n",
    "predicted_tokens_df = pd.DataFrame(logits[0,:,topk_predicted_token_inds].detach().cpu().T,\n",
    "                                   columns = token_df[\"unique_token\"], index = topk_predicted_token_strs)\n",
    "\n",
    "px.line(predicted_tokens_df.sort_values(UNIQUE_TOKEN_INTEREST, ascending=False).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DFA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k_feature_inds = (feature_acts[1:] > 0).sum(dim=0).nonzero().squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DLA\n",
    "\n",
    "decomp, labels = original_cache.get_full_resid_decomposition(layer =  10, expand_neurons=False, return_labels=True)\n",
    "inds = top_k_feature_inds.squeeze()\n",
    "tok1 = \" but\"\n",
    "print(decomp.shape)\n",
    "dla = (decomp[:,0,:] @ model.W_U[:,model.tokenizer.encode(tok1)]).detach().cpu().squeeze()\n",
    "print(dla.shape)\n",
    "tmp = pd.DataFrame(dla.detach().cpu().numpy().T, index = token_df[\"unique_token\"],\n",
    "                   columns = labels)\n",
    "px.line(\n",
    "    tmp.T\n",
    ").show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's do DLA\n",
    "# decomp, labels = original_cache.get_full_resid_decomposition(layer =  10, expand_neurons=False, return_labels=True)\n",
    "# print(decomp.shape)\n",
    "# inds = top_k_feature_inds.squeeze()\n",
    "# test = (decomp[:,0,POS_INTEREST] @ sparse_autoencoder.W_enc[:,inds])\n",
    "# test = (decomp[:,0,-1] @ sparse_autoencoder.W_dec[inds].T) / sparse_autoencoder.W_enc[:,inds].norm(dim=0)\n",
    "# tmp = pd.DataFrame(test.detach().cpu().numpy().T, columns = labels, index = [f\"feature_{i}\" for i in inds])\n",
    "# px.line(\n",
    "#     tmp.T[tmp.T.index.str.contains(\"mlp\")]\n",
    "# ).show()\n",
    "\n",
    "# px.line(\n",
    "#     tmp.T[tmp.T.index.str.contains(\"L\")]\n",
    "# ).show()\n",
    "\n",
    "# test = (decomp[:,0,-1] @ sparse_autoencoder.W_enc[:,inds])\n",
    "test = (decomp[:,0,POS_INTEREST] @ sparse_autoencoder.W_dec[inds].T) / sparse_autoencoder.W_enc[:,inds].norm(dim=0)\n",
    "tmp = pd.DataFrame(test.detach().cpu().numpy().T, columns = labels, index = [f\"feature_{i}\" for i in inds])\n",
    "px.line(\n",
    "    tmp.T[tmp.T.index.str.contains(\"mlp\")]\n",
    ").show()\n",
    "px.line(\n",
    "    tmp.T[tmp.T.index.str.contains(\"L\")]\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_cache[\"pattern\",0, \"attn\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = pd.DataFrame(original_cache[\"pattern\",7, \"attn\"][0,7].detach().cpu().numpy(), columns = token_df.unique_token, index = token_df.unique_token)\n",
    "px.imshow(tmp, color_continuous_midpoint=0, color_continuous_scale=\"RdBu\", height = 800).show()\n",
    "tmp = pd.DataFrame(original_cache[\"pattern\",8, \"attn\"][0,5].detach().cpu().numpy(), columns = token_df.unique_token, index = token_df.unique_token)\n",
    "px.imshow(tmp, color_continuous_midpoint=0, color_continuous_scale=\"RdBu\", height = 800).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_cache[\"pattern\",8, \"attn\"][0,5][13,7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import circuitsvis as cv \n",
    "\n",
    "tokens = token_df[\"unique_token\"].tolist()\n",
    "# print(\"Layer 0 Head Attention Patterns:\")\n",
    "cv.attention.attention_patterns(\n",
    "    tokens=token_df[\"unique_token\"].tolist(), \n",
    "    attention=original_cache[\"pattern\",7, \"attn\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eff_embed = model.W_E + model.blocks[0].mlp(model.blocks[0].ln2(model.W_E[None]))\n",
    "eff_embed = eff_embed.squeeze()\n",
    "eff_embed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eff_embed_but = eff_embed[model.to_single_token(\" but\")]\n",
    "# eff_embed_but = eff_embed[model.to_single_token(\" but\")]\n",
    "\n",
    "layer = 8\n",
    "head = 5\n",
    "W_QK = model.W_K[layer, head] @ model.W_Q[layer, head].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_cache[utils.get_act_name(\"k\", 8)][0,7,8].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals, inds = torch.topk(eff_embed @ W_QK.T @ eff_embed_but, 30)\n",
    "model.to_str_tokens(inds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metric Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's measure these for one prompt\n",
    "prompt = \"This investigation is not only one that is continuing and worldwide, but also one that we expect to continue for quite some time.\" # Not only ... but\n",
    "token_df, original_cache, cache_reconstructed_query, feature_acts_example = eval_prompt([prompt], model, sparse_autoencoder, head_idx_override=7)\n",
    "# print(token_df.columns)\n",
    "# filter_cols = [\"str_tokens\", \"unique_token\", \"context\", \"batch\", \"pos\", \"label\", \"loss\", \"loss_diff\", \"mse_loss\", \"num_active_features\", \"explained_variance\", \"kl_divergence\",\n",
    "#                \"top_k_features\"]\n",
    "# token_df[filter_cols].style.background_gradient(\n",
    "#     subset=[\"loss_diff\", \"mse_loss\",\"explained_variance\", \"num_active_features\", \"kl_divergence\"],\n",
    "#     cmap=\"coolwarm\")\n",
    "\n",
    "print(prompt)\n",
    "feature_acts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "\n",
    "def analyze_events(tensor):\n",
    "    \n",
    "    assert len(tensor.shape) == 2, \"tensor must be 2D\"\n",
    "    results = []\n",
    "\n",
    "    for row in tensor:\n",
    "        in_event = False\n",
    "        event_start = 0\n",
    "        num_events = 0\n",
    "        max_values = []\n",
    "        avg_values = []\n",
    "        durations = []\n",
    "        start_position = np.NAN\n",
    "        final_position = np.NAN\n",
    "        \n",
    "        for i, value in enumerate(row.tolist()):\n",
    "            if value > 0:\n",
    "                if not in_event:\n",
    "                    in_event = True\n",
    "                    event_start = i\n",
    "                    num_events += 1\n",
    "                    max_value = value\n",
    "                    total_value = value\n",
    "                    start_position = i\n",
    "                else:\n",
    "                    max_value = max(max_value, value)\n",
    "                    total_value += value\n",
    "            else:\n",
    "                if in_event:\n",
    "                    in_event = False\n",
    "                    durations.append(i - event_start)\n",
    "                    max_values.append(max_value)\n",
    "                    avg_values.append(total_value / (i - event_start))\n",
    "                    final_position = i\n",
    "        \n",
    "        if in_event:\n",
    "            durations.append(len(row) - event_start)\n",
    "            max_values.append(max_value)\n",
    "            avg_values.append(total_value / (len(row) - event_start))\n",
    "\n",
    "        \n",
    "        # get the average event duration\n",
    "        avg_duration = (sum(durations) / len(durations)) if len(durations) > 0 else np.NaN\n",
    "        \n",
    "        # max duration \n",
    "        max_duration = max(durations) if len(durations) > 0 else np.NaN\n",
    "        \n",
    "        # get the average max value\n",
    "        avg_max_value = (sum(max_values) / (len(max_values)) if len(max_values) > 0 else np.NaN)\n",
    "        num_firings = sum(durations)\n",
    "        \n",
    "        # `zip` avg_valuea, max_values, durations and add it as a subrecord which we could unfurl later\n",
    "        event_stats = zip(avg_values, max_values, durations)\n",
    "        event_stats = [\n",
    "            {\n",
    "                'avg_value': avg_value,\n",
    "                'max_value': max_value,\n",
    "                'duration': duration,\n",
    "                'start_position': start_position, \n",
    "                'final_position': final_position,\n",
    "            }\n",
    "            for avg_value, max_value, duration in event_stats\n",
    "        ]\n",
    "\n",
    "        results.append({\n",
    "            'num_events': num_events,\n",
    "            'num_firings': num_firings,\n",
    "            'avg_values': avg_values,\n",
    "            'max_values': max_values,\n",
    "            'durations': durations,\n",
    "            'avg_duration': avg_duration,\n",
    "            'max_duration': max_duration,\n",
    "            'avg_max_value': avg_max_value,\n",
    "            'events': event_stats,\n",
    "        })\n",
    "\n",
    "    return results\n",
    "\n",
    "# Example usage\n",
    "\n",
    "\n",
    "tensor =feature_acts_example[:, features_of_interest].T\n",
    "\n",
    "start_time = time.time()\n",
    "results = analyze_events(tensor)\n",
    "end_time = time.time()\n",
    "print(f\"Time taken: {end_time - start_time}\")\n",
    "\n",
    "feature_prompt_df  = pd.DataFrame(results, index=features_of_interest)\n",
    "feature_prompt_df[\"feature\"] = feature_prompt_df.index\n",
    "feature_prompt_df.explode('events').sort_values(\"num_events\", ascending=False)\n",
    "display(feature_prompt_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert events to a dataframe\n",
    "tmp = feature_prompt_df.explode('events').apply(lambda x: pd.Series(x['events']), axis=1).reset_index().rename(columns={\"index\": \"feature\"})\n",
    "tmp[\"feature\"] = tmp[\"feature\"].astype(str)\n",
    "px.scatter_matrix(tmp, \n",
    "                  title=\"Event stats for each feature\", color=\"feature\", dimensions=[\"avg_value\", \"max_value\", \"duration\"],\n",
    "                  width=1000, height=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(feature_prompt_df, x=\"num_firings\", y=\"num_events\", hover_name=feature_prompt_df.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens_list = []\n",
    "pbar = tqdm(range(128*6))\n",
    "for i in pbar:\n",
    "    all_tokens_list.append(activation_store.get_batch_tokens())\n",
    "all_tokens = torch.cat(all_tokens_list, dim=0)\n",
    "print(all_tokens.shape)\n",
    "all_tokens = all_tokens[torch.randperm(all_tokens.shape[0])]\n",
    "tokens = all_tokens[:4096*6]\n",
    "del all_tokens\n",
    "torch.mps.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n_prompts = 1000\n",
    "# features_of_interest = features_of_interest\n",
    "features_of_interest = torch.randperm(sparse_autoencoder.cfg.d_sae)[:100].tolist()\n",
    "token_dfs = []\n",
    "event_dfs = []\n",
    "feature_acts_all = []\n",
    "\n",
    "for prompt_index in tqdm(range(n_prompts)):\n",
    "    prompt_tokens = tokens[prompt_index].unsqueeze(0)\n",
    "    # make token df \n",
    "    token_df = make_token_df(model, prompt_tokens, len_suffix=5, len_prefix=10)\n",
    "    token_df[\"prompt_index\"] = prompt_index\n",
    "    \n",
    "    (original_logits, original_loss), original_cache = model.run_with_cache(prompt_tokens, return_type=\"both\", loss_per_token=True)\n",
    "    token_df['loss'] = original_loss.flatten().tolist() + [np.nan]\n",
    "    \n",
    "    original_act = original_cache[sparse_autoencoder.cfg.hook_point]\n",
    "    sae_out, feature_acts, _, mse_loss, _ = sparse_autoencoder(original_act)\n",
    "\n",
    "    feature_acts_of_interest = feature_acts[0, :, features_of_interest].T\n",
    "    results = analyze_events(feature_acts_of_interest)\n",
    "    events_df  = pd.DataFrame(results, index=features_of_interest)\n",
    "    events_df[\"feature\"] = events_df.index.astype(str)\n",
    "    events_df[\"prompt_index\"] = prompt_index\n",
    "    events_df = events_df[events_df[\"num_events\"] > 0]\n",
    "    \n",
    "        \n",
    "    token_dfs.append(token_df.reset_index(drop=True))\n",
    "    event_dfs.append(events_df.reset_index(drop=True))\n",
    "    feature_acts_all.append(feature_acts_of_interest)\n",
    "    \n",
    "feature_acts_all = torch.stack(feature_acts_all, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_acts_all = torch.stack(feature_acts_all, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_df = pd.concat(token_dfs).reset_index(drop=True)\n",
    "prompt_event_df = pd.concat(event_dfs).reset_index(drop=True)\n",
    "events_df = prompt_event_df.explode('events').apply(lambda x: pd.Series(x['events']), axis=1)\n",
    "events_df[\"feature\"] = events_df.index.map(lambda x: prompt_event_df.feature[x]).astype(str)\n",
    "events_df[\"prompt_index\"] = events_df.index.map(lambda x: prompt_event_df.prompt_index[x])\n",
    "#\n",
    "# tmp[\"feature\"] = tmp.index.map(lambda x: event_df[\"feature\"][x]).astype(str)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_event_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter_matrix(prompt_event_df, \n",
    "                  title=\"Event stats for each feature\", color=\"feature\", dimensions=[\"num_events\", \"num_firings\", \"avg_duration\", \"avg_max_value\"],\n",
    "                  width=1000, height=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_event_agg_df = prompt_event_df.groupby([\"feature\", \"prompt_index\"]).agg({\"num_events\": \"sum\", \"num_firings\": \"sum\", \"avg_duration\": \"mean\"}).sort_values(\"num_events\", ascending=False).reset_index()\n",
    "prompt_event_agg_df[\"firings_per_event\"] = prompt_event_agg_df[\"num_firings\"] / prompt_event_agg_df[\"num_events\"]\n",
    "px.strip(prompt_event_agg_df, x = \"feature\", y = \"firings_per_event\", color=\"feature\", title=\"Firings per event\",\n",
    "         hover_data= [\"num_events\", \"num_firings\", \"avg_duration\", \"prompt_index\"],\n",
    "         ).show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_event_agg_df.feature.unique().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_firings_per_event = prompt_event_agg_df.groupby(\"feature\").firings_per_event.mean().sort_values(ascending=False)\n",
    "std_firings_per_event = prompt_event_agg_df.groupby(\"feature\").firings_per_event.std().sort_values(ascending=False)\n",
    "px.scatter(x=mean_firings_per_event.values, \n",
    "           y = std_firings_per_event.values,\n",
    "           hover_name=mean_firings_per_event.index,\n",
    "           marginal_x=\"histogram\",\n",
    "              marginal_y=\"histogram\",\n",
    "           labels = {\"x\": \"Mean firings per event\", \"y\": \"Std firings per event\"},\n",
    "           title=\"Mean vs Std firings per event\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in mean_firings_per_event[mean_firings_per_event<1.3].index[10:30]:\n",
    "    render_feature_dashboard(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Given some token, let's get the distribution of tokens it began firing on\n",
    "events_df[\"token_df_id\"] = events_df.apply(lambda x: token_df_id_from_prompt_and_pos(x[\"prompt_index\"], x[\"start_position\"]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_df.join(token_df, =\"token_df_id\").head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_event_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we want to get the token distribution from events. \n",
    "feature_idx = features_of_interest.index(22768)\n",
    "token_df[\"feature_22768\"] = feature_acts_all[:, feature_idx].flatten().tolist() \n",
    "# token_df[\"feature_22768_quantile\"] = pd.qcut(token_df[\"feature_22768\"], 10, labels=False, duplicates=\"drop\")\n",
    "idxes = token_df.sort_values(\"feature_22768\", ascending=False).head(30).index\n",
    "idxes_minus_1 = idxes - 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_df.groupby()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_df_id_from_prompt_and_pos = lambda prompt_index, pos: token_df[(token_df[\"prompt_index\"] == prompt_index) & (token_df[\"pos\"] == pos)].index[0]\n",
    "str_token_from_prompt_and_pos = lambda prompt_index, pos: token_df[(token_df[\"prompt_index\"] == prompt_index) & (token_df[\"pos\"] == pos)].str_tokens.values[0]\n",
    "\n",
    "token_df_id_from_prompt_and_pos(12,3)\n",
    "# str_token_from_prompt_and_pos(12,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_df.groupby(\"feature\").agg({\"duration\": \"std\"}).sort_values(\"duration\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start id word_cloud\n",
    "\n",
    "feature_of_interest = 22768\n",
    "\n",
    "# step 1. Get the start and end points for the text we care about\n",
    "events_df[events_df.duration == 4]#[events_df.feature == str(feature_of_interest)]\n",
    "# px.strip(tmp, x = \"duration\", y = \"avg_value\",title=\"Firings per event\")\\\n",
    "    \n",
    "\n",
    "# step 2. for each of these, get prompt\n",
    "token_df_ids = [token_df_id_from_prompt_and_pos(i,j) for i,j in zip(tmp.prompt_index, tmp.start_position)]\n",
    "minus_one_token_ids = [token_df_id_from_prompt_and_pos(i,j) for i,j in zip(tmp.prompt_index, tmp.start_position - 1)]\n",
    "final_token_ids = [token_df_id_from_prompt_and_pos(i,j) for i,j in zip(tmp.prompt_index, tmp.final_position.fillna(128) -1)]\n",
    "minus_one_token_fire = token_df.iloc[minus_one_token_ids].str_tokens.reset_index(drop=True)\n",
    "first_token_fire = token_df.iloc[token_df_ids].str_tokens.reset_index(drop=True)\n",
    "final_token_fire = token_df.iloc[final_token_ids].str_tokens.reset_index(drop=True)\n",
    "\n",
    "tmp = pd.concat([first_token_fire, minus_one_token_fire, final_token_fire], axis=1)\n",
    "\n",
    "tmp.columns = [\"first_token\", \"minus_one_token\", \"final_token\"]\n",
    "tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proxy Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens_list = []\n",
    "pbar = tqdm(range(128*6))\n",
    "for i in pbar:\n",
    "    all_tokens_list.append(activation_store_layer_10.get_batch_tokens())\n",
    "all_tokens = torch.cat(all_tokens_list, dim=0)\n",
    "print(all_tokens.shape)\n",
    "all_tokens = all_tokens[torch.randperm(all_tokens.shape[0])]\n",
    "tokens = all_tokens[:4096*6]\n",
    "del all_tokens\n",
    "torch.mps.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_df = pd.DataFrame(\n",
    "    {\"prompt_index\" : range(tokens.shape[0]),\n",
    "        \"prompt\": [model.to_string(tokens[i]) for i in range(tokens.shape[0])]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "correlative_conjunctions = {\n",
    "    \"both_and\": r\"\\bboth\\b(?:(?!\\.|\\?|!).)*?\\band\\b\",\n",
    "    \"either_or\": r\"\\beither\\b(?:(?!\\.|\\?|!).)*?\\bor\\b\",\n",
    "    \"neither_nor\": r\"\\bneither\\b(?:(?!\\.|\\?|!).)*?\\bnor\\b\",\n",
    "    \"not_only_but_also\": r\"\\bnot\\s+only\\b(?:(?!\\.|\\?|!).)*?\\bbut\\s+also\\b\",\n",
    "    \"whether_or\": r\"\\bwhether\\b(?:(?!\\.|\\?|!).)*?\\bor\\b\",\n",
    "}\n",
    "\n",
    "\n",
    "questions = {\n",
    "    \"general_questions\": r\"\\b(who|what|when|where|why|how)\\b.*?\\?\",\n",
    "    \"how\": r\"\\bhow\\b.*?\\?\",\n",
    "    \"what\": r\"\\bwhat\\b.*?\\?\",\n",
    "    \"when\": r\"\\bwhen\\b.*?\\?\",\n",
    "    \"where\": r\"\\bwhere\\b.*?\\?\",\n",
    "    \"why\": r\"\\bwhy\\b.*?\\?\",\n",
    "    \"who\": r\"\\bwho\\b.*?\\?\",\n",
    "    \"choice_questions\": r\"\\b(do you prefer|would you rather)\\b.*?\\?\",\n",
    "}\n",
    "\n",
    "punctuation = {\n",
    "    \"regular_parentheses\": r\"\\(.*?\\)\",\n",
    "    \"square_brackets\": r\"\\[.*?\\]\",\n",
    "    \"curly_brackets\": r\"\\{.*?\\}\",\n",
    "    \"angle_brackets\": r\"\\<.*?\\>\",\n",
    "    \"double_quotes\": r\"\\\".*?\\\"\",\n",
    "    \"single_quotes\": r\"\\'.*?\\'\",\n",
    "    \"backticks\": r\"`.*?`\",\n",
    "}\n",
    "\n",
    "# lists = {\n",
    "#     \"bulleted_lists\": r\"^\\s*[\\-\\*\\+] .*$\",\n",
    "#     \"numbered_lists\": r\"^\\s*\\d+\\..*$\",\n",
    "#     \"alphabetic_lists\": r\"^\\s*[a-zA-Z]\\..*$\",\n",
    "# }\n",
    "\n",
    "formatting = {\n",
    "    \"specific_html_tag\": r\"\\<div\\>.*?\\</div\\>\",  # Example with 'div' tag\n",
    "    \"any_html_tag\": r\"\\<.*?\\>.*?\\</.*?\\>\",\n",
    "    \"inline_code\": r\"`.*?`\",\n",
    "    \"multiline_code_blocks\": r\"```.*?```\",\n",
    "    \"bold_text_markdown\": r\"\\*\\*.*?\\*\\*\" + \"|\" + r\"__.*?__\",\n",
    "    \"italic_text_markdown\": r\"\\*.*?\\*\" + \"|\" + r\"_.*?_\"\n",
    "}\n",
    "\n",
    "\n",
    "# now all all proxies together in one dict\n",
    "proxies = {}\n",
    "proxies.update(correlative_conjunctions)\n",
    "proxies.update(questions)\n",
    "# proxies.update(punctuation)\n",
    "# proxies.update(lists)\n",
    "# proxies.update(formatting)\n",
    "\n",
    "# create a column for each conjunction in the prompt_df\n",
    "\n",
    "for conjunction, regex in proxies.items():\n",
    "    prompt_df[conjunction] = prompt_df.prompt.str.contains(regex, flags=re.IGNORECASE)\n",
    "    \n",
    "# summarize\n",
    "prompt_df.iloc[:,2:].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proxy_hits = prompt_df.iloc[:,2:7].sum()\n",
    "\n",
    "px.bar(proxy_hits, \n",
    "       # add the number above each bar\n",
    "      text=proxy_hits.values,\n",
    "       title=\"Proxy Hits (out of 24576 prompts)\", \n",
    "       labels={\"value\": \"Number of prompts\"}, width = 500).show()\n",
    "\n",
    "\n",
    "proxy_hits = prompt_df.iloc[:,7:13].sum()\n",
    "\n",
    "px.bar(proxy_hits, \n",
    "       # add the number above each bar\n",
    "      text=proxy_hits.values,\n",
    "       title=\"Proxy Hits (out of 24576 prompts)\", \n",
    "       labels={\"value\": \"Number of prompts\"},\n",
    "       width = 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample from prompts containing proxies\n",
    "import re \n",
    "# import HTML\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "\n",
    "def both_and_highlight(prompt, prompt_proxy_regex):\n",
    "    start_pos = re.search(prompt_proxy_regex, prompt, flags=re.IGNORECASE).start()\n",
    "    end_pos = re.search(prompt_proxy_regex, prompt, flags=re.IGNORECASE).end()\n",
    "    # style with red text\n",
    "    style_tag = \"<span style='color:red'>\"\n",
    "    prompt = prompt[:start_pos] + f'{style_tag}'+ prompt[start_pos:end_pos] + \"</span>\" + prompt[end_pos:]\n",
    "    display(HTML(prompt))\n",
    "\n",
    "for i, row in prompt_df[prompt_df[\"both_and\"]].sample(1).iterrows():\n",
    "    both_and_highlight(row.prompt,proxies[\"both_and\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_acts(prompts, features_of_interest: List, sparse_autoencoder):\n",
    "    \n",
    "    n_prompts = len(prompts)\n",
    "    feature_acts_list = []\n",
    "    \n",
    "    for prompt_index in tqdm(range(n_prompts)):\n",
    "        prompt_tokens = prompts[prompt_index].unsqueeze(0)\n",
    "        # make token df \n",
    "        token_df = make_token_df(model, prompt_tokens, len_suffix=5, len_prefix=10)\n",
    "        token_df[\"prompt_index\"] = prompt_index\n",
    "        \n",
    "        (original_logits, original_loss), original_cache = model.run_with_cache(prompt_tokens, return_type=\"both\", loss_per_token=True)\n",
    "        token_df['loss'] = original_loss.flatten().tolist() + [np.nan]\n",
    "        \n",
    "        original_act = original_cache[sparse_autoencoder.cfg.hook_point]\n",
    "        sae_out, feature_acts, _, mse_loss, _ = sparse_autoencoder(original_act)\n",
    "\n",
    "        feature_acts_list.append(feature_acts[:,:,features_of_interest])\n",
    "        \n",
    "    feature_acts = torch.stack(feature_acts_list, dim=0)\n",
    "    \n",
    "    return feature_acts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A huge amount of annoying data crunching so we have a df with the indexes we care about. (work out where the correlative conjunction appeared and then get the token positions we care about.) Then we are ready to go to feature acts and get the feature acts for all of these positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start token ids are any ids that match \"both\"\n",
    "both_token_strs = [\" both\", \"Both\", \" both\", \"Both\"]\n",
    "both_token_ids = [model.to_single_token(token) for token in both_token_strs]\n",
    "and_token_strs = [\" and\", \"and\", \"And\", \" And\"]\n",
    "and_token_ids = [model.to_single_token(token) for token in and_token_strs]\n",
    "\n",
    "\n",
    "both_and_df = prompt_df[prompt_df[\"both_and\"]][[\"prompt_index\", \"prompt\"]]\n",
    "both_and_df[\"tokens\"] = tokens[prompt_df[\"both_and\"]].detach().cpu().numpy().tolist()\n",
    "both_and_df[\"start_pos\"] = both_and_df.prompt.apply(lambda x: re.search(proxies[\"both_and\"], x, flags=re.IGNORECASE).start())\n",
    "both_and_df[\"end_pos\"] = both_and_df.prompt.apply(lambda x: re.search(proxies[\"both_and\"], x, flags=re.IGNORECASE).end())\n",
    "both_and_df[\"offset_mapping\"] = both_and_df.apply(lambda x: model.tokenizer.encode_plus(x.prompt, return_offsets_mapping=True)[\"offset_mapping\"], axis=1)\n",
    "both_and_df[\"start_offset_mapping\"] = both_and_df.apply(lambda x: [i for i,_ in x[\"offset_mapping\"]], axis=1)\n",
    "both_and_df[\"end_offset_mapping\"] = both_and_df.apply(lambda x: [j for _,j in x[\"offset_mapping\"]], axis=1)\n",
    "both_and_df[\"start_pos_tok_id\"] = both_and_df.apply(lambda x: next(i for i, offset in enumerate(x[\"start_offset_mapping\"]) if offset >= x[\"start_pos\"]), axis=1)\n",
    "both_and_df[\"end_pos_tok_id\"] = both_and_df.apply(lambda x: next((i for i, offset in enumerate(x[\"start_offset_mapping\"]) if offset > x[\"end_pos\"]-1), 127), axis=1)\n",
    "both_and_df[\"start_pos_tok_str\"] = both_and_df.apply(lambda x: model.to_single_str_token(x[\"tokens\"][x[\"start_pos_tok_id\"]]), axis = 1) \n",
    "both_and_df[\"end_pos_tok_str\"] = both_and_df.apply(lambda x: model.to_single_str_token(x[\"tokens\"][x[\"end_pos_tok_id\"]]), axis = 1)\n",
    "both_and_df[[ \"prompt_index\", \"prompt\", \"start_pos_tok_str\", \"end_pos_tok_str\"]]\n",
    "\n",
    "\n",
    "features_of_interest = [21604]\n",
    "both_and_feature_acts = get_feature_acts(tokens[prompt_df[\"both_and\"]], features_of_interest=features_of_interest, sparse_autoencoder=sparse_autoencoder_layer_10)\n",
    "both_and_feature_acts = both_and_feature_acts.squeeze()\n",
    "both_and_feature_acts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "both_and_df[\"n_toks_in_proxy_context\"] = both_and_df.apply(lambda x: x[\"end_pos_tok_id\"] - x[\"start_pos_tok_id\"], axis=1)\n",
    "\n",
    "tmp = both_and_df.n_toks_in_proxy_context.value_counts()\n",
    "# bar chart orderer by index, with text labels for the count\n",
    "px.bar(tmp.sort_index(), text=tmp.values, title=\"Number of tokens in proxy context\",\n",
    "       # not legend\n",
    "        labels={\"n_toks_in_proxy_context\": \"Number of tokens in proxy context\", \"value\": \"Number of prompts\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "def display_prompt_selector(df, indexes, prompt_proxy_regex=proxies[\"both_and\"]):\n",
    "    # Output widget to display the prompt\n",
    "    output = widgets.Output()\n",
    "\n",
    "    # Function to display the prompt example\n",
    "    def on_dropdown_change(change):\n",
    "        with output:\n",
    "            clear_output(wait=True)\n",
    "            index = change['new']\n",
    "            if index in df.index:\n",
    "                # Your logic to display the prompt example\n",
    "                both_and_highlight(df.loc[index].prompt, prompt_proxy_regex=prompt_proxy_regex)\n",
    "            else:\n",
    "                print(\"Invalid selection\")\n",
    "\n",
    "    # Create a dropdown widget for prompt examples\n",
    "    prompt_selector = widgets.Dropdown(\n",
    "        options=['Select a prompt'] + list(indexes),\n",
    "        description='Select Prompt:',\n",
    "        disabled=False,\n",
    "    )\n",
    "\n",
    "    # Observe changes in the dropdown\n",
    "    prompt_selector.observe(on_dropdown_change, names='value')\n",
    "\n",
    "    # Display the widgets and output\n",
    "    display(prompt_selector, output)\n",
    "\n",
    "# Example usage\n",
    "# display_prompt_selector(both_and_df, both_and_df.index)\n",
    "\n",
    "\n",
    "# using the start and end pos token ids from both_and_df to get the feature acts, padding with 0s\n",
    "start_pos_tok_ids = both_and_df[\"start_pos_tok_id\"].tolist()\n",
    "end_pos_tok_ids = both_and_df[\"end_pos_tok_id\"].tolist()\n",
    "aligned_feature_actions = torch.zeros_like(both_and_feature_acts)\n",
    "for i , (start, end) in enumerate(zip(start_pos_tok_ids, end_pos_tok_ids)):\n",
    "    aligned_feature_actions[i, :(end-start+1)] = both_and_feature_acts[i, (start-1):(end)]\n",
    "\n",
    "for gap_length in range(6,10):\n",
    "    length_mask = (both_and_df.n_toks_in_proxy_context == gap_length).values\n",
    "    tmp = pd.DataFrame(aligned_feature_actions[length_mask,:gap_length+2].detach().cpu().numpy().T,\n",
    "                       columns = both_and_df[length_mask].prompt_index.astype(int).to_list())\n",
    "    \n",
    "    feature_present_indexes = tmp.columns[((tmp.values > 0).sum(axis=0) > 0).nonzero()]\n",
    "    feature_missing_indexes = tmp.columns[((tmp.values > 0).sum(axis=0) == 0).nonzero()]\n",
    "    \n",
    "    display(HTML(\"<h2>Feature present</h2>\"))\n",
    "    print(feature_present_indexes)\n",
    "    display_prompt_selector(both_and_df[length_mask], feature_present_indexes)\n",
    "    display(HTML(\"<h2>Feature Absent</h2>\"))\n",
    "    print(feature_missing_indexes)\n",
    "    display_prompt_selector(both_and_df[length_mask], feature_missing_indexes)\n",
    "    px.line(tmp, title=f\"Feature activations for prompts containing 'both ... and' with a gap of {gap_length}\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_1_1 = \"\"\"himself into a shadow war in order to expose it. His only clue is the keyword \"El Dorado.\" He meets Sophie, a woman searching for her older brother who left her with only a message with the same word: \"El Dorado.\" With Sword having also lost his younger sister in the past, both are drawn together by the word,\"\"\"\n",
    "prompt_2_1 = \"\"\"tern and a banana. \"The fan section was louder than it had been all season long, and the fans, of both sides I may add, were thoroughly amused\"\"\"\n",
    "answer = \" and\"\n",
    "utils.test_prompt(prompt_1_1, answer, model)\n",
    "utils.test_prompt(prompt_2_1, answer, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What ... ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start token ids are any ids that match \"both\"\n",
    "what_token_strs = [\" what\", \"What\"]\n",
    "what_token_ids = [model.to_single_token(token) for token in what_token_strs]\n",
    "qmark_token_strs = [\"?\", \" ?\", \"?!\"]\n",
    "qmark_token_ids = [model.to_single_token(token) for token in and_token_strs]\n",
    "\n",
    "\n",
    "what_question_df = prompt_df[prompt_df[\"what\"]][[\"prompt_index\", \"prompt\"]]\n",
    "what_question_df[\"tokens\"] = tokens[prompt_df[\"what\"]].detach().cpu().numpy().tolist()\n",
    "what_question_df[\"start_pos\"] = what_question_df.prompt.apply(lambda x: re.search(proxies[\"what\"], x, flags=re.IGNORECASE).start())\n",
    "what_question_df[\"end_pos\"] = what_question_df.prompt.apply(lambda x: re.search(proxies[\"what\"], x, flags=re.IGNORECASE).end())\n",
    "what_question_df[\"offset_mapping\"] = what_question_df.apply(lambda x: model.tokenizer.encode_plus(x.prompt, return_offsets_mapping=True)[\"offset_mapping\"], axis=1)\n",
    "what_question_df[\"start_offset_mapping\"] = what_question_df.apply(lambda x: [i for i,_ in x[\"offset_mapping\"]], axis=1)\n",
    "what_question_df[\"end_offset_mapping\"] = what_question_df.apply(lambda x: [j for _,j in x[\"offset_mapping\"]], axis=1)\n",
    "what_question_df[\"start_pos_tok_id\"] = what_question_df.apply(lambda x: next(i for i, offset in enumerate(x[\"start_offset_mapping\"]) if offset >= x[\"start_pos\"]), axis=1)\n",
    "what_question_df[\"end_pos_tok_id\"] = what_question_df.apply(lambda x: next((i for i, offset in enumerate(x[\"start_offset_mapping\"]) if offset > x[\"end_pos\"]-1), 127), axis=1)\n",
    "what_question_df[\"start_pos_tok_str\"] = what_question_df.apply(lambda x: model.to_single_str_token(x[\"tokens\"][x[\"start_pos_tok_id\"]]), axis = 1) \n",
    "what_question_df[\"end_pos_tok_str\"] = what_question_df.apply(lambda x: model.to_single_str_token(x[\"tokens\"][x[\"end_pos_tok_id\"]]), axis = 1)\n",
    "what_question_df[[ \"prompt_index\", \"prompt\", \"start_pos_tok_str\", \"end_pos_tok_str\"]]\n",
    "\n",
    "\n",
    "features_of_interest = [18962]\n",
    "what_question_acts = get_feature_acts(tokens[prompt_df[\"what\"]], features_of_interest=features_of_interest)\n",
    "what_question_acts = what_question_acts.squeeze()\n",
    "\n",
    "\n",
    "what_question_df[\"n_toks_in_proxy_context\"] = what_question_df.apply(lambda x: x[\"end_pos_tok_id\"] - x[\"start_pos_tok_id\"], axis=1)\n",
    "\n",
    "tmp = what_question_df.n_toks_in_proxy_context.value_counts()\n",
    "# bar chart orderer by index, with text labels for the count\n",
    "px.bar(tmp.sort_index(), text=tmp.values, title=\"Number of tokens in proxy context\",\n",
    "       # not legend\n",
    "        labels={\"n_toks_in_proxy_context\": \"Number of tokens in proxy context\", \"value\": \"Number of prompts\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the start and end pos token ids from what_question_df to get the feature acts, padding with 0s\n",
    "start_pos_tok_ids = what_question_df[\"start_pos_tok_id\"].tolist()\n",
    "end_pos_tok_ids = what_question_df[\"end_pos_tok_id\"].tolist()\n",
    "aligned_feature_actions = torch.zeros_like(what_question_acts)\n",
    "for i , (start, end) in enumerate(zip(start_pos_tok_ids, end_pos_tok_ids)):\n",
    "    aligned_feature_actions[i, :(end-start+1)] = what_question_acts[i, (start-1):(end)]\n",
    "\n",
    "for gap_length in range(5,9):\n",
    "    length_mask = (what_question_df.n_toks_in_proxy_context == gap_length).values\n",
    "    tmp = pd.DataFrame(aligned_feature_actions[length_mask,:gap_length+2].detach().cpu().numpy().T,\n",
    "                       columns = what_question_df[length_mask].prompt_index.astype(int).to_list())\n",
    "    \n",
    "    feature_present_indexes = tmp.columns[((tmp.values > 0).sum(axis=0) > 0).nonzero()]\n",
    "    feature_missing_indexes = tmp.columns[((tmp.values > 0).sum(axis=0) == 0).nonzero()]\n",
    "    \n",
    "    display(HTML(\"<h2>Feature present</h2>\"))\n",
    "    print(feature_present_indexes)\n",
    "    display_prompt_selector(what_question_df[length_mask], feature_present_indexes, prompt_proxy_regex=proxies[\"what\"])\n",
    "    display(HTML(\"<h2>Feature Absent</h2>\"))\n",
    "    print(feature_missing_indexes)\n",
    "    display_prompt_selector(what_question_df[length_mask], feature_missing_indexes, prompt_proxy_regex=proxies[\"what\"])\n",
    "    px.line(tmp, title=f\"Feature activations for prompts containing 'both ... and' with a gap of {gap_length}\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DFA to get heads\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_index = 2\n",
    "prompts = tokens[prompt_df[\"both_and\"]]\n",
    "features_of_interest = 21604\n",
    "feature_dir = sparse_autoencoder_layer_10.W_dec[features_of_interest].cpu()\n",
    "\n",
    "prompt_tokens = prompts[prompt_index].unsqueeze(0)\n",
    "text = model.to_string(prompt_tokens)[0]\n",
    "offset_mapping = model.tokenizer.encode_plus(text, return_offsets_mapping=True)[\"offset_mapping\"]\n",
    "start_offset_mapping = [i for i,_ in offset_mapping]\n",
    "end_offset_mapping = [j for _,j in offset_mapping]\n",
    "(original_logits, original_loss), original_cache = model.run_with_cache(prompt_tokens, return_type=\"both\", loss_per_token=True)\n",
    "\n",
    "decomp, labels = original_cache.get_full_resid_decomposition(layer =  10, expand_neurons=False, return_labels=True)\n",
    "original_act = original_cache[sparse_autoencoder_layer_10.cfg.hook_point]\n",
    "sae_out, feature_acts, _, mse_loss, _ = sparse_autoencoder_layer_10(original_act)\n",
    "feature_acts[:,:,features_of_interest].shape\n",
    "\n",
    "\n",
    "# Now I want to know where the proxy starts, where the proxy ends, and where the feature starts and ends\n",
    "\n",
    "\n",
    "# get the start and end positions of the proxy\n",
    "proxy_start_pos = re.search(proxies[\"both_and\"], text, flags=re.IGNORECASE).start()\n",
    "start_pos_tok_id = next(i for i, offset in enumerate(start_offset_mapping) if offset >= proxy_start_pos)\n",
    "proxy_end_pos = re.search(proxies[\"both_and\"], text, flags=re.IGNORECASE).end()\n",
    "end_pos_tok_id = next((i for i, offset in enumerate(start_offset_mapping) if offset > proxy_end_pos-1), 127)\n",
    "\n",
    "n_offset = 5\n",
    "# get the start and end positions of the feature\n",
    "feature_fired = (feature_acts[:,:,features_of_interest].squeeze() > 0)\n",
    "feature_fired[:start_pos_tok_id - n_offset] = False\n",
    "\n",
    "start_feature_pos = feature_fired.nonzero().min().item()\n",
    "end_feature_pos = feature_fired.nonzero().max().item()\n",
    "\n",
    "\n",
    "result_metrics = {\n",
    "    \"start_pos_tok_id\": start_pos_tok_id,\n",
    "    \"end_pos_tok_id\": end_pos_tok_id,\n",
    "    \"start_feature_pos\": start_feature_pos,\n",
    "    \"end_feature_pos\": end_feature_pos,\n",
    "    \"fired_early\": start_feature_pos < start_pos_tok_id,\n",
    "}\n",
    "\n",
    "# now we want to the decomp for the first instance of the proxy\n",
    "projection = decomp.squeeze(1)[:, start_pos_tok_id].cpu() @ feature_dir\n",
    "print(result_metrics)\n",
    "\n",
    "\n",
    "def get_feature_acts_and_projection_at_start_pos(\n",
    "    prompts, features_of_interest: List, feature_dir: torch.Tensor, n_offset: int = 5,\n",
    "    ablate_head = None,\n",
    "    sparse_autoencoder = sparse_autoencoder_layer_10\n",
    "):\n",
    "    \n",
    "    n_prompts = len(prompts)\n",
    "    token_dfs = []\n",
    "    feature_acts_list = []\n",
    "    projections = []\n",
    "    attn_patterns = []\n",
    "    \n",
    "    if ablate_head is not None:\n",
    "        head_layer, head_idx = ablate_head\n",
    "        head_hook_result_name = f\"blocks.{head_layer}.attn.hook_z\"\n",
    "    \n",
    "        def hook_to_ablate_head(head_output: Float[Tensor, \"batch seq_len head_idx d_head\"], hook: HookPoint, head = (head_layer, head_idx), pos = -1):\n",
    "            # print(hook.layer(), hook.name)\n",
    "            assert head[0] == hook.layer(), f\"{head[0]} != {hook.layer()}\"\n",
    "            assert (\"result\" in hook.name) or (\"q\" in hook.name) or (\"z\" in hook.name)\n",
    "            # print(head_output.shape)\n",
    "            head_output[:, pos, head[1], :] = 0\n",
    "            return head_output\n",
    "\n",
    "    \n",
    "    \n",
    "    for prompt_index in tqdm(range(n_prompts)):\n",
    "        prompt_tokens = prompts[prompt_index].unsqueeze(0)\n",
    "        text = model.to_string(prompt_tokens)[0]\n",
    "        offset_mapping = model.tokenizer.encode_plus(text, return_offsets_mapping=True)[\"offset_mapping\"]\n",
    "        start_offset_mapping = [i for i,_ in offset_mapping]\n",
    "        end_offset_mapping = [j for _,j in offset_mapping]\n",
    "        \n",
    "        # make token df \n",
    "        token_df = make_token_df(model, prompt_tokens, len_suffix=10, len_prefix=10)\n",
    "        token_df[\"prompt_index\"] = prompt_index\n",
    "\n",
    "        \n",
    "        # work out where the proxy starts and ends\n",
    "        proxy_start_pos = re.search(proxies[\"both_and\"], text, flags=re.IGNORECASE).start()\n",
    "        start_pos_tok_id = next(i for i, offset in enumerate(start_offset_mapping) if offset >= proxy_start_pos)\n",
    "        proxy_end_pos = re.search(proxies[\"both_and\"], text, flags=re.IGNORECASE).end()\n",
    "        end_pos_tok_id = next((i for i, offset in enumerate(start_offset_mapping) if offset > proxy_end_pos-1), 127)\n",
    "\n",
    "\n",
    "        \n",
    "        if ablate_head is None:\n",
    "            _, original_cache = model.run_with_cache(prompt_tokens, return_type=\"both\", loss_per_token=True)\n",
    "        else:\n",
    "            pos_to_ablate_feature = start_pos_tok_id # feature usually fires after the proxy token.\n",
    "            hook_to_ablate_head_with_pos = partial(hook_to_ablate_head, head = ablate_head, pos = pos_to_ablate_feature)\n",
    "            with model.hooks(fwd_hooks=[(head_hook_result_name, hook_to_ablate_head_with_pos)]):\n",
    "                _, original_cache = model.run_with_cache(prompt_tokens, return_type=\"both\", loss_per_token=True)\n",
    "            \n",
    "        \n",
    "        original_act = original_cache[sparse_autoencoder.cfg.hook_point]\n",
    "        sae_out, feature_acts, _, mse_loss, _ = sparse_autoencoder(original_act)\n",
    "        feature_acts_list.append(feature_acts[:,:,features_of_interest])\n",
    "\n",
    "\n",
    "        # get the start and end positions of the feature\n",
    "        feature_fired = (feature_acts[:,:,features_of_interest].squeeze() > 0)\n",
    "        feature_fired[:start_pos_tok_id - n_offset] = False\n",
    "\n",
    "        start_feature_pos = feature_fired.nonzero().min().item()\n",
    "        end_feature_pos = feature_fired.nonzero().max().item()\n",
    "\n",
    "        result_metrics = {\n",
    "            \"start_pos_tok_id\": start_pos_tok_id,\n",
    "            \"end_pos_tok_id\": end_pos_tok_id,\n",
    "            \"start_end_proxy_gap\": end_pos_tok_id - start_pos_tok_id,\n",
    "            \"start_feature_pos\": start_feature_pos,\n",
    "            \"end_feature_pos\": end_feature_pos,\n",
    "            \"start_end_feature_gap\": end_feature_pos - start_feature_pos,\n",
    "            \"fired_early\": start_feature_pos < start_pos_tok_id,\n",
    "        }\n",
    "        \n",
    "        # get the decomp\n",
    "        decomp, labels = original_cache.get_full_resid_decomposition(layer =  10, expand_neurons=False, return_labels=True)\n",
    "        projection = decomp.squeeze(1)[:, start_pos_tok_id].cpu() @ feature_dir\n",
    "        projections.append(projection)\n",
    "        \n",
    "        # get the attention pattern for L4H1\n",
    "        layer = 4\n",
    "        head =1\n",
    "        attn_pattern = original_cache[utils.get_act_name(\"pattern\",layer)].squeeze(0)[head,:].cpu()\n",
    "        attn_patterns.append(attn_pattern)\n",
    "        \n",
    "        token_df = token_df.iloc[start_pos_tok_id]\n",
    "        for metric, value in result_metrics.items():\n",
    "            token_df[metric] = value\n",
    "        token_dfs.append(token_df)\n",
    "        \n",
    "    feature_acts = torch.stack(feature_acts_list, dim=0)\n",
    "    projections = torch.stack(projections, dim=0)\n",
    "    token_df = pd.concat(token_dfs, axis =1).T\n",
    "    attn_patterns = torch.stack(attn_patterns, dim=0)\n",
    "    \n",
    "    return token_df, feature_acts, projections, labels, attn_patterns\n",
    "\n",
    "\n",
    "token_df_test, feature_acts_test, projections_test, labels, attn_patterns = get_feature_acts_and_projection_at_start_pos(\n",
    "    tokens[prompt_df[\"both_and\"]], features_of_interest=features_of_interest, feature_dir=feature_dir\n",
    ")\n",
    "\n",
    "token_df_test_ablate, feature_acts_test_ablate, projections_test_ablate, labels, attn_patterns_ablate = get_feature_acts_and_projection_at_start_pos(\n",
    "    tokens[prompt_df[\"both_and\"]], features_of_interest=features_of_interest, feature_dir=feature_dir,\n",
    "    ablate_head=(4,1)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projections_test_ablate.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"<|endoftext|> a competitive CrossFitter to get into this mentality that you’re better than a “normal” member, or that your activity in the gym has more value. What if we changed our mindset and looked at it the other way around? Not to burst any bubbles, but if you – as a competitive athlete – to think that you working<|endoftext|>Although there is little case law on medical cannabis use in the Canadian workplace, there are a few cases that can guide both employees\" #and employers on this topic. \"\n",
    "# both_and_highlight(prompt, proxies[\"both_and\"])\n",
    "answer = \"and\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head_layer = 4\n",
    "HEAD_HOOK_RESULT_NAME = f\"blocks.{head_layer}.attn.hook_z\"\n",
    "head_idx = 1\n",
    "def hook_to_ablate_head(head_output: Float[Tensor, \"batch seq_len head_idx d_head\"], hook: HookPoint, head = (head_layer, head_idx), pos = -1):\n",
    "    print(hook.layer(), hook.name)\n",
    "    assert head[0] == hook.layer(), f\"{head[0]} != {hook.layer()}\"\n",
    "    assert (\"result\" in hook.name) or (\"q\" in hook.name) or (\"z\" in hook.name)\n",
    "    print(head_output.shape)\n",
    "    head_output[:, pos, head[1], :] = 0\n",
    "    return head_output\n",
    "\n",
    "HTML(\"<h2>Original</h2>\")\n",
    "utils.test_prompt(prompt, answer, model)\n",
    "\n",
    "HTML(\"<h2>With head ablated</h2>\")\n",
    "with model.hooks(fwd_hooks=[(HEAD_HOOK_RESULT_NAME, hook_to_ablate_head)]):\n",
    "    utils.test_prompt(prompt, answer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas don't limit column width\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "display(token_df_test.head(10))\n",
    "print(token_df_test.fired_early.mean())\n",
    "print(token_df_test.start_end_proxy_gap.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize DFA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting_df = pd.DataFrame({\n",
    "    \"labels\": labels,\n",
    "    \"projection\": projections_test.mean(0).numpy(),\n",
    "    \"projection_ablate\": projections_test_ablate.mean(0).numpy(),\n",
    "    \"projection_diff\":  projections_test_ablate.mean(0).numpy() - projections_test.mean(0).numpy(),\n",
    "})\n",
    "\n",
    "px.line(plotting_df, x = \"labels\",  y = [\"projection\", \"projection_ablate\", \"projection_diff\"], title=\"Projection of proxy onto feature\",  width = 1000, height=500).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "head_mask = torch.tensor([1 if re.match(r\"L\\d+H\\d+\", label) is not None else 0 for label in labels], dtype=torch.bool)\n",
    "head_dfa = projections_test[:,head_mask].reshape(-1, sparse_autoencoder_layer_10.cfg.hook_point_layer, model.cfg.n_heads)\n",
    "head_df_ablation = projections_test_ablate[:,head_mask].reshape(-1, sparse_autoencoder_layer_10.cfg.hook_point_layer, model.cfg.n_heads)\n",
    "\n",
    "px.imshow(head_dfa[token_df_test.fired_early.values==False].median(0).values,\n",
    "            title=\"DFA into Feature Decoder Direction by Head\",\n",
    "            color_continuous_midpoint=0,\n",
    "            color_continuous_scale=\"RdBu\",\n",
    "            height = 500,\n",
    "            width = 500,\n",
    "            labels=dict(x=\"Head\", y=\"Layer\")).show()\n",
    "\n",
    "px.imshow(head_df_ablation[token_df_test_ablate.fired_early.values==False].median(0).values,\n",
    "            title=\"DFA into Feature Decoder Direction by Head\",\n",
    "            color_continuous_midpoint=0,\n",
    "            color_continuous_scale=\"RdBu\",\n",
    "            height = 500,\n",
    "            width = 500,\n",
    "            labels=dict(x=\"Head\", y=\"Layer\")).show()\n",
    "\n",
    "# px.imshow(head_dfa[token_df_test.fired_early.values==False].std(0),\n",
    "#             title=\"DFA into Feature Decoder Direction by Head\",\n",
    "#             color_continuous_midpoint=0,\n",
    "#             color_continuous_scale=\"RdBu\",\n",
    "#             height = 500,\n",
    "#             width = 500,\n",
    "#             labels=dict(x=\"Head\", y=\"Layer\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize Attn Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for gap_duration in range(2, 5):\n",
    "    tmp_attn_patterns = attn_patterns[\n",
    "        token_df_test.query(\"fired_early == False\")\n",
    "        .query(f\"start_end_proxy_gap == {gap_duration}\")\n",
    "        .index\n",
    "    ]\n",
    "    filtered_attn_patterns = torch.stack(\n",
    "        [\n",
    "            attn_patterns[\n",
    "                i,\n",
    "                (token_df_test.start_pos_tok_id.iloc[i] - 2) : (\n",
    "                    token_df_test.start_pos_tok_id.iloc[i] + gap_duration\n",
    "                ),\n",
    "                (token_df_test.start_pos_tok_id.iloc[i] - 2) : (\n",
    "                    token_df_test.start_pos_tok_id.iloc[i] + gap_duration\n",
    "                ),\n",
    "            ]\n",
    "            for i in range(tmp_attn_patterns.shape[0])\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    print(filtered_attn_patterns.shape)\n",
    "    # px.imshow(filtered_attn_patterns.detach().cpu(),animation_frame=0, color_continuous_midpoint=0, color_continuous_scale=\"RdBu\", height=500, width=500).show()\n",
    "    px.imshow(\n",
    "        filtered_attn_patterns.detach().cpu().mean(0),\n",
    "        color_continuous_midpoint=0,\n",
    "        color_continuous_scale=\"RdBu\",\n",
    "        height=500,\n",
    "        width=500,\n",
    "        title = f\"Attention pattern 'both ... and' with a gap of {gap_duration}\",\n",
    "    ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Features Firing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_acts_test.nonzero()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(token_df_test, \n",
    "           marginal_x=\"histogram\",\n",
    "              marginal_y=\"histogram\",\n",
    "           x=\"start_pos_tok_id\", y=\"start_feature_pos\", color=\"fired_early\", title=\"Feature firing position vs proxy start position\", width = 1000).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(-1,9):\n",
    "    firing_indexes = torch.tensor(token_df_test.start_pos_tok_id.astype(int).values)\n",
    "    positions = (torch.min(firing_indexes + i, 127*torch.ones_like(firing_indexes))).unsqueeze(0).T\n",
    "    feature_activation_at_first_proxy_pos = feature_acts_test.cpu().squeeze().gather(1, positions).unsqueeze(0)\n",
    "    feature_activation_with_ablation_at_first_proxy_pos = feature_acts_test_ablate.cpu().squeeze().gather(1, positions).unsqueeze(0)\n",
    "\n",
    "    plotting_df = token_df_test.copy()\n",
    "    plotting_df[\"feature\"] = feature_activation_at_first_proxy_pos.squeeze().numpy()\n",
    "    plotting_df[\"feature_with_head_ablated\"] = feature_activation_with_ablation_at_first_proxy_pos.squeeze().numpy()\n",
    "    plotting_df[\"ablation_diff\"] = feature_activation_at_first_proxy_pos.squeeze().numpy() - feature_activation_with_ablation_at_first_proxy_pos.squeeze().numpy()\n",
    "\n",
    "    fig = px.scatter(\n",
    "        plotting_df[plotting_df.start_end_feature_gap >= i].query(\"fired_early == False\"),\n",
    "        x=\"feature\",\n",
    "        y=\"feature_with_head_ablated\",\n",
    "        hover_data=[\"prompt_index\", \"start_end_feature_gap\"],\n",
    "        title=f\"Feature activation at proxy position {i} with ablation\",\n",
    "        width = 1000,\n",
    "    )\n",
    "    # add y=x from 0 to 40\n",
    "    fig.add_shape(\n",
    "        type=\"line\", line=dict(dash=\"dash\"), x0=0, y0=0, x1=40, y1=40\n",
    "    )\n",
    "    fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the start and end pos token ids from both_and_df to get the feature acts, padding with 0s\n",
    "start_pos_tok_ids = both_and_df[\"start_pos_tok_id\"].tolist()\n",
    "end_pos_tok_ids = both_and_df[\"end_pos_tok_id\"].tolist()\n",
    "aligned_feature_actions = torch.zeros_like(both_and_feature_acts)\n",
    "aligned_feature_ablate = torch.zeros_like(both_and_feature_acts)\n",
    "for i , (start, end) in enumerate(zip(start_pos_tok_ids, end_pos_tok_ids)):\n",
    "    aligned_feature_actions[i, :(end-start+1)] = both_and_feature_acts[i, (start-1):(end)]\n",
    "\n",
    "for i , (start, end) in enumerate(zip(start_pos_tok_ids, end_pos_tok_ids)):\n",
    "    aligned_feature_ablate[i, :(end-start+1)] = feature_acts_test_ablate[i, 0, (start-1):(end)]\n",
    "\n",
    "\n",
    "for gap_length in range(1,3):\n",
    "    length_mask = (both_and_df.n_toks_in_proxy_context == gap_length).values\n",
    "    tmp = pd.DataFrame(aligned_feature_actions[length_mask,:gap_length+2].detach().cpu().numpy().T,\n",
    "                       columns = both_and_df[length_mask].prompt_index.astype(int).to_list())\n",
    "    \n",
    "    feature_present_indexes = tmp.columns[((tmp.values > 0).sum(axis=0) > 0).nonzero()]\n",
    "    feature_missing_indexes = tmp.columns[((tmp.values > 0).sum(axis=0) == 0).nonzero()]\n",
    "    \n",
    "    display(HTML(\"<h2>Feature present</h2>\"))\n",
    "    print(feature_present_indexes)\n",
    "    display_prompt_selector(both_and_df[length_mask], feature_present_indexes)\n",
    "    display(HTML(\"<h2>Feature Absent</h2>\"))\n",
    "    print(feature_missing_indexes)\n",
    "    display_prompt_selector(both_and_df[length_mask], feature_missing_indexes)\n",
    "    px.line(tmp, title=f\"Feature activations for prompts containing 'both ... and' with a gap of {gap_length}\", width = 1000).show()\n",
    "    \n",
    "    \n",
    "    tmp = pd.DataFrame(aligned_feature_ablate[length_mask,:gap_length+2].detach().cpu().numpy().T,\n",
    "                    columns = both_and_df[length_mask].prompt_index.astype(int).to_list())\n",
    "    \n",
    "    feature_present_indexes = tmp.columns[((tmp.values > 0).sum(axis=0) > 0).nonzero()]\n",
    "    feature_missing_indexes = tmp.columns[((tmp.values > 0).sum(axis=0) == 0).nonzero()]\n",
    "    \n",
    "    display(HTML(\"<h2>Feature present</h2>\"))\n",
    "    print(feature_present_indexes)\n",
    "    display_prompt_selector(both_and_df[length_mask], feature_present_indexes)\n",
    "    display(HTML(\"<h2>Feature Absent</h2>\"))\n",
    "    print(feature_missing_indexes)\n",
    "    display_prompt_selector(both_and_df[length_mask], feature_missing_indexes)\n",
    "    px.line(tmp, title=f\"Feature activations for prompts containing 'both ... and' with a gap of {gap_length}\", width = 1000).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Do a deep dive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_sample = torch.randint(prompts.shape[0], (1,)).item()\n",
    "prompt_tokens = prompts[random_sample].unsqueeze(0)\n",
    "text = model.to_string(prompt_tokens)[0]\n",
    "offset_mapping = model.tokenizer.encode_plus(text, return_offsets_mapping=True)[\"offset_mapping\"]\n",
    "start_offset_mapping = [i for i,_ in offset_mapping]\n",
    "end_offset_mapping = [j for _,j in offset_mapping]\n",
    "both_and_highlight(text, prompt_proxy_regex=proxies[\"both_and\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make token df \n",
    "token_df = make_token_df(model, prompt_tokens, len_suffix=10, len_prefix=10)\n",
    "token_df[\"prompt_index\"] = prompt_index\n",
    "\n",
    "with suppress_output():\n",
    "    _, original_cache = model.run_with_cache(prompt_tokens, return_type=\"both\", loss_per_token=True)\n",
    "\n",
    "original_act = original_cache[sparse_autoencoder.cfg.hook_point]\n",
    "sae_out, feature_acts, _, mse_loss, _ = sparse_autoencoder(original_act)\n",
    "\n",
    "# work out where the proxy starts and ends\n",
    "proxy_start_pos = re.search(proxies[\"both_and\"], text, flags=re.IGNORECASE).start()\n",
    "start_pos_tok_id = next(i for i, offset in enumerate(start_offset_mapping) if offset >= proxy_start_pos)\n",
    "proxy_end_pos = re.search(proxies[\"both_and\"], text, flags=re.IGNORECASE).end()\n",
    "end_pos_tok_id = next((i for i, offset in enumerate(start_offset_mapping) if offset > proxy_end_pos-1), 127)\n",
    "\n",
    "\n",
    "# get the start and end positions of the feature\n",
    "feature_fired = (feature_acts[:,:,features_of_interest].squeeze() > 0)\n",
    "feature_fired[:start_pos_tok_id - n_offset] = False\n",
    "\n",
    "start_feature_pos = feature_fired.nonzero().min().item()\n",
    "end_feature_pos = feature_fired.nonzero().max().item()\n",
    "\n",
    "result_metrics = {\n",
    "    \"start_pos_tok_id\": start_pos_tok_id,\n",
    "    \"end_pos_tok_id\": end_pos_tok_id,\n",
    "    \"start_end_proxy_gap\": end_pos_tok_id - start_pos_tok_id,\n",
    "    \"start_feature_pos\": start_feature_pos,\n",
    "    \"end_feature_pos\": end_feature_pos,\n",
    "    \"start_end_feature_gap\": end_feature_pos - start_feature_pos,\n",
    "    \"fired_early\": start_feature_pos < start_pos_tok_id,\n",
    "}\n",
    "\n",
    "# get the decomp\n",
    "decomp, labels = original_cache.get_full_resid_decomposition(layer =  10, expand_neurons=False, return_labels=True)\n",
    "projection = decomp.squeeze(1)[:, start_pos_tok_id].cpu() @ feature_dir\n",
    "\n",
    "# get the attention pattern for L4H1\n",
    "layer = 4\n",
    "head =1\n",
    "attn_pattern = original_cache[utils.get_act_name(\"pattern\",layer)].squeeze(0)[head,:].cpu()\n",
    "\n",
    "\n",
    "\n",
    "tmp_df = pd.DataFrame(attn_pattern, columns = token_df.unique_token.values, index=token_df.unique_token.values)\n",
    "\n",
    "start_fig_idx = start_feature_pos - 2\n",
    "end_fig_idx = end_feature_pos+3\n",
    "px.imshow(\n",
    "    tmp_df.iloc[start_fig_idx:end_fig_idx, start_fig_idx:end_fig_idx],\n",
    "    title=\"Attention pattern for L4H1\",\n",
    "    color_continuous_midpoint=0,\n",
    "    color_continuous_scale=\"RdBu\",\n",
    "    height=1000,\n",
    "    width=1000,\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "what if we look at attn to both vs feature activation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.line(feature_acts[0,start_fig_idx:(start_fig_idx+8),features_of_interest].squeeze().detach().cpu(), title=\"Feature activations for 'both ... and'\").show()\n",
    "px.line(attn_pattern[start_fig_idx:(start_fig_idx+8), start_feature_pos-1], title=\"Attention to \\\"Both\\\" activations for 'both ... and'\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### End Deep Dive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mats_sae_training",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
