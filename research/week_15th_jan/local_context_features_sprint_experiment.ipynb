{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append(\"../..\")\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from importlib import reload\n",
    "from tqdm import tqdm\n",
    "\n",
    "import joseph\n",
    "from joseph.analysis import *\n",
    "from joseph.visualisation import *\n",
    "from joseph.utils import *\n",
    "from joseph.data import *\n",
    "\n",
    "\n",
    "reload(joseph.analysis)\n",
    "reload(joseph.visualisation)\n",
    "reload(joseph.utils)\n",
    "reload(joseph.data)\n",
    "\n",
    "from joseph.analysis import *\n",
    "from joseph.visualisation import *\n",
    "from joseph.utils import *\n",
    "from joseph.data import *\n",
    "\n",
    "# turn torch grad tracking off\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = HookedTransformer.from_pretrained(\n",
    "    \"gpt2-small\",\n",
    "    # \"pythia-2.8b\",\n",
    "    # \"pythia-70m-deduped\",\n",
    "    # \"tiny-stories-2L-33M\",\n",
    "    # \"attn-only-2l\",\n",
    "    # center_unembed=True,\n",
    "    # center_writing_weights=True,\n",
    "    # fold_ln=True,\n",
    "    # refactor_factored_attn_matrices=True,\n",
    "    fold_ln=True,\n",
    ")\n",
    "model.set_use_split_qkv_input(True)\n",
    "model.set_use_attn_result(True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load SAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "path = \"../week_8_jan/artifacts/sparse_autoencoder_gpt2-small_blocks.10.hook_resid_pre_49152:v28/1100001280_sparse_autoencoder_gpt2-small_blocks.10.hook_resid_pre_49152.pt\"\n",
    "# path = \"../week_8_jan/artifacts/sparse_autoencoder_gpt2-small_blocks.5.hook_resid_pre_49152:v9/final_sparse_autoencoder_gpt2-small_blocks.5.hook_resid_pre_49152.pt\"\n",
    "sparse_autoencoder = SparseAutoencoder.load_from_pretrained(path)\n",
    "\n",
    "print(sparse_autoencoder.cfg)\n",
    "\n",
    "\n",
    "# sanity check\n",
    "text = \"Many important transition points in the history of science have been moments when science 'zoomed in.' At these points, we develop a visualization or tool that allows us to see the world in a new level of detail, and a new field of science develops to study the world through this lens.\"\n",
    "model(text, return_type=\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sae_training.utils import LMSparseAutoencoderSessionloader\n",
    "model, sparse_autoencoder, activation_store = LMSparseAutoencoderSessionloader.load_session_from_pretrained(\n",
    "    path\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Dashboard generator util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import webbrowser\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "path_to_html = \"../week_8_jan/gpt2_small_features\"\n",
    "def render_feature_dashboard(feature_id):\n",
    "    \n",
    "    path = f\"{path_to_html}/data_{feature_id:04}.html\"\n",
    "    \n",
    "    print(f\"Feature {feature_id}\")\n",
    "    if os.path.exists(path):\n",
    "        # with open(path, \"r\") as f:\n",
    "        #     html = f.read()\n",
    "        #     display(HTML(html))\n",
    "        webbrowser.open_new_tab(\"file://\" + os.path.abspath(path))\n",
    "    else:\n",
    "        print(\"No HTML file found\")\n",
    "    \n",
    "    return\n",
    "\n",
    "# for feature in [100,300,400]:\n",
    "#     render_feature_dashboard(feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fun Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt1 = \"The war caused not only destruction and death but also generations of hatred between the two communities.\"\n",
    "prompt2 = \"The car not only is economical but also feels good to drive.\"\n",
    "prompt3 = \"This investigation is not only one that is continuing and worldwide,\"  # but also one that we expect to continue for quite some time.\"\n",
    "prompt = prompt3\n",
    "answer = \"but\"\n",
    "model.reset_hooks()\n",
    "utils.test_prompt(prompt, answer, model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joseph\n",
    "reload(joseph.analysis)\n",
    "from joseph.analysis import *\n",
    "\n",
    "# prompt3 = \"This investigation is not only one that is continuing and worldwide, but also one that we expect to continue for quite some time.\" # Not only ... but\n",
    "# prompt3 = \"The market is evolving rapidly. Either we must adjust our strategy to meet the new market demands, or we risk falling behind our competitors significantly.\" # either or one (dud?)\n",
    "# prompt3 = \"Culinary trends are constantly changing. Either we experiment with new flavors and techniques in our recipes, or we risk losing the interest of our adventurous diners.\" #maybe a dud as well\n",
    "# prompt3 = \"I thought it was a great book. Both the intricate plot twists and the strong character development make this novel exceptionally engaging.\" # both .... and\n",
    "# prompt3 = \"The team, despite facing numerous challenges and unexpected setbacks, remains optimistic about the upcoming project.\" # Noun verb agreement\n",
    "# prompt3 = \"The book on the shelf in the corner needs a new cover.\" # Noun verb agreement\n",
    "\n",
    "# title = \"which way to the beach\"\n",
    "# prompt = \"She asked 'Which way to the beach?', to which I replied,  'It's over there. You can't miss it.'. She thanked me and walked away.\"\n",
    "# POS_INTEREST = 9\n",
    "\n",
    "# title = \"lots of questions\"\n",
    "# prompt = \"The text read \\\"In the realm of deep learning, how do we best quantify the interpretability of neural networks? While considering this, it's important to remember the balance between complexity and clarity in model design. What are the most effective methods for visualizing high-dimensional data? This leads to another crucial aspect: the role of data quality. Can we establish a standard for data that optimally trains these models? Amidst these inquiries, the evolution of AI safety protocols remains a pivotal concern. How are current safety measures adapting to the rapidly advancing AI landscape? Each question marks a stepping stone towards a deeper understanding and more effective utilization of AI technologies.\"\n",
    "# POS_INTEREST = 10\n",
    "\n",
    "# title = \"both_and\"\n",
    "# prompt = \"<|endoftext|>This was my first redditgifts exchange and I couldn't be more pleased! I'd heard stories both good and bad about gifting experiences,\"\n",
    "# POS_INTEREST = 10\n",
    "\n",
    "title = \"What will you create?\"\n",
    "prompt = \"<|endoftext|> mat for it. Sewing Patterns Aprons, bags, totes, oven mitts, doll clothes and more. What will you Cre8? Weekend Bags Sewing Patterns Table Toppers & Wreaths Fold'n Stitch Patterns Use Poorhouse Fold 'n Stitch Pattern + Bosal Fusible Foam to make a beautiful fabric wreath or table topper! The patterns come with step-by-step instructions. (fabric shown here may vary from our selection).<|endoftext|>Political correctness is a pathological disorder. You can't say\"\n",
    "POS_INTEREST = 31\n",
    "\n",
    "token_df, original_cache, cache_reconstructed_query, feature_acts = eval_prompt([prompt], model, sparse_autoencoder, head_idx_override=7)\n",
    "filter_cols = [\"str_tokens\", \"unique_token\", \"context\", \"batch\", \"pos\", \"label\", \"loss\", \"loss_diff\", \"mse_loss\", \"num_active_features\", \"explained_variance\", \"kl_divergence\",\n",
    "            \"top_k_features\"]\n",
    "display(token_df[filter_cols].style.background_gradient(\n",
    "    subset=[\"loss_diff\", \"mse_loss\",\"explained_variance\", \"num_active_features\", \"kl_divergence\"],\n",
    "    cmap=\"coolwarm\"))\n",
    "\n",
    "\n",
    "\n",
    "UNIQUE_TOKEN_INTEREST = token_df[\"unique_token\"][POS_INTEREST]\n",
    "feature_acts_of_interest = feature_acts[POS_INTEREST]\n",
    "# plot_line_with_top_10_labels(feature_acts_of_interest, \"\", 25)\n",
    "# vals, inds = torch.topk(feature_acts_of_interest,39)\n",
    "\n",
    "top_k_feature_inds = (feature_acts[1:] > 0).sum(dim=0).nonzero().squeeze()\n",
    "\n",
    "features_acts_by_token_df = pd.DataFrame(\n",
    "    feature_acts[:,top_k_feature_inds[:]].detach().cpu().T,\n",
    "    index = [f\"feature_{i}\" for i in top_k_feature_inds.flatten().tolist()],\n",
    "    columns = token_df[\"unique_token\"])\n",
    "\n",
    "# features_acts_by_token_df.sort_values(by=\",/12\", ascending=False).head(10).style.background_gradient(\n",
    "#     cmap=\"coolwarm\", axis=0)\n",
    "\n",
    "# px.imshow(features_acts_by_token_df.sort_values(by=\",/12\", ascending=False).head(10).T.corr(), color_continuous_midpoint=0, color_continuous_scale=\"RdBu\")\n",
    "\n",
    "tmp = features_acts_by_token_df.sort_values(UNIQUE_TOKEN_INTEREST, ascending=False).T\n",
    "# dashboard_features = features_acts_by_token_df.sort_values(UNIQUE_TOKEN_INTEREST, ascending=False).index[:10].to_series().apply(lambda x: x.split(\"_\")[1]).tolist()\n",
    "# for feature in dashboard_features:\n",
    "#     render_feature_dashboard(feature)\n",
    "\n",
    "px.line(tmp, \n",
    "        title=f\"{title}: Features Activation by Token in Prompt\", \n",
    "        color_discrete_sequence=px.colors.qualitative.Plotly,\n",
    "        height=1000).show()\n",
    "\n",
    "tmp = features_acts_by_token_df.head(100).T\n",
    "px.imshow(tmp, \n",
    "            title=f\"{title}: Top k features by activation\", \n",
    "            color_continuous_midpoint=0, \n",
    "            color_continuous_scale=\"RdBu\", \n",
    "            height=800).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def analyse_lcf(prompt, title = \"\", model=model, sparse_autoencoder=sparse_autoencoder, head_idx_override=None):\n",
    "\n",
    "    token_df, original_cache, cache_reconstructed_query, feature_acts = eval_prompt([prompt], model, sparse_autoencoder, head_idx_override=7)\n",
    "    filter_cols = [\"str_tokens\", \"unique_token\", \"context\", \"batch\", \"pos\", \"label\", \"loss\", \"loss_diff\", \"mse_loss\", \"num_active_features\", \"explained_variance\", \"kl_divergence\",\n",
    "                \"top_k_features\"]\n",
    "    display(token_df[filter_cols].style.background_gradient(\n",
    "        subset=[\"loss_diff\", \"mse_loss\",\"explained_variance\", \"num_active_features\", \"kl_divergence\"],\n",
    "        cmap=\"coolwarm\"))\n",
    "    \n",
    "    \n",
    "    POS_INTEREST = token_df.index.max()\n",
    "    UNIQUE_TOKEN_INTEREST = token_df[\"unique_token\"][POS_INTEREST]\n",
    "    feature_acts_of_interest = feature_acts[POS_INTEREST]\n",
    "    # plot_line_with_top_10_labels(feature_acts_of_interest, \"\", 25)\n",
    "    # vals, inds = torch.topk(feature_acts_of_interest,39)\n",
    "\n",
    "    top_k_feature_inds = (feature_acts[1:] > 0).sum(dim=0).nonzero().squeeze()\n",
    "\n",
    "    features_acts_by_token_df = pd.DataFrame(\n",
    "        feature_acts[:,top_k_feature_inds[:]].detach().cpu().T,\n",
    "        index = [f\"feature_{i}\" for i in top_k_feature_inds.flatten().tolist()],\n",
    "        columns = token_df[\"unique_token\"])\n",
    "\n",
    "    # features_acts_by_token_df.sort_values(by=\",/12\", ascending=False).head(10).style.background_gradient(\n",
    "    #     cmap=\"coolwarm\", axis=0)\n",
    "\n",
    "    # px.imshow(features_acts_by_token_df.sort_values(by=\",/12\", ascending=False).head(10).T.corr(), color_continuous_midpoint=0, color_continuous_scale=\"RdBu\")\n",
    "\n",
    "    tmp = features_acts_by_token_df.sort_values(UNIQUE_TOKEN_INTEREST, ascending=False).T\n",
    "    px.line(tmp, \n",
    "            title=f\"{title}: Features Activation by Token in Prompt\", \n",
    "            color_discrete_sequence=px.colors.qualitative.Plotly).show()\n",
    "\n",
    "    tmp = features_acts_by_token_df.T\n",
    "    px.imshow(tmp, \n",
    "              title=f\"{title}: Top k features by activation\", \n",
    "              color_continuous_midpoint=0, \n",
    "              color_continuous_scale=\"RdBu\", \n",
    "              height=800).show()\n",
    "\n",
    "\n",
    "\n",
    "correlative_conjunction_prompts = {\n",
    "    \"Either - or\": {\n",
    "        \"prompt\": \"Either you are with me,\",\n",
    "        \"answer\": \" or\"\n",
    "    },\n",
    "    \"Neither - nor\": {\n",
    "        \"prompt\": \"I wasn't hired at any of the companies I'd applied to. Neither my experience great amount of experience,\",\n",
    "        \"answer\": \" nor\"\n",
    "    },\n",
    "    \"Such - that\": {\n",
    "        \"prompt\": \"Such is the intensity of the pollen outside,\",\n",
    "        \"answer\": \" that\"\n",
    "    },\n",
    "        \"Whether - or\": {\n",
    "        \"prompt\": \"Whether you bike to work and love that\",\n",
    "        \"answer\": \" or\"\n",
    "    },\n",
    "    \"Not only - but\": {\n",
    "        \"prompt\": \"Not only did my boyfriend buy me a Nintendo Switch,\",\n",
    "        \"answer\": \" but\"\n",
    "    },\n",
    "    \"Not only - but also\": {\n",
    "        \"prompt\": \"Not only did my boyfriend buy me a Nintendo Switch, but\",\n",
    "        \"answer\": \" also\"\n",
    "    },\n",
    "    \"Both - And\": {\n",
    "        \"prompt\": \"My parents went to both Hawaii\",\n",
    "        \"answer\": \" and\"\n",
    "    },\n",
    "    \"As many - as\": {\n",
    "        \"prompt\": \"There were as many applicants\",\n",
    "        \"answer\": \" as\"\n",
    "    },\n",
    "    \"No sooner - than\": {\n",
    "        \"prompt\": \"She would no sooner cheat on an exam\",\n",
    "        \"answer\": \" than\"\n",
    "    },\n",
    "    \"Rather - than\": {\n",
    "        \"prompt\": \"They would rather go to the movies\",\n",
    "        \"answer\": \" than\"\n",
    "    },\n",
    "}\n",
    "\n",
    "comparative_phrases_prompts = {\n",
    "    \"The more - the more\": {\n",
    "         \"prompt\": f\"the more you learn, the more you realize how much you don't know\",\n",
    "    },\n",
    "    \"The fewer - the fewer\": {\n",
    "        \"prompt\":f\"The fewer people who know about this, the better\",\n",
    "    },\n",
    "    \"Less on - more on\": {\n",
    "        \"prompt\":f\"I think we should focus less on talking about doing the work and more on doing the work\",\n",
    "    },\n",
    "}\n",
    " \n",
    " \n",
    "random_sentences = {\n",
    "    \"Random 1\": {\n",
    "         \"prompt\": f\"Each of these patterns shares the property of linking elements in language, creating relationships between them that are similar to those established by correlative conjunctions.\"\n",
    "    },\n",
    "    \"Random 2\": {\n",
    "        \"prompt\":f\"I feel that the insight/intuitions/skills I’ve spent trying to make progress on trajectory models, are best utilized by **studying sparse-autoencoders on language models**.\",\n",
    "    },\n",
    "    \"Random 3\": {\n",
    "        \"prompt\":f\"In most cases, professional emails are formal emails. A formal email is an email between professionals or academics that contains information related to their work.\",\n",
    "    },\n",
    "}   \n",
    "\n",
    "\n",
    "for title, prompt_dict in correlative_conjunction_prompts.items():\n",
    "    print(title)\n",
    "    analyse_lcf(prompt_dict[\"prompt\"], title=title)\n",
    "    print(\"\\n\\n\") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mech interp on a few examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"I have to say, not only is this a great book, but also the author is a great person.\"\n",
    "token_df, original_cache, cache_reconstructed_query, feature_acts = eval_prompt([prompt], model, sparse_autoencoder, head_idx_override=7)\n",
    "filter_cols = [\"str_tokens\", \"unique_token\", \"context\", \"batch\", \"pos\", \"label\", \"loss\", \"loss_diff\", \"mse_loss\", \"num_active_features\", \"explained_variance\", \"kl_divergence\",\n",
    "            \"top_k_features\"]\n",
    "# display(token_df[filter_cols].style.background_gradient(\n",
    "#     subset=[\"loss_diff\", \"mse_loss\",\"explained_variance\", \"num_active_features\", \"kl_divergence\"],\n",
    "#     cmap=\"coolwarm\"))\n",
    "\n",
    "\n",
    "# POS_INTEREST = token_df.index.max()\n",
    "# UNIQUE_TOKEN_INTEREST = token_df[\"unique_token\"][POS_INTEREST]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "POS_INTEREST = 13\n",
    "UNIQUE_TOKEN_INTEREST = token_df[\"unique_token\"][POS_INTEREST]\n",
    "feature_acts_of_interest = feature_acts[POS_INTEREST]\n",
    "# plot_line_with_top_10_labels(feature_acts_of_interest, \"\", 25)\n",
    "# vals, inds = torch.topk(feature_acts_of_interest,39)\n",
    "\n",
    "top_k_feature_inds = (feature_acts[POS_INTEREST] > 0).nonzero().squeeze()\n",
    "\n",
    "features_acts_by_token_df = pd.DataFrame(\n",
    "    feature_acts[:,top_k_feature_inds[:]].detach().cpu().T,\n",
    "    index = [f\"feature_{i}\" for i in top_k_feature_inds.flatten().tolist()],\n",
    "    columns = token_df[\"unique_token\"])\n",
    "\n",
    "# features_acts_by_token_df.sort_values(by=\",/12\", ascending=False).head(10).style.background_gradient(\n",
    "#     cmap=\"coolwarm\", axis=0)\n",
    "\n",
    "# px.imshow(features_acts_by_token_df.sort_values(by=\",/12\", ascending=False).head(10).T.corr(), color_continuous_midpoint=0, color_continuous_scale=\"RdBu\")\n",
    "\n",
    "tmp = features_acts_by_token_df.sort_values(UNIQUE_TOKEN_INTEREST, ascending=False).T\n",
    "px.line(tmp, \n",
    "        title=f\"{title}: Features Activation by Token in Prompt\", \n",
    "        color_discrete_sequence=px.colors.qualitative.Plotly).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = original_cache.apply_ln_to_stack(original_cache['blocks.11.hook_resid_post']) @ model.W_U\n",
    "print(logits.shape)\n",
    "vals, inds =torch.topk(logits[:,1:], 10, dim=-1)\n",
    "topk_predicted_token_inds = list(set(inds.flatten().tolist()))\n",
    "topk_predicted_token_strs =model.tokenizer.convert_ids_to_tokens(topk_predicted_token_inds)\n",
    "\n",
    "predicted_tokens_df = pd.DataFrame(logits[0,:,topk_predicted_token_inds].detach().cpu().T,\n",
    "                                   columns = token_df[\"unique_token\"], index = topk_predicted_token_strs)\n",
    "\n",
    "px.line(predicted_tokens_df.sort_values(UNIQUE_TOKEN_INTEREST, ascending=False).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DFA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k_feature_inds = (feature_acts[1:] > 0).sum(dim=0).nonzero().squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DLA\n",
    "\n",
    "decomp, labels = original_cache.get_full_resid_decomposition(layer =  10, expand_neurons=False, return_labels=True)\n",
    "inds = top_k_feature_inds.squeeze()\n",
    "tok1 = \" but\"\n",
    "print(decomp.shape)\n",
    "dla = (decomp[:,0,:] @ model.W_U[:,model.tokenizer.encode(tok1)]).detach().cpu().squeeze()\n",
    "print(dla.shape)\n",
    "tmp = pd.DataFrame(dla.detach().cpu().numpy().T, index = token_df[\"unique_token\"],\n",
    "                   columns = labels)\n",
    "px.line(\n",
    "    tmp.T\n",
    ").show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's do DLA\n",
    "# decomp, labels = original_cache.get_full_resid_decomposition(layer =  10, expand_neurons=False, return_labels=True)\n",
    "# print(decomp.shape)\n",
    "# inds = top_k_feature_inds.squeeze()\n",
    "# test = (decomp[:,0,POS_INTEREST] @ sparse_autoencoder.W_enc[:,inds])\n",
    "# test = (decomp[:,0,-1] @ sparse_autoencoder.W_dec[inds].T) / sparse_autoencoder.W_enc[:,inds].norm(dim=0)\n",
    "# tmp = pd.DataFrame(test.detach().cpu().numpy().T, columns = labels, index = [f\"feature_{i}\" for i in inds])\n",
    "# px.line(\n",
    "#     tmp.T[tmp.T.index.str.contains(\"mlp\")]\n",
    "# ).show()\n",
    "\n",
    "# px.line(\n",
    "#     tmp.T[tmp.T.index.str.contains(\"L\")]\n",
    "# ).show()\n",
    "\n",
    "# test = (decomp[:,0,-1] @ sparse_autoencoder.W_enc[:,inds])\n",
    "test = (decomp[:,0,POS_INTEREST] @ sparse_autoencoder.W_dec[inds].T) / sparse_autoencoder.W_enc[:,inds].norm(dim=0)\n",
    "tmp = pd.DataFrame(test.detach().cpu().numpy().T, columns = labels, index = [f\"feature_{i}\" for i in inds])\n",
    "px.line(\n",
    "    tmp.T[tmp.T.index.str.contains(\"mlp\")]\n",
    ").show()\n",
    "px.line(\n",
    "    tmp.T[tmp.T.index.str.contains(\"L\")]\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_cache[\"pattern\",0, \"attn\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = pd.DataFrame(original_cache[\"pattern\",7, \"attn\"][0,7].detach().cpu().numpy(), columns = token_df.unique_token, index = token_df.unique_token)\n",
    "px.imshow(tmp, color_continuous_midpoint=0, color_continuous_scale=\"RdBu\", height = 800).show()\n",
    "tmp = pd.DataFrame(original_cache[\"pattern\",8, \"attn\"][0,5].detach().cpu().numpy(), columns = token_df.unique_token, index = token_df.unique_token)\n",
    "px.imshow(tmp, color_continuous_midpoint=0, color_continuous_scale=\"RdBu\", height = 800).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_cache[\"pattern\",8, \"attn\"][0,5][13,7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import circuitsvis as cv \n",
    "\n",
    "tokens = token_df[\"unique_token\"].tolist()\n",
    "# print(\"Layer 0 Head Attention Patterns:\")\n",
    "cv.attention.attention_patterns(\n",
    "    tokens=token_df[\"unique_token\"].tolist(), \n",
    "    attention=original_cache[\"pattern\",7, \"attn\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eff_embed = model.W_E + model.blocks[0].mlp(model.blocks[0].ln2(model.W_E[None]))\n",
    "eff_embed = eff_embed.squeeze()\n",
    "eff_embed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eff_embed_but = eff_embed[model.to_single_token(\" but\")]\n",
    "# eff_embed_but = eff_embed[model.to_single_token(\" but\")]\n",
    "\n",
    "layer = 8\n",
    "head = 5\n",
    "W_QK = model.W_K[layer, head] @ model.W_Q[layer, head].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_cache[utils.get_act_name(\"k\", 8)][0,7,8].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals, inds = torch.topk(eff_embed @ W_QK.T @ eff_embed_but, 30)\n",
    "model.to_str_tokens(inds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metric Development\n",
    "\n",
    "- For Loop for a bunch of inference + getting SAE activations\n",
    "- Metrics:\n",
    "    - Max Contiguous fires \n",
    "    - Average Length of Contiguous Fires (given it fired, how many tokens do we expect it to keep firing on)\n",
    "    - Number of contiguous blocks in any given prompt\n",
    "    - Std on activation within a set of contigous fires\n",
    "    - Profile of firing indexed to first fire in contigous section of fires\n",
    "    - Proporition of total activation in prompt of feature / total number of times it fired\n",
    "    - Token based\n",
    "        - histogram of tokens on which it fires first\n",
    "        - histogram of tokens on which it stops firing\n",
    "    - Other SAE Properties\n",
    "        - Sparsity\n",
    "        - b_enc\n",
    "        - W_dec @ b_dec\n",
    "        - W_enc @ W_dec\n",
    "\n",
    "\n",
    "Data:\n",
    "    - Sequence Data (tokens in a prompt)\n",
    "    - Per Feature Data \n",
    "    - Token (Feature in Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's measure these for one prompt\n",
    "prompt = \"This investigation is not only one that is continuing and worldwide, but also one that we expect to continue for quite some time.\" # Not only ... but\n",
    "token_df, original_cache, cache_reconstructed_query, feature_acts_example = eval_prompt([prompt], model, sparse_autoencoder, head_idx_override=7)\n",
    "# print(token_df.columns)\n",
    "# filter_cols = [\"str_tokens\", \"unique_token\", \"context\", \"batch\", \"pos\", \"label\", \"loss\", \"loss_diff\", \"mse_loss\", \"num_active_features\", \"explained_variance\", \"kl_divergence\",\n",
    "#                \"top_k_features\"]\n",
    "# token_df[filter_cols].style.background_gradient(\n",
    "#     subset=[\"loss_diff\", \"mse_loss\",\"explained_variance\", \"num_active_features\", \"kl_divergence\"],\n",
    "#     cmap=\"coolwarm\")\n",
    "\n",
    "print(prompt)\n",
    "feature_acts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Multithreading (broken in jupyter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import multiprocessing\n",
    "# from itertools import repeat\n",
    "\n",
    "# def analyze_row(row):\n",
    "#     in_event = False\n",
    "#     event_start = 0\n",
    "#     num_events = 0\n",
    "#     max_values = []\n",
    "#     avg_values = []\n",
    "#     durations = []\n",
    "\n",
    "#     for i, value in enumerate(row):\n",
    "#         if value > 0:\n",
    "#             if not in_event:\n",
    "#                 in_event = True\n",
    "#                 event_start = i\n",
    "#                 num_events += 1\n",
    "#                 max_value = value\n",
    "#                 total_value = value\n",
    "#             else:\n",
    "#                 max_value = max(max_value, value)\n",
    "#                 total_value += value\n",
    "#         else:\n",
    "#             if in_event:\n",
    "#                 in_event = False\n",
    "#                 durations.append(i - event_start)\n",
    "#                 max_values.append(max_value)\n",
    "#                 avg_values.append(total_value / (i - event_start))\n",
    "\n",
    "#     if in_event:\n",
    "#         durations.append(len(row) - event_start)\n",
    "#         max_values.append(max_value)\n",
    "#         avg_values.append(total_value / (len(row) - event_start))\n",
    "\n",
    "#     return {\n",
    "#         'num_events': num_events,\n",
    "#         'max_values': max_values,\n",
    "#         'avg_values': avg_values,\n",
    "#         'durations': durations\n",
    "#     }\n",
    "\n",
    "# def analyze_events_parallel(tensor, num_processes=None):\n",
    "#     if num_processes is None:\n",
    "#         num_processes = multiprocessing.cpu_count()\n",
    "\n",
    "#     with multiprocessing.Pool(num_processes) as pool:\n",
    "#         results = pool.map(analyze_row, tensor)\n",
    "\n",
    "#     return results\n",
    "\n",
    "import time\n",
    "\n",
    "import events_experiment_multithreading\n",
    "reload(events_experiment_multithreading)\n",
    "\n",
    "# Example usage\n",
    "tensor = feature_acts_example[:, features_of_interest].T.cpu()\n",
    "\n",
    "start_time = time.time()\n",
    "print(tensor.shape)\n",
    "results = events_experiment_multithreading.analyze_events_parallel(tensor)\n",
    "end_time = time.time()\n",
    "print(f\"Time taken: {end_time - start_time}\")\n",
    "print(results)\n",
    "feature_prompt_df  = pd.DataFrame(results, index=features_of_interest)\n",
    "feature_prompt_df.head()\n",
    "# print(feature_prompt_df.shape)\n",
    "# feature_prompt_df[\"feature\"] = feature_prompt_df.index\n",
    "# feature_prompt_df.explode('events').sort_values(\"num_events\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(features_of_interest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Don't use Multithreading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "\n",
    "def analyze_events(tensor):\n",
    "    \n",
    "    assert len(tensor.shape) == 2, \"tensor must be 2D\"\n",
    "    results = []\n",
    "\n",
    "    for row in tensor:\n",
    "        in_event = False\n",
    "        event_start = 0\n",
    "        num_events = 0\n",
    "        max_values = []\n",
    "        avg_values = []\n",
    "        durations = []\n",
    "        start_position = np.NAN\n",
    "        final_position = np.NAN\n",
    "        \n",
    "        for i, value in enumerate(row.tolist()):\n",
    "            if value > 0:\n",
    "                if not in_event:\n",
    "                    in_event = True\n",
    "                    event_start = i\n",
    "                    num_events += 1\n",
    "                    max_value = value\n",
    "                    total_value = value\n",
    "                    start_position = i\n",
    "                else:\n",
    "                    max_value = max(max_value, value)\n",
    "                    total_value += value\n",
    "            else:\n",
    "                if in_event:\n",
    "                    in_event = False\n",
    "                    durations.append(i - event_start)\n",
    "                    max_values.append(max_value)\n",
    "                    avg_values.append(total_value / (i - event_start))\n",
    "                    final_position = i\n",
    "        \n",
    "        if in_event:\n",
    "            durations.append(len(row) - event_start)\n",
    "            max_values.append(max_value)\n",
    "            avg_values.append(total_value / (len(row) - event_start))\n",
    "\n",
    "        \n",
    "        # get the average event duration\n",
    "        avg_duration = (sum(durations) / len(durations)) if len(durations) > 0 else np.NaN\n",
    "        \n",
    "        # max duration \n",
    "        max_duration = max(durations) if len(durations) > 0 else np.NaN\n",
    "        \n",
    "        # get the average max value\n",
    "        avg_max_value = (sum(max_values) / (len(max_values)) if len(max_values) > 0 else np.NaN)\n",
    "        num_firings = sum(durations)\n",
    "        \n",
    "        # `zip` avg_valuea, max_values, durations and add it as a subrecord which we could unfurl later\n",
    "        event_stats = zip(avg_values, max_values, durations)\n",
    "        event_stats = [\n",
    "            {\n",
    "                'avg_value': avg_value,\n",
    "                'max_value': max_value,\n",
    "                'duration': duration,\n",
    "                'start_position': start_position, \n",
    "                'final_position': final_position,\n",
    "            }\n",
    "            for avg_value, max_value, duration in event_stats\n",
    "        ]\n",
    "\n",
    "        results.append({\n",
    "            'num_events': num_events,\n",
    "            'num_firings': num_firings,\n",
    "            'avg_values': avg_values,\n",
    "            'max_values': max_values,\n",
    "            'durations': durations,\n",
    "            'avg_duration': avg_duration,\n",
    "            'max_duration': max_duration,\n",
    "            'avg_max_value': avg_max_value,\n",
    "            'events': event_stats,\n",
    "        })\n",
    "\n",
    "    return results\n",
    "\n",
    "# Example usage\n",
    "\n",
    "\n",
    "tensor =feature_acts_example[:, features_of_interest].T\n",
    "\n",
    "start_time = time.time()\n",
    "results = analyze_events(tensor)\n",
    "end_time = time.time()\n",
    "print(f\"Time taken: {end_time - start_time}\")\n",
    "\n",
    "feature_prompt_df  = pd.DataFrame(results, index=features_of_interest)\n",
    "feature_prompt_df[\"feature\"] = feature_prompt_df.index\n",
    "feature_prompt_df.explode('events').sort_values(\"num_events\", ascending=False)\n",
    "display(feature_prompt_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert events to a dataframe\n",
    "tmp = feature_prompt_df.explode('events').apply(lambda x: pd.Series(x['events']), axis=1).reset_index().rename(columns={\"index\": \"feature\"})\n",
    "tmp[\"feature\"] = tmp[\"feature\"].astype(str)\n",
    "px.scatter_matrix(tmp, \n",
    "                  title=\"Event stats for each feature\", color=\"feature\", dimensions=[\"avg_value\", \"max_value\", \"duration\"],\n",
    "                  width=1000, height=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(feature_prompt_df, x=\"num_firings\", y=\"num_events\", hover_name=feature_prompt_df.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens_list = []\n",
    "pbar = tqdm(range(128*6))\n",
    "for i in pbar:\n",
    "    all_tokens_list.append(activation_store.get_batch_tokens())\n",
    "all_tokens = torch.cat(all_tokens_list, dim=0)\n",
    "print(all_tokens.shape)\n",
    "all_tokens = all_tokens[torch.randperm(all_tokens.shape[0])]\n",
    "tokens = all_tokens[:4096*6]\n",
    "del all_tokens\n",
    "torch.mps.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n_prompts = 1000\n",
    "# features_of_interest = features_of_interest\n",
    "features_of_interest = torch.randperm(sparse_autoencoder.cfg.d_sae)[:100].tolist()\n",
    "token_dfs = []\n",
    "event_dfs = []\n",
    "feature_acts_all = []\n",
    "\n",
    "for prompt_index in tqdm(range(n_prompts)):\n",
    "    prompt_tokens = tokens[prompt_index].unsqueeze(0)\n",
    "    # make token df \n",
    "    token_df = make_token_df(model, prompt_tokens, len_suffix=5, len_prefix=10)\n",
    "    token_df[\"prompt_index\"] = prompt_index\n",
    "    \n",
    "    (original_logits, original_loss), original_cache = model.run_with_cache(prompt_tokens, return_type=\"both\", loss_per_token=True)\n",
    "    token_df['loss'] = original_loss.flatten().tolist() + [np.nan]\n",
    "    \n",
    "    original_act = original_cache[sparse_autoencoder.cfg.hook_point]\n",
    "    sae_out, feature_acts, _, mse_loss, _ = sparse_autoencoder(original_act)\n",
    "\n",
    "    feature_acts_of_interest = feature_acts[0, :, features_of_interest].T\n",
    "    results = analyze_events(feature_acts_of_interest)\n",
    "    events_df  = pd.DataFrame(results, index=features_of_interest)\n",
    "    events_df[\"feature\"] = events_df.index.astype(str)\n",
    "    events_df[\"prompt_index\"] = prompt_index\n",
    "    events_df = events_df[events_df[\"num_events\"] > 0]\n",
    "    \n",
    "        \n",
    "    token_dfs.append(token_df.reset_index(drop=True))\n",
    "    event_dfs.append(events_df.reset_index(drop=True))\n",
    "    feature_acts_all.append(feature_acts_of_interest)\n",
    "    \n",
    "feature_acts_all = torch.stack(feature_acts_all, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_acts_all = torch.stack(feature_acts_all, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_df = pd.concat(token_dfs).reset_index(drop=True)\n",
    "prompt_event_df = pd.concat(event_dfs).reset_index(drop=True)\n",
    "events_df = prompt_event_df.explode('events').apply(lambda x: pd.Series(x['events']), axis=1)\n",
    "events_df[\"feature\"] = events_df.index.map(lambda x: prompt_event_df.feature[x]).astype(str)\n",
    "events_df[\"prompt_index\"] = events_df.index.map(lambda x: prompt_event_df.prompt_index[x])\n",
    "#\n",
    "# tmp[\"feature\"] = tmp.index.map(lambda x: event_df[\"feature\"][x]).astype(str)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_event_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter_matrix(prompt_event_df, \n",
    "                  title=\"Event stats for each feature\", color=\"feature\", dimensions=[\"num_events\", \"num_firings\", \"avg_duration\", \"avg_max_value\"],\n",
    "                  width=1000, height=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_event_agg_df = prompt_event_df.groupby([\"feature\", \"prompt_index\"]).agg({\"num_events\": \"sum\", \"num_firings\": \"sum\", \"avg_duration\": \"mean\"}).sort_values(\"num_events\", ascending=False).reset_index()\n",
    "prompt_event_agg_df[\"firings_per_event\"] = prompt_event_agg_df[\"num_firings\"] / prompt_event_agg_df[\"num_events\"]\n",
    "px.strip(prompt_event_agg_df, x = \"feature\", y = \"firings_per_event\", color=\"feature\", title=\"Firings per event\",\n",
    "         hover_data= [\"num_events\", \"num_firings\", \"avg_duration\", \"prompt_index\"],\n",
    "         ).show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_event_agg_df.feature.unique().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_firings_per_event = prompt_event_agg_df.groupby(\"feature\").firings_per_event.mean().sort_values(ascending=False)\n",
    "std_firings_per_event = prompt_event_agg_df.groupby(\"feature\").firings_per_event.std().sort_values(ascending=False)\n",
    "px.scatter(x=mean_firings_per_event.values, \n",
    "           y = std_firings_per_event.values,\n",
    "           hover_name=mean_firings_per_event.index,\n",
    "           marginal_x=\"histogram\",\n",
    "              marginal_y=\"histogram\",\n",
    "           labels = {\"x\": \"Mean firings per event\", \"y\": \"Std firings per event\"},\n",
    "           title=\"Mean vs Std firings per event\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in mean_firings_per_event[mean_firings_per_event<1.3].index[10:30]:\n",
    "    render_feature_dashboard(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Given some token, let's get the distribution of tokens it began firing on\n",
    "events_df[\"token_df_id\"] = events_df.apply(lambda x: token_df_id_from_prompt_and_pos(x[\"prompt_index\"], x[\"start_position\"]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_df.join(token_df, =\"token_df_id\").head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_event_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we want to get the token distribution from events. \n",
    "feature_idx = features_of_interest.index(22768)\n",
    "token_df[\"feature_22768\"] = feature_acts_all[:, feature_idx].flatten().tolist() \n",
    "# token_df[\"feature_22768_quantile\"] = pd.qcut(token_df[\"feature_22768\"], 10, labels=False, duplicates=\"drop\")\n",
    "idxes = token_df.sort_values(\"feature_22768\", ascending=False).head(30).index\n",
    "idxes_minus_1 = idxes - 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_df.groupby()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_df_id_from_prompt_and_pos = lambda prompt_index, pos: token_df[(token_df[\"prompt_index\"] == prompt_index) & (token_df[\"pos\"] == pos)].index[0]\n",
    "str_token_from_prompt_and_pos = lambda prompt_index, pos: token_df[(token_df[\"prompt_index\"] == prompt_index) & (token_df[\"pos\"] == pos)].str_tokens.values[0]\n",
    "\n",
    "token_df_id_from_prompt_and_pos(12,3)\n",
    "# str_token_from_prompt_and_pos(12,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_df.groupby(\"feature\").agg({\"duration\": \"std\"}).sort_values(\"duration\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start id word_cloud\n",
    "\n",
    "feature_of_interest = 22768\n",
    "\n",
    "# step 1. Get the start and end points for the text we care about\n",
    "events_df[events_df.duration == 4]#[events_df.feature == str(feature_of_interest)]\n",
    "# px.strip(tmp, x = \"duration\", y = \"avg_value\",title=\"Firings per event\")\\\n",
    "    \n",
    "\n",
    "# step 2. for each of these, get prompt\n",
    "token_df_ids = [token_df_id_from_prompt_and_pos(i,j) for i,j in zip(tmp.prompt_index, tmp.start_position)]\n",
    "minus_one_token_ids = [token_df_id_from_prompt_and_pos(i,j) for i,j in zip(tmp.prompt_index, tmp.start_position - 1)]\n",
    "final_token_ids = [token_df_id_from_prompt_and_pos(i,j) for i,j in zip(tmp.prompt_index, tmp.final_position.fillna(128) -1)]\n",
    "minus_one_token_fire = token_df.iloc[minus_one_token_ids].str_tokens.reset_index(drop=True)\n",
    "first_token_fire = token_df.iloc[token_df_ids].str_tokens.reset_index(drop=True)\n",
    "final_token_fire = token_df.iloc[final_token_ids].str_tokens.reset_index(drop=True)\n",
    "\n",
    "tmp = pd.concat([first_token_fire, minus_one_token_fire, final_token_fire], axis=1)\n",
    "\n",
    "tmp.columns = [\"first_token\", \"minus_one_token\", \"final_token\"]\n",
    "tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proxy Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens_list = []\n",
    "pbar = tqdm(range(128*6))\n",
    "for i in pbar:\n",
    "    all_tokens_list.append(activation_store.get_batch_tokens())\n",
    "all_tokens = torch.cat(all_tokens_list, dim=0)\n",
    "print(all_tokens.shape)\n",
    "all_tokens = all_tokens[torch.randperm(all_tokens.shape[0])]\n",
    "tokens = all_tokens[:4096*6]\n",
    "del all_tokens\n",
    "torch.mps.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_df = pd.DataFrame(\n",
    "    {\"prompt_index\" : range(tokens.shape[0]),\n",
    "        \"prompt\": [model.to_string(tokens[i]) for i in range(tokens.shape[0])]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "correlative_conjunctions = {\n",
    "    \"both_and\": r\"\\bboth\\b(?:(?!\\.|\\?|!).)*?\\band\\b\",\n",
    "    \"either_or\": r\"\\beither\\b(?:(?!\\.|\\?|!).)*?\\bor\\b\",\n",
    "    \"neither_nor\": r\"\\bneither\\b(?:(?!\\.|\\?|!).)*?\\bnor\\b\",\n",
    "    \"not_only_but_also\": r\"\\bnot\\s+only\\b(?:(?!\\.|\\?|!).)*?\\bbut\\s+also\\b\",\n",
    "    \"whether_or\": r\"\\bwhether\\b(?:(?!\\.|\\?|!).)*?\\bor\\b\",\n",
    "}\n",
    "\n",
    "\n",
    "questions = {\n",
    "    \"general_questions\": r\"\\b(who|what|when|where|why|how)\\b.*?\\?\",\n",
    "    \"how\": r\"\\bhow\\b.*?\\?\",\n",
    "    \"what\": r\"\\bwhat\\b.*?\\?\",\n",
    "    \"when\": r\"\\bwhen\\b.*?\\?\",\n",
    "    \"where\": r\"\\bwhere\\b.*?\\?\",\n",
    "    \"why\": r\"\\bwhy\\b.*?\\?\",\n",
    "    \"who\": r\"\\bwho\\b.*?\\?\",\n",
    "    \"choice_questions\": r\"\\b(do you prefer|would you rather)\\b.*?\\?\",\n",
    "}\n",
    "\n",
    "punctuation = {\n",
    "    \"regular_parentheses\": r\"\\(.*?\\)\",\n",
    "    \"square_brackets\": r\"\\[.*?\\]\",\n",
    "    \"curly_brackets\": r\"\\{.*?\\}\",\n",
    "    \"angle_brackets\": r\"\\<.*?\\>\",\n",
    "    \"double_quotes\": r\"\\\".*?\\\"\",\n",
    "    \"single_quotes\": r\"\\'.*?\\'\",\n",
    "    \"backticks\": r\"`.*?`\",\n",
    "}\n",
    "\n",
    "# lists = {\n",
    "#     \"bulleted_lists\": r\"^\\s*[\\-\\*\\+] .*$\",\n",
    "#     \"numbered_lists\": r\"^\\s*\\d+\\..*$\",\n",
    "#     \"alphabetic_lists\": r\"^\\s*[a-zA-Z]\\..*$\",\n",
    "# }\n",
    "\n",
    "formatting = {\n",
    "    \"specific_html_tag\": r\"\\<div\\>.*?\\</div\\>\",  # Example with 'div' tag\n",
    "    \"any_html_tag\": r\"\\<.*?\\>.*?\\</.*?\\>\",\n",
    "    \"inline_code\": r\"`.*?`\",\n",
    "    \"multiline_code_blocks\": r\"```.*?```\",\n",
    "    \"bold_text_markdown\": r\"\\*\\*.*?\\*\\*\" + \"|\" + r\"__.*?__\",\n",
    "    \"italic_text_markdown\": r\"\\*.*?\\*\" + \"|\" + r\"_.*?_\"\n",
    "}\n",
    "\n",
    "\n",
    "# now all all proxies together in one dict\n",
    "proxies = {}\n",
    "proxies.update(correlative_conjunctions)\n",
    "proxies.update(questions)\n",
    "# proxies.update(punctuation)\n",
    "# proxies.update(lists)\n",
    "# proxies.update(formatting)\n",
    "\n",
    "# create a column for each conjunction in the prompt_df\n",
    "\n",
    "for conjunction, regex in proxies.items():\n",
    "    prompt_df[conjunction] = prompt_df.prompt.str.contains(regex, flags=re.IGNORECASE)\n",
    "    \n",
    "# summarize\n",
    "prompt_df.iloc[:,2:].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proxy_hits = prompt_df.iloc[:,2:7].sum()\n",
    "\n",
    "px.bar(proxy_hits, \n",
    "       # add the number above each bar\n",
    "      text=proxy_hits.values,\n",
    "       title=\"Number of prompts with each proxy (out of 24576 prompts)\", \n",
    "       labels={\"value\": \"Number of prompts\"}).show()\n",
    "\n",
    "\n",
    "proxy_hits = prompt_df.iloc[:,7:13].sum()\n",
    "\n",
    "px.bar(proxy_hits, \n",
    "       # add the number above each bar\n",
    "      text=proxy_hits.values,\n",
    "       title=\"Number of prompts with each proxy (out of 24576 prompts)\", \n",
    "       labels={\"value\": \"Number of prompts\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample from prompts containing proxies\n",
    "import re \n",
    "# import HTML\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "\n",
    "def both_and_highlight(prompt, prompt_proxy_regex):\n",
    "    start_pos = re.search(prompt_proxy_regex, prompt, flags=re.IGNORECASE).start()\n",
    "    end_pos = re.search(prompt_proxy_regex, prompt, flags=re.IGNORECASE).end()\n",
    "    # style with red text\n",
    "    style_tag = \"<span style='color:red'>\"\n",
    "    prompt = prompt[:start_pos] + f'{style_tag}'+ prompt[start_pos:end_pos] + \"</span>\" + prompt[end_pos:]\n",
    "    display(HTML(prompt))\n",
    "\n",
    "for i, row in prompt_df[prompt_df[\"both_and\"]].sample(1).iterrows():\n",
    "    both_and_highlight(row.prompt,proxies[\"both_and\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_acts(prompts, features_of_interest: List):\n",
    "    \n",
    "    n_prompts = len(prompts)\n",
    "    feature_acts_list = []\n",
    "    \n",
    "    for prompt_index in tqdm(range(n_prompts)):\n",
    "        prompt_tokens = prompts[prompt_index].unsqueeze(0)\n",
    "        # make token df \n",
    "        token_df = make_token_df(model, prompt_tokens, len_suffix=5, len_prefix=10)\n",
    "        token_df[\"prompt_index\"] = prompt_index\n",
    "        \n",
    "        (original_logits, original_loss), original_cache = model.run_with_cache(prompt_tokens, return_type=\"both\", loss_per_token=True)\n",
    "        token_df['loss'] = original_loss.flatten().tolist() + [np.nan]\n",
    "        \n",
    "        original_act = original_cache[sparse_autoencoder.cfg.hook_point]\n",
    "        sae_out, feature_acts, _, mse_loss, _ = sparse_autoencoder(original_act)\n",
    "\n",
    "        feature_acts_list.append(feature_acts[:,:,features_of_interest])\n",
    "        \n",
    "    feature_acts = torch.stack(feature_acts_list, dim=0)\n",
    "    \n",
    "    return feature_acts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A huge amount of annoying data crunching so we have a df with the indexes we care about. (work out where the correlative conjunction appeared and then get the token positions we care about.) Then we are ready to go to feature acts and get the feature acts for all of these positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start token ids are any ids that match \"both\"\n",
    "both_token_strs = [\" both\", \"Both\", \" both\", \"Both\"]\n",
    "both_token_ids = [model.to_single_token(token) for token in both_token_strs]\n",
    "and_token_strs = [\" and\", \"and\", \"And\", \" And\"]\n",
    "and_token_ids = [model.to_single_token(token) for token in and_token_strs]\n",
    "\n",
    "\n",
    "both_and_df = prompt_df[prompt_df[\"both_and\"]][[\"prompt_index\", \"prompt\"]]\n",
    "both_and_df[\"tokens\"] = tokens[prompt_df[\"both_and\"]].detach().cpu().numpy().tolist()\n",
    "both_and_df[\"start_pos\"] = both_and_df.prompt.apply(lambda x: re.search(proxies[\"both_and\"], x, flags=re.IGNORECASE).start())\n",
    "both_and_df[\"end_pos\"] = both_and_df.prompt.apply(lambda x: re.search(proxies[\"both_and\"], x, flags=re.IGNORECASE).end())\n",
    "both_and_df[\"offset_mapping\"] = both_and_df.apply(lambda x: model.tokenizer.encode_plus(x.prompt, return_offsets_mapping=True)[\"offset_mapping\"], axis=1)\n",
    "both_and_df[\"start_offset_mapping\"] = both_and_df.apply(lambda x: [i for i,_ in x[\"offset_mapping\"]], axis=1)\n",
    "both_and_df[\"end_offset_mapping\"] = both_and_df.apply(lambda x: [j for _,j in x[\"offset_mapping\"]], axis=1)\n",
    "both_and_df[\"start_pos_tok_id\"] = both_and_df.apply(lambda x: next(i for i, offset in enumerate(x[\"start_offset_mapping\"]) if offset >= x[\"start_pos\"]), axis=1)\n",
    "both_and_df[\"end_pos_tok_id\"] = both_and_df.apply(lambda x: next((i for i, offset in enumerate(x[\"start_offset_mapping\"]) if offset > x[\"end_pos\"]-1), 127), axis=1)\n",
    "both_and_df[\"start_pos_tok_str\"] = both_and_df.apply(lambda x: model.to_single_str_token(x[\"tokens\"][x[\"start_pos_tok_id\"]]), axis = 1) \n",
    "both_and_df[\"end_pos_tok_str\"] = both_and_df.apply(lambda x: model.to_single_str_token(x[\"tokens\"][x[\"end_pos_tok_id\"]]), axis = 1)\n",
    "both_and_df[[ \"prompt_index\", \"prompt\", \"start_pos_tok_str\", \"end_pos_tok_str\"]]\n",
    "\n",
    "\n",
    "features_of_interest = [21604]\n",
    "both_and_feature_acts = get_feature_acts(tokens[prompt_df[\"both_and\"]], features_of_interest=features_of_interest)\n",
    "both_and_feature_acts = both_and_feature_acts.squeeze()\n",
    "both_and_feature_acts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "both_and_df[\"n_toks_in_proxy_context\"] = both_and_df.apply(lambda x: x[\"end_pos_tok_id\"] - x[\"start_pos_tok_id\"], axis=1)\n",
    "\n",
    "tmp = both_and_df.n_toks_in_proxy_context.value_counts()\n",
    "# bar chart orderer by index, with text labels for the count\n",
    "px.bar(tmp.sort_index(), text=tmp.values, title=\"Number of tokens in proxy context\",\n",
    "       # not legend\n",
    "        labels={\"n_toks_in_proxy_context\": \"Number of tokens in proxy context\", \"value\": \"Number of prompts\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "def display_prompt_selector(df, indexes, prompt_proxy_regex=proxies[\"both_and\"]):\n",
    "    # Output widget to display the prompt\n",
    "    output = widgets.Output()\n",
    "\n",
    "    # Function to display the prompt example\n",
    "    def on_dropdown_change(change):\n",
    "        with output:\n",
    "            clear_output(wait=True)\n",
    "            index = change['new']\n",
    "            if index in df.index:\n",
    "                # Your logic to display the prompt example\n",
    "                both_and_highlight(df.loc[index].prompt, prompt_proxy_regex=prompt_proxy_regex)\n",
    "            else:\n",
    "                print(\"Invalid selection\")\n",
    "\n",
    "    # Create a dropdown widget for prompt examples\n",
    "    prompt_selector = widgets.Dropdown(\n",
    "        options=['Select a prompt'] + list(indexes),\n",
    "        description='Select Prompt:',\n",
    "        disabled=False,\n",
    "    )\n",
    "\n",
    "    # Observe changes in the dropdown\n",
    "    prompt_selector.observe(on_dropdown_change, names='value')\n",
    "\n",
    "    # Display the widgets and output\n",
    "    display(prompt_selector, output)\n",
    "\n",
    "# Example usage\n",
    "# display_prompt_selector(both_and_df, both_and_df.index)\n",
    "\n",
    "\n",
    "# using the start and end pos token ids from both_and_df to get the feature acts, padding with 0s\n",
    "start_pos_tok_ids = both_and_df[\"start_pos_tok_id\"].tolist()\n",
    "end_pos_tok_ids = both_and_df[\"end_pos_tok_id\"].tolist()\n",
    "aligned_feature_actions = torch.zeros_like(both_and_feature_acts)\n",
    "for i , (start, end) in enumerate(zip(start_pos_tok_ids, end_pos_tok_ids)):\n",
    "    aligned_feature_actions[i, :(end-start+1)] = both_and_feature_acts[i, (start-1):(end)]\n",
    "\n",
    "for gap_length in range(6,10):\n",
    "    length_mask = (both_and_df.n_toks_in_proxy_context == gap_length).values\n",
    "    tmp = pd.DataFrame(aligned_feature_actions[length_mask,:gap_length+2].detach().cpu().numpy().T,\n",
    "                       columns = both_and_df[length_mask].prompt_index.astype(int).to_list())\n",
    "    \n",
    "    feature_present_indexes = tmp.columns[((tmp.values > 0).sum(axis=0) > 0).nonzero()]\n",
    "    feature_missing_indexes = tmp.columns[((tmp.values > 0).sum(axis=0) == 0).nonzero()]\n",
    "    \n",
    "    display(HTML(\"<h2>Feature present</h2>\"))\n",
    "    print(feature_present_indexes)\n",
    "    display_prompt_selector(both_and_df[length_mask], feature_present_indexes)\n",
    "    display(HTML(\"<h2>Feature Absent</h2>\"))\n",
    "    print(feature_missing_indexes)\n",
    "    display_prompt_selector(both_and_df[length_mask], feature_missing_indexes)\n",
    "    px.line(tmp, title=f\"Feature activations for prompts containing 'both ... and' with a gap of {gap_length}\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_1_1 = \"\"\"himself into a shadow war in order to expose it. His only clue is the keyword \"El Dorado.\" He meets Sophie, a woman searching for her older brother who left her with only a message with the same word: \"El Dorado.\" With Sword having also lost his younger sister in the past, both are drawn together by the word,\"\"\"\n",
    "prompt_2_1 = \"\"\"tern and a banana. \"The fan section was louder than it had been all season long, and the fans, of both sides I may add, were thoroughly amused\"\"\"\n",
    "answer = \" and\"\n",
    "utils.test_prompt(prompt_1_1, answer, model)\n",
    "utils.test_prompt(prompt_2_1, answer, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What ... ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start token ids are any ids that match \"both\"\n",
    "what_token_strs = [\" what\", \"What\"]\n",
    "what_token_ids = [model.to_single_token(token) for token in what_token_strs]\n",
    "qmark_token_strs = [\"?\", \" ?\", \"?!\"]\n",
    "qmark_token_ids = [model.to_single_token(token) for token in and_token_strs]\n",
    "\n",
    "\n",
    "what_question_df = prompt_df[prompt_df[\"what\"]][[\"prompt_index\", \"prompt\"]]\n",
    "what_question_df[\"tokens\"] = tokens[prompt_df[\"what\"]].detach().cpu().numpy().tolist()\n",
    "what_question_df[\"start_pos\"] = what_question_df.prompt.apply(lambda x: re.search(proxies[\"what\"], x, flags=re.IGNORECASE).start())\n",
    "what_question_df[\"end_pos\"] = what_question_df.prompt.apply(lambda x: re.search(proxies[\"what\"], x, flags=re.IGNORECASE).end())\n",
    "what_question_df[\"offset_mapping\"] = what_question_df.apply(lambda x: model.tokenizer.encode_plus(x.prompt, return_offsets_mapping=True)[\"offset_mapping\"], axis=1)\n",
    "what_question_df[\"start_offset_mapping\"] = what_question_df.apply(lambda x: [i for i,_ in x[\"offset_mapping\"]], axis=1)\n",
    "what_question_df[\"end_offset_mapping\"] = what_question_df.apply(lambda x: [j for _,j in x[\"offset_mapping\"]], axis=1)\n",
    "what_question_df[\"start_pos_tok_id\"] = what_question_df.apply(lambda x: next(i for i, offset in enumerate(x[\"start_offset_mapping\"]) if offset >= x[\"start_pos\"]), axis=1)\n",
    "what_question_df[\"end_pos_tok_id\"] = what_question_df.apply(lambda x: next((i for i, offset in enumerate(x[\"start_offset_mapping\"]) if offset > x[\"end_pos\"]-1), 127), axis=1)\n",
    "what_question_df[\"start_pos_tok_str\"] = what_question_df.apply(lambda x: model.to_single_str_token(x[\"tokens\"][x[\"start_pos_tok_id\"]]), axis = 1) \n",
    "what_question_df[\"end_pos_tok_str\"] = what_question_df.apply(lambda x: model.to_single_str_token(x[\"tokens\"][x[\"end_pos_tok_id\"]]), axis = 1)\n",
    "what_question_df[[ \"prompt_index\", \"prompt\", \"start_pos_tok_str\", \"end_pos_tok_str\"]]\n",
    "\n",
    "\n",
    "features_of_interest = [18962]\n",
    "what_question_acts = get_feature_acts(tokens[prompt_df[\"what\"]], features_of_interest=features_of_interest)\n",
    "what_question_acts = what_question_acts.squeeze()\n",
    "\n",
    "\n",
    "what_question_df[\"n_toks_in_proxy_context\"] = what_question_df.apply(lambda x: x[\"end_pos_tok_id\"] - x[\"start_pos_tok_id\"], axis=1)\n",
    "\n",
    "tmp = what_question_df.n_toks_in_proxy_context.value_counts()\n",
    "# bar chart orderer by index, with text labels for the count\n",
    "px.bar(tmp.sort_index(), text=tmp.values, title=\"Number of tokens in proxy context\",\n",
    "       # not legend\n",
    "        labels={\"n_toks_in_proxy_context\": \"Number of tokens in proxy context\", \"value\": \"Number of prompts\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the start and end pos token ids from what_question_df to get the feature acts, padding with 0s\n",
    "start_pos_tok_ids = what_question_df[\"start_pos_tok_id\"].tolist()\n",
    "end_pos_tok_ids = what_question_df[\"end_pos_tok_id\"].tolist()\n",
    "aligned_feature_actions = torch.zeros_like(what_question_acts)\n",
    "for i , (start, end) in enumerate(zip(start_pos_tok_ids, end_pos_tok_ids)):\n",
    "    aligned_feature_actions[i, :(end-start+1)] = what_question_acts[i, (start-1):(end)]\n",
    "\n",
    "for gap_length in range(5,9):\n",
    "    length_mask = (what_question_df.n_toks_in_proxy_context == gap_length).values\n",
    "    tmp = pd.DataFrame(aligned_feature_actions[length_mask,:gap_length+2].detach().cpu().numpy().T,\n",
    "                       columns = what_question_df[length_mask].prompt_index.astype(int).to_list())\n",
    "    \n",
    "    feature_present_indexes = tmp.columns[((tmp.values > 0).sum(axis=0) > 0).nonzero()]\n",
    "    feature_missing_indexes = tmp.columns[((tmp.values > 0).sum(axis=0) == 0).nonzero()]\n",
    "    \n",
    "    display(HTML(\"<h2>Feature present</h2>\"))\n",
    "    print(feature_present_indexes)\n",
    "    display_prompt_selector(what_question_df[length_mask], feature_present_indexes, prompt_proxy_regex=proxies[\"what\"])\n",
    "    display(HTML(\"<h2>Feature Absent</h2>\"))\n",
    "    print(feature_missing_indexes)\n",
    "    display_prompt_selector(what_question_df[length_mask], feature_missing_indexes, prompt_proxy_regex=proxies[\"what\"])\n",
    "    px.line(tmp, title=f\"Feature activations for prompts containing 'both ... and' with a gap of {gap_length}\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Dashboards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate new features if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens_list = []\n",
    "pbar = tqdm(range(128*6))\n",
    "for i in pbar:\n",
    "    all_tokens_list.append(activation_store.get_batch_tokens())\n",
    "all_tokens = torch.cat(all_tokens_list, dim=0)\n",
    "print(all_tokens.shape)\n",
    "all_tokens = all_tokens[torch.randperm(all_tokens.shape[0])]\n",
    "tokens = all_tokens[:4096*6]\n",
    "del all_tokens\n",
    "torch.mps.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../week_8_jan/gpt2_small_features\"\n",
    "# path = \"../week_8_jan/gpt2_small_features_layer_5\"\n",
    "\n",
    "import os \n",
    "for i in range(sparse_autoencoder.cfg.d_sae):\n",
    "    # print(f\"Checking {i}\")\n",
    "    if not os.path.exists(f\"{path}/data_{i:04}.html\"):\n",
    "        break \n",
    "    \n",
    "n_features_at_a_time = 512\n",
    "id_of_last_feature_without_dashboard = i\n",
    "id_to_start_from = id_of_last_feature_without_dashboard - id_of_last_feature_without_dashboard % n_features_at_a_time\n",
    "print(f\"File {i} does not exist\")\n",
    "print(f\"id_to_start_from: {id_to_start_from}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sae_analysis.visualizer import data_fns, model_fns, html_fns\n",
    "import importlib\n",
    "\n",
    "importlib.reload(data_fns)\n",
    "importlib.reload(html_fns)\n",
    "from sae_analysis.visualizer.data_fns import get_feature_data, FeatureData\n",
    "\n",
    "# Currently, don't think much more time can be squeezed out of it. Maybe the best saving would be to\n",
    "# make the entire sequence indexing parallelized, but that's possibly not worth it right now.\n",
    "\n",
    "max_batch_size = 512\n",
    "total_batch_size = 4096*6\n",
    "feature_idx = [i for i in range(id_to_start_from, sparse_autoencoder.cfg.d_sae)]\n",
    "feature_idx = torch.tensor(feature_idx).reshape(n_features_at_a_time, -1).T\n",
    "n_features_batches = feature_idx.shape[0]\n",
    "print(feature_idx.shape)\n",
    "feature_idx = [feature_idx.T[i].tolist() for i in range(n_features_batches)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for interesting_features in tqdm(feature_idx):\n",
    "    print(interesting_features)\n",
    "    feature_data = get_feature_data(\n",
    "        encoder=sparse_autoencoder,\n",
    "        # encoder_B=sparse_autoencoder,\n",
    "        model=model,\n",
    "        hook_point=sparse_autoencoder.cfg.hook_point,\n",
    "        hook_point_layer=sparse_autoencoder.cfg.hook_point_layer,\n",
    "        hook_point_head_index=None,\n",
    "        tokens=tokens,\n",
    "        feature_idx=interesting_features,\n",
    "        max_batch_size=max_batch_size,\n",
    "        left_hand_k = 3,\n",
    "        buffer = (5, 5),\n",
    "        n_groups = 10,\n",
    "        first_group_size = 20,\n",
    "        other_groups_size = 5,\n",
    "        verbose = True,\n",
    "    )\n",
    "    \n",
    "    for test_idx in feature_data.keys():\n",
    "        html_str = feature_data[test_idx].get_all_html()\n",
    "        with open(f\"{path}/data_{test_idx:04}.html\", \"w\") as f:\n",
    "            f.write(html_str)\n",
    "# feature_data = get_feature_data(\n",
    "#     encoder=sparse_autoencoder,\n",
    "#     # encoder_B=sparse_autoencoder,\n",
    "#     model=model,\n",
    "#     hook_point=sparse_autoencoder.cfg.hook_point,\n",
    "#     hook_point_layer=sparse_autoencoder.cfg.hook_point_layer,\n",
    "#     hook_point_head_index=None,\n",
    "#     tokens=tokens,\n",
    "#     feature_idx=interesting_features,\n",
    "#     max_batch_size=max_batch_size,\n",
    "#     left_hand_k = 3,\n",
    "#     buffer = (5, 5),\n",
    "#     n_groups = 10,\n",
    "#     first_group_size = 20,\n",
    "#     other_groups_size = 5,\n",
    "#     verbose = True,\n",
    "# )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mats_sae_training",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
