{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append(\"../..\")\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from importlib import reload\n",
    "from tqdm import tqdm\n",
    "\n",
    "import joseph\n",
    "from joseph.analysis import *\n",
    "from joseph.visualisation import *\n",
    "from joseph.utils import *\n",
    "from joseph.data import *\n",
    "\n",
    "\n",
    "reload(joseph.analysis)\n",
    "reload(joseph.visualisation)\n",
    "reload(joseph.utils)\n",
    "reload(joseph.data)\n",
    "\n",
    "from joseph.analysis import *\n",
    "from joseph.visualisation import *\n",
    "from joseph.utils import *\n",
    "from joseph.data import *\n",
    "\n",
    "# turn torch grad tracking off\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = HookedTransformer.from_pretrained(\n",
    "    \"gpt2-small\",\n",
    "    # \"pythia-2.8b\",\n",
    "    # \"pythia-70m-deduped\",\n",
    "    # \"tiny-stories-2L-33M\",\n",
    "    # \"attn-only-2l\",\n",
    "    # center_unembed=True,\n",
    "    # center_writing_weights=True,\n",
    "    # fold_ln=True,\n",
    "    # refactor_factored_attn_matrices=True,\n",
    "    fold_ln=True,\n",
    ")\n",
    "model.set_use_split_qkv_input(True)\n",
    "model.set_use_attn_result(True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load SAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "path = \"../week_8_jan/artifacts/sparse_autoencoder_gpt2-small_blocks.10.hook_resid_pre_49152:v28/1100001280_sparse_autoencoder_gpt2-small_blocks.10.hook_resid_pre_49152.pt\"\n",
    "# path = \"./artifacts/sparse_autoencoder_gpt2-small_blocks.5.hook_resid_pre_49152:v9/final_sparse_autoencoder_gpt2-small_blocks.5.hook_resid_pre_49152.pt\"\n",
    "sparse_autoencoder = SparseAutoencoder.load_from_pretrained(path)\n",
    "\n",
    "print(sparse_autoencoder.cfg)\n",
    "\n",
    "\n",
    "# sanity check\n",
    "text = \"Many important transition points in the history of science have been moments when science 'zoomed in.' At these points, we develop a visualization or tool that allows us to see the world in a new level of detail, and a new field of science develops to study the world through this lens.\"\n",
    "model(text, return_type=\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sae_training.utils import LMSparseAutoencoderSessionloader\n",
    "model, sparse_autoencoder, activation_store = LMSparseAutoencoderSessionloader.load_session_from_pretrained(\n",
    "    path\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Dashboard generator util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import webbrowser\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "path_to_html = \"../week_8_jan/gpt2_small_features\"\n",
    "def render_feature_dashboard(feature_id):\n",
    "    \n",
    "    path = f\"{path_to_html}/data_{feature_id:04}.html\"\n",
    "    \n",
    "    print(f\"Feature {feature_id}\")\n",
    "    if os.path.exists(path):\n",
    "        # with open(path, \"r\") as f:\n",
    "        #     html = f.read()\n",
    "        #     display(HTML(html))\n",
    "        webbrowser.open_new_tab(\"file://\" + os.path.abspath(path))\n",
    "    else:\n",
    "        print(\"No HTML file found\")\n",
    "    \n",
    "    return\n",
    "\n",
    "# for feature in [100,300,400]:\n",
    "#     render_feature_dashboard(feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fun Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt1 = \"The war caused not only destruction and death but also generations of hatred between the two communities.\"\n",
    "prompt2 = \"The car not only is economical but also feels good to drive.\"\n",
    "prompt3 = \"This investigation is not only one that is continuing and worldwide,\"  # but also one that we expect to continue for quite some time.\"\n",
    "prompt = prompt3\n",
    "answer = \"but\"\n",
    "model.reset_hooks()\n",
    "utils.test_prompt(prompt, answer, model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joseph\n",
    "reload(joseph.analysis)\n",
    "from joseph.analysis import *\n",
    "\n",
    "prompt3 = \"This investigation is not only one that is continuing and worldwide, but also one that we expect to continue for quite some time.\"\n",
    "token_df, original_cache, cache_reconstructed_query, feature_acts = eval_prompt([prompt3], model, sparse_autoencoder, head_idx_override=7)\n",
    "print(token_df.columns)\n",
    "filter_cols = [\"str_tokens\", \"unique_token\", \"context\", \"batch\", \"pos\", \"label\", \"loss\", \"loss_diff\", \"mse_loss\", \"num_active_features\", \"explained_variance\", \"kl_divergence\",\n",
    "               \"top_k_features\"]\n",
    "token_df[filter_cols].style.background_gradient(\n",
    "    subset=[\"loss_diff\", \"mse_loss\",\"explained_variance\", \"num_active_features\", \"kl_divergence\"],\n",
    "    cmap=\"coolwarm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "POS_INTEREST = 12# index from 0.\n",
    "print(token_df.shape)\n",
    "print(feature_acts.shape)\n",
    "print(token_df[\"unique_token\"][POS_INTEREST]) \n",
    "feature_acts_of_interest = feature_acts[POS_INTEREST]\n",
    "plot_line_with_top_10_labels(feature_acts_of_interest, \"\", 25)\n",
    "vals, inds = torch.topk(feature_acts_of_interest,39)\n",
    "print(vals.nonzero().shape)\n",
    "print(inds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize activations over Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k_feature_inds = inds\n",
    "print(feature_acts.shape)\n",
    "features_acts_by_token_df = pd.DataFrame(\n",
    "    feature_acts[:,top_k_feature_inds[:]].detach().cpu().T,\n",
    "    index = [f\"feature_{i}\" for i in top_k_feature_inds.flatten().tolist()],\n",
    "    columns = token_df[\"unique_token\"])\n",
    "\n",
    "features_acts_by_token_df.sort_values(by=\",/12\", ascending=False).style.background_gradient(\n",
    "    cmap=\"coolwarm\", axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.line(features_acts_by_token_df.sort_values(\",/12\", ascending=False).T, title=\"Top k features by activation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metric Development\n",
    "\n",
    "- For Loop for a bunch of inference + getting SAE activations\n",
    "- Metrics:\n",
    "    - Max Contiguous fires \n",
    "    - Average Length of Contiguous Fires (given it fired, how many tokens do we expect it to keep firing on)\n",
    "    - Number of contiguous blocks in any given prompt\n",
    "    - Std on activation within a set of contigous fires\n",
    "    - Profile of firing indexed to first fire in contigous section of fires\n",
    "    - Proporition of total activation in prompt of feature / total number of times it fired\n",
    "    - Token based\n",
    "        - histogram of tokens on which it fires first\n",
    "        - histogram of tokens on which it stops firing\n",
    "    - Other SAE Properties\n",
    "        - Sparsity\n",
    "        - b_enc\n",
    "        - W_dec @ b_dec\n",
    "        - W_enc @ W_dec\n",
    "\n",
    "\n",
    "Data:\n",
    "    - Sequence Data (tokens in a prompt)\n",
    "    - Per Feature Data \n",
    "    - Token (Feature in Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's measure these for one prompt\n",
    "\n",
    "token_df, original_cache, cache_reconstructed_query, feature_acts_example = eval_prompt([prompt3], model, sparse_autoencoder, head_idx_override=7)\n",
    "# print(token_df.columns)\n",
    "# filter_cols = [\"str_tokens\", \"unique_token\", \"context\", \"batch\", \"pos\", \"label\", \"loss\", \"loss_diff\", \"mse_loss\", \"num_active_features\", \"explained_variance\", \"kl_divergence\",\n",
    "#                \"top_k_features\"]\n",
    "# token_df[filter_cols].style.background_gradient(\n",
    "#     subset=[\"loss_diff\", \"mse_loss\",\"explained_variance\", \"num_active_features\", \"kl_divergence\"],\n",
    "#     cmap=\"coolwarm\")\n",
    "\n",
    "print(prompt)\n",
    "feature_acts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_of_interest = (feature_acts_example[12] > 0).nonzero().flatten().tolist()\n",
    "print(features_of_interest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_acts_binary = (feature_acts_example > 0).float()[:,features_of_interest]\n",
    "# number times active\n",
    "num_fires_on_prompt = feature_acts_binary.sum(dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Multithreading (broken in jupyter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import multiprocessing\n",
    "# from itertools import repeat\n",
    "\n",
    "# def analyze_row(row):\n",
    "#     in_event = False\n",
    "#     event_start = 0\n",
    "#     num_events = 0\n",
    "#     max_values = []\n",
    "#     avg_values = []\n",
    "#     durations = []\n",
    "\n",
    "#     for i, value in enumerate(row):\n",
    "#         if value > 0:\n",
    "#             if not in_event:\n",
    "#                 in_event = True\n",
    "#                 event_start = i\n",
    "#                 num_events += 1\n",
    "#                 max_value = value\n",
    "#                 total_value = value\n",
    "#             else:\n",
    "#                 max_value = max(max_value, value)\n",
    "#                 total_value += value\n",
    "#         else:\n",
    "#             if in_event:\n",
    "#                 in_event = False\n",
    "#                 durations.append(i - event_start)\n",
    "#                 max_values.append(max_value)\n",
    "#                 avg_values.append(total_value / (i - event_start))\n",
    "\n",
    "#     if in_event:\n",
    "#         durations.append(len(row) - event_start)\n",
    "#         max_values.append(max_value)\n",
    "#         avg_values.append(total_value / (len(row) - event_start))\n",
    "\n",
    "#     return {\n",
    "#         'num_events': num_events,\n",
    "#         'max_values': max_values,\n",
    "#         'avg_values': avg_values,\n",
    "#         'durations': durations\n",
    "#     }\n",
    "\n",
    "# def analyze_events_parallel(tensor, num_processes=None):\n",
    "#     if num_processes is None:\n",
    "#         num_processes = multiprocessing.cpu_count()\n",
    "\n",
    "#     with multiprocessing.Pool(num_processes) as pool:\n",
    "#         results = pool.map(analyze_row, tensor)\n",
    "\n",
    "#     return results\n",
    "\n",
    "import time\n",
    "\n",
    "import events_experiment_multithreading\n",
    "reload(events_experiment_multithreading)\n",
    "\n",
    "# Example usage\n",
    "tensor = feature_acts_example[:, features_of_interest].T.cpu()\n",
    "\n",
    "start_time = time.time()\n",
    "print(tensor.shape)\n",
    "results = events_experiment_multithreading.analyze_events_parallel(tensor)\n",
    "end_time = time.time()\n",
    "print(f\"Time taken: {end_time - start_time}\")\n",
    "print(results)\n",
    "feature_prompt_df  = pd.DataFrame(results, index=features_of_interest)\n",
    "feature_prompt_df.head()\n",
    "# print(feature_prompt_df.shape)\n",
    "# feature_prompt_df[\"feature\"] = feature_prompt_df.index\n",
    "# feature_prompt_df.explode('events').sort_values(\"num_events\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(features_of_interest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Don't use Multithreading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "\n",
    "def analyze_events(tensor):\n",
    "    \n",
    "    assert len(tensor.shape) == 2, \"tensor must be 2D\"\n",
    "    results = []\n",
    "\n",
    "    for row in tensor:\n",
    "        in_event = False\n",
    "        event_start = 0\n",
    "        num_events = 0\n",
    "        max_values = []\n",
    "        avg_values = []\n",
    "        durations = []\n",
    "        start_position = np.NAN\n",
    "        final_position = np.NAN\n",
    "        \n",
    "        for i, value in enumerate(row.tolist()):\n",
    "            if value > 0:\n",
    "                if not in_event:\n",
    "                    in_event = True\n",
    "                    event_start = i\n",
    "                    num_events += 1\n",
    "                    max_value = value\n",
    "                    total_value = value\n",
    "                    start_position = i\n",
    "                else:\n",
    "                    max_value = max(max_value, value)\n",
    "                    total_value += value\n",
    "            else:\n",
    "                if in_event:\n",
    "                    in_event = False\n",
    "                    durations.append(i - event_start)\n",
    "                    max_values.append(max_value)\n",
    "                    avg_values.append(total_value / (i - event_start))\n",
    "                    final_position = i\n",
    "        \n",
    "        if in_event:\n",
    "            durations.append(len(row) - event_start)\n",
    "            max_values.append(max_value)\n",
    "            avg_values.append(total_value / (len(row) - event_start))\n",
    "\n",
    "        \n",
    "        # get the average event duration\n",
    "        avg_duration = (sum(durations) / len(durations)) if len(durations) > 0 else np.NaN\n",
    "        \n",
    "        # max duration \n",
    "        max_duration = max(durations) if len(durations) > 0 else np.NaN\n",
    "        \n",
    "        # get the average max value\n",
    "        avg_max_value = (sum(max_values) / (len(max_values)) if len(max_values) > 0 else np.NaN)\n",
    "        num_firings = sum(durations)\n",
    "        \n",
    "        # `zip` avg_valuea, max_values, durations and add it as a subrecord which we could unfurl later\n",
    "        event_stats = zip(avg_values, max_values, durations)\n",
    "        event_stats = [\n",
    "            {\n",
    "                'avg_value': avg_value,\n",
    "                'max_value': max_value,\n",
    "                'duration': duration,\n",
    "                'start_position': start_position, \n",
    "                'final_position': final_position,\n",
    "            }\n",
    "            for avg_value, max_value, duration in event_stats\n",
    "        ]\n",
    "\n",
    "        results.append({\n",
    "            'num_events': num_events,\n",
    "            'num_firings': num_firings,\n",
    "            'avg_values': avg_values,\n",
    "            'max_values': max_values,\n",
    "            'durations': durations,\n",
    "            'avg_duration': avg_duration,\n",
    "            'max_duration': max_duration,\n",
    "            'avg_max_value': avg_max_value,\n",
    "            'events': event_stats,\n",
    "        })\n",
    "\n",
    "    return results\n",
    "\n",
    "# Example usage\n",
    "\n",
    "\n",
    "tensor =feature_acts_example[:, features_of_interest].T\n",
    "\n",
    "start_time = time.time()\n",
    "results = analyze_events(tensor)\n",
    "end_time = time.time()\n",
    "print(f\"Time taken: {end_time - start_time}\")\n",
    "\n",
    "feature_prompt_df  = pd.DataFrame(results, index=features_of_interest)\n",
    "feature_prompt_df[\"feature\"] = feature_prompt_df.index\n",
    "feature_prompt_df.explode('events').sort_values(\"num_events\", ascending=False)\n",
    "display(feature_prompt_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert events to a dataframe\n",
    "tmp = feature_prompt_df.explode('events').apply(lambda x: pd.Series(x['events']), axis=1).reset_index().rename(columns={\"index\": \"feature\"})\n",
    "tmp[\"feature\"] = tmp[\"feature\"].astype(str)\n",
    "px.scatter_matrix(tmp, \n",
    "                  title=\"Event stats for each feature\", color=\"feature\", dimensions=[\"avg_value\", \"max_value\", \"duration\"],\n",
    "                  width=1000, height=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(feature_prompt_df, x=\"num_firings\", y=\"num_events\", hover_name=feature_prompt_df.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens_list = []\n",
    "pbar = tqdm(range(128*6))\n",
    "for i in pbar:\n",
    "    all_tokens_list.append(activation_store.get_batch_tokens())\n",
    "all_tokens = torch.cat(all_tokens_list, dim=0)\n",
    "print(all_tokens.shape)\n",
    "all_tokens = all_tokens[torch.randperm(all_tokens.shape[0])]\n",
    "tokens = all_tokens[:4096*6]\n",
    "del all_tokens\n",
    "torch.mps.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n_prompts = 1000\n",
    "# features_of_interest = features_of_interest\n",
    "features_of_interest = torch.randperm(feature_acts.shape[-1])[:3000].tolist()\n",
    "token_dfs = []\n",
    "event_dfs = []\n",
    "feature_acts_all = []\n",
    "\n",
    "for prompt_index in tqdm(range(n_prompts)):\n",
    "    prompt_tokens = tokens[prompt_index].unsqueeze(0)\n",
    "    # make token df \n",
    "    token_df = make_token_df(model, prompt_tokens, len_suffix=5, len_prefix=10)\n",
    "    token_df[\"prompt_index\"] = prompt_index\n",
    "    \n",
    "    (original_logits, original_loss), original_cache = model.run_with_cache(prompt_tokens, return_type=\"both\", loss_per_token=True)\n",
    "    token_df['loss'] = original_loss.flatten().tolist() + [np.nan]\n",
    "    \n",
    "    original_act = original_cache[sparse_autoencoder.cfg.hook_point]\n",
    "    sae_out, feature_acts, _, mse_loss, _ = sparse_autoencoder(original_act)\n",
    "\n",
    "    feature_acts_of_interest = feature_acts[0, :, features_of_interest].T\n",
    "    results = analyze_events(feature_acts_of_interest)\n",
    "    events_df  = pd.DataFrame(results, index=features_of_interest)\n",
    "    events_df[\"feature\"] = events_df.index.astype(str)\n",
    "    events_df[\"prompt_index\"] = prompt_index\n",
    "    events_df = events_df[events_df[\"num_events\"] > 0]\n",
    "    \n",
    "        \n",
    "    token_dfs.append(token_df.reset_index(drop=True))\n",
    "    event_dfs.append(events_df.reset_index(drop=True))\n",
    "    feature_acts_all.append(feature_acts_of_interest)\n",
    "    \n",
    "feature_acts_all = torch.stack(feature_acts_all, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_df = pd.concat(token_dfs).reset_index(drop=True)\n",
    "prompt_event_df = pd.concat(event_dfs).reset_index(drop=True)\n",
    "events_df = prompt_event_df.explode('events').apply(lambda x: pd.Series(x['events']), axis=1)\n",
    "events_df[\"feature\"] = events_df.index.map(lambda x: prompt_event_df.feature[x]).astype(str)\n",
    "events_df[\"prompt_index\"] = events_df.index.map(lambda x: prompt_event_df.prompt_index[x])\n",
    "#\n",
    "# tmp[\"feature\"] = tmp.index.map(lambda x: event_df[\"feature\"][x]).astype(str)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_event_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter_matrix(prompt_event_df, \n",
    "                  title=\"Event stats for each feature\", color=\"feature\", dimensions=[\"num_events\", \"num_firings\", \"avg_duration\", \"avg_max_value\"],\n",
    "                  width=1000, height=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_event_agg_df = prompt_event_df.groupby([\"feature\", \"prompt_index\"]).agg({\"num_events\": \"sum\", \"num_firings\": \"sum\", \"avg_duration\": \"mean\"}).sort_values(\"num_events\", ascending=False).reset_index()\n",
    "prompt_event_agg_df[\"firings_per_event\"] = prompt_event_agg_df[\"num_firings\"] / prompt_event_agg_df[\"num_events\"]\n",
    "px.strip(prompt_event_agg_df, x = \"feature\", y = \"firings_per_event\", color=\"feature\", title=\"Firings per event\",\n",
    "         hover_data= [\"num_events\", \"num_firings\", \"avg_duration\", \"prompt_index\"],\n",
    "         ).show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_event_agg_df.feature.unique().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_firings_per_event = prompt_event_agg_df.groupby(\"feature\").firings_per_event.mean().sort_values(ascending=False)\n",
    "std_firings_per_event = prompt_event_agg_df.groupby(\"feature\").firings_per_event.std().sort_values(ascending=False)\n",
    "px.scatter(x=mean_firings_per_event.values, \n",
    "           y = std_firings_per_event.values,\n",
    "           hover_name=mean_firings_per_event.index,\n",
    "           marginal_x=\"histogram\",\n",
    "              marginal_y=\"histogram\",\n",
    "           labels = {\"x\": \"Mean firings per event\", \"y\": \"Std firings per event\"},\n",
    "           title=\"Mean vs Std firings per event\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in mean_firings_per_event[mean_firings_per_event<1.3].index[10:30]:\n",
    "    render_feature_dashboard(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Given some token, let's get the distribution of tokens it began firing on\n",
    "events_df[\"token_df_id\"] = events_df.apply(lambda x: token_df_id_from_prompt_and_pos(x[\"prompt_index\"], x[\"start_position\"]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_df.join(token_df, =\"token_df_id\").head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_event_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we want to get the token distribution from events. \n",
    "feature_idx = features_of_interest.index(22768)\n",
    "token_df[\"feature_22768\"] = feature_acts_all[:, feature_idx].flatten().tolist() \n",
    "# token_df[\"feature_22768_quantile\"] = pd.qcut(token_df[\"feature_22768\"], 10, labels=False, duplicates=\"drop\")\n",
    "idxes = token_df.sort_values(\"feature_22768\", ascending=False).head(30).index\n",
    "idxes_minus_1 = idxes - 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_df.groupby()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_df_id_from_prompt_and_pos = lambda prompt_index, pos: token_df[(token_df[\"prompt_index\"] == prompt_index) & (token_df[\"pos\"] == pos)].index[0]\n",
    "str_token_from_prompt_and_pos = lambda prompt_index, pos: token_df[(token_df[\"prompt_index\"] == prompt_index) & (token_df[\"pos\"] == pos)].str_tokens.values[0]\n",
    "\n",
    "token_df_id_from_prompt_and_pos(12,3)\n",
    "# str_token_from_prompt_and_pos(12,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_df.groupby(\"feature\").agg({\"duration\": \"std\"}).sort_values(\"duration\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start id word_cloud\n",
    "\n",
    "feature_of_interest = 22768\n",
    "\n",
    "# step 1. Get the start and end points for the text we care about\n",
    "events_df[events_df.duration == 4]#[events_df.feature == str(feature_of_interest)]\n",
    "# px.strip(tmp, x = \"duration\", y = \"avg_value\",title=\"Firings per event\")\\\n",
    "    \n",
    "\n",
    "# step 2. for each of these, get prompt\n",
    "token_df_ids = [token_df_id_from_prompt_and_pos(i,j) for i,j in zip(tmp.prompt_index, tmp.start_position)]\n",
    "minus_one_token_ids = [token_df_id_from_prompt_and_pos(i,j) for i,j in zip(tmp.prompt_index, tmp.start_position - 1)]\n",
    "final_token_ids = [token_df_id_from_prompt_and_pos(i,j) for i,j in zip(tmp.prompt_index, tmp.final_position.fillna(128) -1)]\n",
    "minus_one_token_fire = token_df.iloc[minus_one_token_ids].str_tokens.reset_index(drop=True)\n",
    "first_token_fire = token_df.iloc[token_df_ids].str_tokens.reset_index(drop=True)\n",
    "final_token_fire = token_df.iloc[final_token_ids].str_tokens.reset_index(drop=True)\n",
    "\n",
    "tmp = pd.concat([first_token_fire, minus_one_token_fire, final_token_fire], axis=1)\n",
    "\n",
    "tmp.columns = [\"first_token\", \"minus_one_token\", \"final_token\"]\n",
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Dashboards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate new features if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens_list = []\n",
    "pbar = tqdm(range(128*6))\n",
    "for i in pbar:\n",
    "    all_tokens_list.append(activation_store.get_batch_tokens())\n",
    "all_tokens = torch.cat(all_tokens_list, dim=0)\n",
    "print(all_tokens.shape)\n",
    "all_tokens = all_tokens[torch.randperm(all_tokens.shape[0])]\n",
    "tokens = all_tokens[:4096*6]\n",
    "del all_tokens\n",
    "torch.mps.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sae_analysis.visualizer import data_fns, model_fns, html_fns\n",
    "import importlib\n",
    "\n",
    "importlib.reload(data_fns)\n",
    "importlib.reload(html_fns)\n",
    "from sae_analysis.visualizer.data_fns import get_feature_data, FeatureData\n",
    "\n",
    "# Currently, don't think much more time can be squeezed out of it. Maybe the best saving would be to\n",
    "# make the entire sequence indexing parallelized, but that's possibly not worth it right now.\n",
    "\n",
    "max_batch_size = 512\n",
    "total_batch_size = 4096*6\n",
    "feature_idx = [i for i in range(sparse_autoencoder.cfg.d_sae)]\n",
    "feature_idx = torch.tensor(feature_idx).reshape(512, -1)\n",
    "feature_idx = [feature_idx[i].tolist() for i in range(512)]\n",
    "# max_batch_size = 512\n",
    "# total_batch_size = 16384\n",
    "# feature_idx = list(range(1000))\n",
    "\n",
    "\n",
    "# shuffle\n",
    "interesting_features = mean_firings_per_event.index.astype(int).to_list()\n",
    "\n",
    "feature_data = get_feature_data(\n",
    "    encoder=sparse_autoencoder,\n",
    "    # encoder_B=sparse_autoencoder,\n",
    "    model=model,\n",
    "    hook_point=sparse_autoencoder.cfg.hook_point,\n",
    "    hook_point_layer=sparse_autoencoder.cfg.hook_point_layer,\n",
    "    hook_point_head_index=None,\n",
    "    tokens=tokens,\n",
    "    feature_idx=interesting_features,\n",
    "    max_batch_size=max_batch_size,\n",
    "    left_hand_k = 3,\n",
    "    buffer = (5, 5),\n",
    "    n_groups = 10,\n",
    "    first_group_size = 20,\n",
    "    other_groups_size = 5,\n",
    "    verbose = True,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for test_idx in feature_data.keys():\n",
    "    html_str = feature_data[test_idx].get_all_html()\n",
    "    with open(f\"../week_8_jan/gpt2_small_features/data_{test_idx:04}.html\", \"w\") as f:\n",
    "        f.write(html_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mats_sae_training",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
