{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append(\"../..\")\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from importlib import reload\n",
    "from tqdm import tqdm\n",
    "\n",
    "import joseph\n",
    "from joseph.analysis import *\n",
    "from joseph.visualisation import *\n",
    "from joseph.utils import *\n",
    "from joseph.data import *\n",
    "\n",
    "\n",
    "reload(joseph.analysis)\n",
    "reload(joseph.visualisation)\n",
    "reload(joseph.utils)\n",
    "reload(joseph.data)\n",
    "\n",
    "from joseph.analysis import *\n",
    "from joseph.visualisation import *\n",
    "from joseph.utils import *\n",
    "from joseph.data import *\n",
    "\n",
    "# turn torch grad tracking off\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = HookedTransformer.from_pretrained(\n",
    "    \"gpt2-small\",\n",
    "    # \"pythia-2.8b\",\n",
    "    # \"pythia-70m-deduped\",\n",
    "    # \"tiny-stories-2L-33M\",\n",
    "    # \"attn-only-2l\",\n",
    "    # center_unembed=True,\n",
    "    # center_writing_weights=True,\n",
    "    # fold_ln=True,\n",
    "    # refactor_factored_attn_matrices=True,\n",
    "    fold_ln=True,\n",
    ")\n",
    "model.set_use_split_qkv_input(True)\n",
    "model.set_use_attn_result(True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load SAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "path = \"../week_8_jan/artifacts/sparse_autoencoder_gpt2-small_blocks.10.hook_resid_pre_49152:v28/1100001280_sparse_autoencoder_gpt2-small_blocks.10.hook_resid_pre_49152.pt\"\n",
    "# path = \"../week_8_jan/artifacts/sparse_autoencoder_gpt2-small_blocks.5.hook_resid_pre_49152:v9/final_sparse_autoencoder_gpt2-small_blocks.5.hook_resid_pre_49152.pt\"\n",
    "sparse_autoencoder = SparseAutoencoder.load_from_pretrained(path)\n",
    "\n",
    "print(sparse_autoencoder.cfg)\n",
    "\n",
    "\n",
    "# sanity check\n",
    "text = \"Many important transition points in the history of science have been moments when science 'zoomed in.' At these points, we develop a visualization or tool that allows us to see the world in a new level of detail, and a new field of science develops to study the world through this lens.\"\n",
    "model(text, return_type=\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sae_training.utils import LMSparseAutoencoderSessionloader\n",
    "model, sparse_autoencoder, activation_store = LMSparseAutoencoderSessionloader.load_session_from_pretrained(\n",
    "    path\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Dashboard Util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import webbrowser\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "path_to_html = \"../week_8_jan/gpt2_small_features\"\n",
    "def render_feature_dashboard(feature_id):\n",
    "    \n",
    "    path = f\"{path_to_html}/data_{feature_id:04}.html\"\n",
    "    \n",
    "    print(f\"Feature {feature_id}\")\n",
    "    if os.path.exists(path):\n",
    "        # with open(path, \"r\") as f:\n",
    "        #     html = f.read()\n",
    "        #     display(HTML(html))\n",
    "        webbrowser.open_new_tab(\"file://\" + os.path.abspath(path))\n",
    "    else:\n",
    "        print(\"No HTML file found\")\n",
    "    \n",
    "    return\n",
    "\n",
    "# for feature in [100,300,400]:\n",
    "#     render_feature_dashboard(feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features by Token in an Example Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joseph\n",
    "reload(joseph.analysis)\n",
    "from joseph.analysis import *\n",
    "\n",
    "\n",
    "title = \"Anthrax\"\n",
    "prompt = \"Anthrax is a serious infectious disease caused by gram-positive, rod-shaped bacteria known as Bacillus anthracis. It occurs naturally in soil and commonly affects domestic and wild animals around the world. People can get sick with anthrax if they come in contact with infected animals or contaminated animal products.\"\n",
    "POS_INTEREST = 17\n",
    "\n",
    "\n",
    "token_df, original_cache, cache_reconstructed_query, feature_acts = eval_prompt([prompt], model, sparse_autoencoder, head_idx_override=7)\n",
    "filter_cols = [\"str_tokens\", \"unique_token\", \"context\", \"batch\", \"pos\", \"label\", \"loss\", \"loss_diff\", \"mse_loss\", \"num_active_features\", \"explained_variance\", \"kl_divergence\",\n",
    "            \"top_k_features\"]\n",
    "# display(token_df[filter_cols].style.background_gradient(\n",
    "#     subset=[\"loss_diff\", \"mse_loss\",\"explained_variance\", \"num_active_features\", \"kl_divergence\"],\n",
    "#     cmap=\"coolwarm\"))\n",
    "\n",
    "\n",
    "\n",
    "UNIQUE_TOKEN_INTEREST = token_df[\"unique_token\"][POS_INTEREST]\n",
    "feature_acts_of_interest = feature_acts[POS_INTEREST]\n",
    "# plot_line_with_top_10_labels(feature_acts_of_interest, \"\", 25)\n",
    "# vals, inds = torch.topk(feature_acts_of_interest,39)\n",
    "\n",
    "top_k_feature_inds = (feature_acts[1:] > 0).sum(dim=0).nonzero().squeeze()\n",
    "\n",
    "features_acts_by_token_df = pd.DataFrame(\n",
    "    feature_acts[:,top_k_feature_inds[:]].detach().cpu().T,\n",
    "    index = [f\"feature_{i}\" for i in top_k_feature_inds.flatten().tolist()],\n",
    "    columns = token_df[\"unique_token\"])\n",
    "\n",
    "# features_acts_by_token_df.sort_values(by=\",/12\", ascending=False).head(10).style.background_gradient(\n",
    "#     cmap=\"coolwarm\", axis=0)\n",
    "\n",
    "# px.imshow(features_acts_by_token_df.sort_values(by=\",/12\", ascending=False).head(10).T.corr(), color_continuous_midpoint=0, color_continuous_scale=\"RdBu\")\n",
    "\n",
    "tmp = features_acts_by_token_df.sort_values(UNIQUE_TOKEN_INTEREST, ascending=False).T\n",
    "dashboard_features = features_acts_by_token_df.sort_values(UNIQUE_TOKEN_INTEREST, ascending=False).index[:10].to_series().apply(lambda x: x.split(\"_\")[1]).tolist()\n",
    "for feature in dashboard_features:\n",
    "    render_feature_dashboard(feature)\n",
    "    \n",
    "px.line(token_df,\n",
    "        x = \"unique_token\",\n",
    "        y = \"loss\",\n",
    "        hover_data=[\"pos\", \"label\", \"loss_diff\", \"mse_loss\", \"num_active_features\", \"explained_variance\"],\n",
    "        height = 300).show()\n",
    "\n",
    "px.line(tmp, \n",
    "        title=f\"{title}: Features Activation by Token in Prompt\", \n",
    "        color_discrete_sequence=px.colors.qualitative.Plotly,\n",
    "        height=1000).show()\n",
    "\n",
    "tmp = features_acts_by_token_df.head(100).T\n",
    "px.imshow(tmp, \n",
    "            title=f\"{title}: Top k features by activation\", \n",
    "            color_continuous_midpoint=0, \n",
    "            color_continuous_scale=\"RdBu\", \n",
    "            height=800).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Searching over Features by Token or Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens_list = []\n",
    "pbar = tqdm(range(128*6))\n",
    "for i in pbar:\n",
    "    all_tokens_list.append(activation_store.get_batch_tokens())\n",
    "all_tokens = torch.cat(all_tokens_list, dim=0)\n",
    "print(all_tokens.shape)\n",
    "all_tokens = all_tokens[torch.randperm(all_tokens.shape[0])]\n",
    "tokens = all_tokens[:4096*6]\n",
    "del all_tokens\n",
    "torch.mps.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_tokens  = activation_store.get_batch_tokens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperpars\n",
    "target_word = \" to\"\n",
    "token = model.to_tokens(target_word, prepend_bos=False)\n",
    "assert len(token) == 1, \"Token must be a single token\"\n",
    "target_n_topical_prompts = 500\n",
    "\n",
    "n_topical_prompts = 0\n",
    "all_tokens_list = []\n",
    "pbar = tqdm(total = target_n_topical_prompts)\n",
    "while n_topical_prompts < target_n_topical_prompts:\n",
    "    batch_tokens  = activation_store.get_batch_tokens()\n",
    "    \n",
    "    # filter batch tokens for containing the target word's token\n",
    "    mask = (batch_tokens == token).any(dim=-1)\n",
    "    batch_tokens = batch_tokens[mask]\n",
    "    all_tokens_list.append(batch_tokens)\n",
    "    n_topical_prompts += batch_tokens.shape[0]\n",
    "    \n",
    "    pbar.update(batch_tokens.shape[0])\n",
    "\n",
    "all_tokens = torch.cat(all_tokens_list, dim=0)\n",
    "torch.mps.empty_cache()\n",
    "\n",
    "# save the tokens to disk\n",
    "torch.save(all_tokens, f\"{target_word}_prompts.pt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok so the idea here is that we only track a fraction of tokens, let's go with \" anthrax\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample from prompts containing proxies\n",
    "import re \n",
    "# import HTML\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "\n",
    "def find_word_and_highlight(prompt, prompt_proxy_regex):\n",
    "    start_pos = re.search(prompt_proxy_regex, prompt, flags=re.IGNORECASE).start()\n",
    "    end_pos = re.search(prompt_proxy_regex, prompt, flags=re.IGNORECASE).end()\n",
    "    # style with red text\n",
    "    style_tag = \"<span style='color:red'>\"\n",
    "    prompt = prompt[:start_pos] + f'{style_tag}'+ prompt[start_pos:end_pos] + \"</span>\" + prompt[end_pos:]\n",
    "    display(HTML(prompt))\n",
    "\n",
    "random_token= torch.randint(0, all_tokens.shape[0], (1,)).item()\n",
    "find_word_and_highlight(model.to_string(all_tokens[random_token]), r\" \\bto\\b \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_dfs = []\n",
    "event_dfs = []\n",
    "feature_acts_all = []\n",
    "\n",
    "pbar = tqdm(range(all_tokens.shape[0]))\n",
    "\n",
    "for prompt_index in pbar:\n",
    "    prompt_tokens = all_tokens[prompt_index].unsqueeze(0)\n",
    "    \n",
    "    token_df = make_token_df(model, prompt_tokens, len_suffix=5, len_prefix=10)\n",
    "    token_df[\"prompt_index\"] = prompt_index\n",
    "    \n",
    "    (original_logits, original_loss), original_cache = model.run_with_cache(prompt_tokens, return_type=\"both\", loss_per_token=True)\n",
    "    token_df['loss'] = original_loss.flatten().tolist() + [np.nan]\n",
    "    \n",
    "    original_act = original_cache[sparse_autoencoder.cfg.hook_point]\n",
    "    sae_out, feature_acts, _, mse_loss, _ = sparse_autoencoder(original_act)\n",
    "\n",
    "    feature_acts_of_interest = feature_acts[0, :, :]\n",
    "    token_dfs.append(token_df.reset_index(drop=True))\n",
    "    feature_acts_all.append(feature_acts_of_interest)\n",
    "    \n",
    "feature_acts_all = torch.stack(feature_acts_all, dim=0)\n",
    "token_df = pd.concat(token_dfs).reset_index(drop=True)\n",
    "\n",
    "bacteria_token_mask = token_df.str_tokens.str.contains(target_word, regex=False)\n",
    "feature_acts_bacteria = feature_acts_all.flatten(0,1)[bacteria_token_mask]\n",
    "feature_acts_bacteria.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ok so now we want to filter for features acts and token_df positions which actually include the word bacteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals, inds = torch.topk(feature_acts_bacteria.mean(dim=0),100)\n",
    "tmp = pd.DataFrame(vals.detach().cpu().numpy(), index=inds.detach().cpu().numpy(), columns=[\"mean_activation\"])\n",
    "tmp = tmp.sort_values(\"mean_activation\", ascending=False)\n",
    "tmp.index= tmp.index.map(lambda x: f\"feature_{x}\")\n",
    "px.bar(\n",
    "    tmp,\n",
    "    x = tmp.index,\n",
    "    y = \"mean_activation\",\n",
    "    title=\"Mean activation of top 100 features for bacteria\",\n",
    "    text_auto=True,\n",
    "    height=500,\n",
    "    color_discrete_sequence=px.colors.qualitative.Plotly,\n",
    ").show()\n",
    "\n",
    "\n",
    "vals, inds = torch.topk((feature_acts_bacteria > 0).float().mean(dim=0),100)\n",
    "tmp = pd.DataFrame(vals.detach().cpu().numpy(), index=inds.detach().cpu().numpy(), columns=[\"mean_activation\"])\n",
    "tmp = tmp.sort_values(\"mean_activation\", ascending=False)\n",
    "tmp.index= tmp.index.map(lambda x: f\"feature_{x}\")\n",
    "px.bar(\n",
    "    tmp,\n",
    "    x = tmp.index,\n",
    "    y = \"mean_activation\",\n",
    "    title=\"Mean Binary Activation of top 100 features for bacteria\",\n",
    "    text_auto=True,\n",
    "    height=500,\n",
    "    color_discrete_sequence=px.colors.qualitative.Plotly,\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in tmp.index[10:40]:\n",
    "    render_feature_dashboard(feature.split(\"_\")[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(token_df[\"unique_token\"] == target_word).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_acts_all.flatten(0,1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's get precision vs recall on all the features for the \"to\" token. \n",
    "\n",
    "total_fires = (feature_acts_bacteria > 0).float().sum(dim=0)\n",
    "total_fires_on_target = (feature_acts_all.flatten(0,1)[token_df[\"str_tokens\"] == target_word]>0).float().sum(dim=0)\n",
    "\n",
    "total_target_word_appearances = (token_df[\"str_tokens\"] == target_word).sum()\n",
    "\n",
    "precision = total_fires_on_target / total_fires\n",
    "recall = total_fires_on_target / total_target_word_appearances\n",
    "\n",
    "precision_recall_df = pd.DataFrame(\n",
    "    torch.stack([\n",
    "        total_fires,\n",
    "        precision,\n",
    "        recall], dim=1).detach().cpu().numpy(),\n",
    "    index = [f\"feature_{i}\" for i in range(precision.shape[0])],\n",
    "    columns = [\"total_fires\",\"precision\", \"recall\"]\n",
    ")\n",
    "precision_recall_df = precision_recall_df[precision_recall_df[\"total_fires\"] > 100]\n",
    "# precision_recall_df.head(10)\n",
    "px.scatter(\n",
    "    precision_recall_df,\n",
    "    x = \"precision\",\n",
    "    y = \"recall\",\n",
    "    title=\"Precision vs Recall for features\",\n",
    "    # text=precision_recall_df.index,\n",
    "    height=500,\n",
    "    color_discrete_sequence=px.colors.qualitative.Plotly,\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_target_word_appearances"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mats_sae_training",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
