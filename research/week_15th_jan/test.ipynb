{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append(\"../..\")\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from importlib import reload\n",
    "from tqdm import tqdm\n",
    "\n",
    "import joseph\n",
    "from joseph.analysis import *\n",
    "from joseph.visualisation import *\n",
    "from joseph.utils import *\n",
    "from joseph.data import *\n",
    "\n",
    "\n",
    "reload(joseph.analysis)\n",
    "reload(joseph.visualisation)\n",
    "reload(joseph.utils)\n",
    "reload(joseph.data)\n",
    "\n",
    "from joseph.analysis import *\n",
    "from joseph.visualisation import *\n",
    "from joseph.utils import *\n",
    "from joseph.data import *\n",
    "\n",
    "# turn torch grad tracking off\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = HookedTransformer.from_pretrained(\n",
    "    \"gpt2-small\",\n",
    "    # \"pythia-2.8b\",\n",
    "    # \"pythia-70m-deduped\",\n",
    "    # \"tiny-stories-2L-33M\",\n",
    "    # \"attn-only-2l\",\n",
    "    # center_unembed=True,\n",
    "    # center_writing_weights=True,\n",
    "    # fold_ln=True,\n",
    "    # refactor_factored_attn_matrices=True,\n",
    "    fold_ln=True,\n",
    ")\n",
    "model.set_use_split_qkv_input(True)\n",
    "model.set_use_attn_result(True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load SAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "path = \"../week_8_jan/artifacts/sparse_autoencoder_gpt2-small_blocks.10.hook_resid_pre_49152:v28/1100001280_sparse_autoencoder_gpt2-small_blocks.10.hook_resid_pre_49152.pt\"\n",
    "# path = \"./artifacts/sparse_autoencoder_gpt2-small_blocks.5.hook_resid_pre_49152:v9/final_sparse_autoencoder_gpt2-small_blocks.5.hook_resid_pre_49152.pt\"\n",
    "sparse_autoencoder = SparseAutoencoder.load_from_pretrained(path)\n",
    "\n",
    "print(sparse_autoencoder.cfg)\n",
    "\n",
    "\n",
    "# sanity check\n",
    "text = \"Many important transition points in the history of science have been moments when science 'zoomed in.' At these points, we develop a visualization or tool that allows us to see the world in a new level of detail, and a new field of science develops to study the world through this lens.\"\n",
    "model(text, return_type=\"loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ** What is a correlative conjunction?  **\n",
    " \n",
    "Correlative conjunctions are conjunctions used to illustrate how two words or phrases within a sentence relate to each other. Correlative conjunctions always come in pairs. \n",
    "\n",
    "Though they can illustrate a correlation between the two words or phrases, they don’t necessarily have to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Either - or - Success\n",
    "prompt = f\"Either you are with me,\"\n",
    "answer = \" or\"\n",
    "# utils.test_prompt(prompt, answer, model, prepend_space_to_answer=False, top_k=3)\n",
    "\n",
    "# Such - that - Fail\n",
    "prompt = f\"Such is the intensity of the pollen outside,\"\n",
    "answer = \" that\"\n",
    "# utils.test_prompt(prompt, answer, model, prepend_space_to_answer=False, top_k=3)\n",
    "\n",
    "# Both - And - Success\n",
    "prompt = f\"My parents went to both Hawaii\"\n",
    "answer = \" and\"\n",
    "# utils.test_prompt(prompt, answer, model, prepend_space_to_answer=False, top_k=3)\n",
    "\n",
    "# No sonner - than - Success\n",
    "prompt = f\"She would no sooner cheat on an exam\"\n",
    "answer = \" than\"\n",
    "# utils.test_prompt(prompt, answer, model, prepend_space_to_answer=False, top_k=3)\n",
    "\n",
    "# Rather - than - Pass\n",
    "prompt = f\"They would rather go to the movies\"\n",
    "answer = \" than\"\n",
    "# utils.test_prompt(prompt, answer, model, prepend_space_to_answer=False, top_k=3)\n",
    "\n",
    "# Not only - but - Pass\n",
    "prompt = f\"Not only did my boyfriend buy me a Nintendo Switch,\"\n",
    "answer = \" but\"\n",
    "# utils.test_prompt(prompt, answer, model, prepend_space_to_answer=False, top_k=3)\n",
    "\n",
    "# not only - but also - Pass\n",
    "prompt = f\"Not only did my boyfriend buy me a Nintendo Switch, but\"\n",
    "answer = \" also\"\n",
    "# utils.test_prompt(prompt, answer, model, prepend_space_to_answer=False, top_k=3)\n",
    "\n",
    "# as many - as - Pass\n",
    "prompt = f\"There were as many applicants\"\n",
    "answer = \" as\"\n",
    "# utils.test_prompt(prompt, answer, model, prepend_space_to_answer=False, top_k=3)\n",
    "\n",
    "# Whether - or - Pass\n",
    "prompt = f\"Whether you bike\"\n",
    "answer = \" or\"\n",
    "# utils.test_prompt(prompt, answer, model, prepend_space_to_answer=False, top_k=3)\n",
    "\n",
    "# Neither - nor - Pass\n",
    "prompt = f\"I wasn’t hired at any of the companies I’d applied to. Neither my experience\"\n",
    "answer = \" nor\"\n",
    "utils.test_prompt(prompt, answer, model, prepend_space_to_answer=False, top_k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The more... the more - Pass\n",
    "prompt = f\"the more you learn, the\" #more you realize how much you don't know\"\n",
    "answer = \" more\"\n",
    "# utils.test_prompt(prompt, answer, model, prepend_space_to_answer=False, top_k=3)\n",
    "\n",
    "# less on ... and -> more - Pass\n",
    "prompt = f\"I think we should focus less on talking about doing the work and\"#more on doing the work\"\n",
    "answer = \" more\"\n",
    "utils.test_prompt(prompt, answer, model, prepend_space_to_answer=False, top_k=3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If then - fail\n",
    "prompt = f\"If it is the case that it rains,\"\n",
    "answer = \" then\"\n",
    "utils.test_prompt(prompt, answer, model, prepend_space_to_answer=False, top_k=3)\n",
    "\n",
    "# Unless - Otherwise\n",
    "prompt = f\"Unless we hear\"\n",
    "answer = \" otherwise\"\n",
    "utils.test_prompt(prompt, answer, model, prepend_space_to_answer=False, top_k=3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Red-Teaming Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"When Bob and Michael went to the shops, Bob gave the shopping to\"\n",
    "answer = \" Michael\"\n",
    "# prompt = \"All's fair in love and\"\n",
    "# answer = \" war\"\n",
    "# prompt = \" The cat is cute. The dog is\"\n",
    "# prompt = \" Alice, with her keen intelligence and artistic talent, discussed philosophy with Bob, who shared her intellect and also possessed remarkable culinary skills, while\"\n",
    "# answer = \" cute\"\n",
    "model.reset_hooks()\n",
    "utils.test_prompt(prompt, answer, model)\n",
    "\n",
    "HEAD_HOOK_RESULT_NAME = \"blocks.10.attn.hook_z\"\n",
    "LAYER_IDX = sparse_autoencoder.cfg.hook_point_layer\n",
    "HEAD_IDX = 7\n",
    "def hook_to_ablate_head(head_output: Float[Tensor, \"batch seq_len head_idx d_head\"], hook: HookPoint, head = (LAYER_IDX, HEAD_IDX)):\n",
    "    print(hook.layer(), hook.name)\n",
    "    assert head[0] == hook.layer(), f\"{head[0]} != {hook.layer()}\"\n",
    "    assert (\"result\" in hook.name) or (\"q\" in hook.name) or (\"z\" in hook.name)\n",
    "    head_output[:, :, head[1], :] = 0\n",
    "    return head_output\n",
    "\n",
    "with model.hooks(fwd_hooks=[(HEAD_HOOK_RESULT_NAME, hook_to_ablate_head)]):\n",
    "    utils.test_prompt(prompt, answer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joseph\n",
    "reload(joseph.analysis)\n",
    "from joseph.analysis import *\n",
    "\n",
    "\n",
    "token_df, original_cache, cache_reconstructed_query, feature_acts = eval_prompt([prompt + answer], model, sparse_autoencoder, head_idx_override=7)\n",
    "print(token_df.columns)\n",
    "filter_cols = [\"str_tokens\", \"unique_token\", \"context\", \"batch\", \"pos\", \"label\", \"loss\", \"loss_diff\", \"mse_loss\", \"num_active_features\", \"explained_variance\", \"kl_divergence\",\n",
    "               \"top_k_features\"]\n",
    "token_df[filter_cols].style.background_gradient(\n",
    "    subset=[\"loss_diff\", \"mse_loss\",\"explained_variance\", \"num_active_features\", \"kl_divergence\"],\n",
    "    cmap=\"coolwarm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "POS_INTEREST = 14# index from 0.\n",
    "print(token_df.shape)\n",
    "print(feature_acts.shape)\n",
    "print(token_df[\"unique_token\"][POS_INTEREST]) \n",
    "feature_acts_of_interest = feature_acts[POS_INTEREST]\n",
    "plot_line_with_top_10_labels(feature_acts_of_interest, \"\", 30)\n",
    "vals, inds = torch.topk(feature_acts_of_interest,64)\n",
    "print(vals.nonzero().shape)\n",
    "print(inds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import webbrowser\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "path_to_html = \"../week_8_jan/gpt2_small_features\"\n",
    "def render_feature_dashboard(feature_id):\n",
    "    \n",
    "    path = f\"{path_to_html}/data_{feature_id}.html\"\n",
    "    \n",
    "    \n",
    "    print(f\"Feature {feature_id}\")\n",
    "    \n",
    "    if os.path.exists(path):\n",
    "        # with open(path, \"r\") as f:\n",
    "        #     html = f.read()\n",
    "        #     display(HTML(html))\n",
    "        webbrowser.open_new_tab(\"file://\" + os.path.abspath(path))\n",
    "    else:\n",
    "        print(\"No HTML file found\")\n",
    "    \n",
    "    return\n",
    "\n",
    "for feature in inds:\n",
    "    render_feature_dashboard(feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do:\n",
    "- Check whether we can use only the top_k features and get the same result.\n",
    "- Quickly estimate the counter-factual for why features fired in terms of tokens / this sentence.\n",
    "- Use this to reduce the dimensionality of what we're thinking about?\n",
    "- Select a feature, red-team it as hard as possible (as automatically as possible)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top - K forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = model.to_tokens([prompt + answer])\n",
    "layer_idx = sparse_autoencoder.cfg.hook_point_layer\n",
    "head_idx = 7\n",
    "position_of_interest = 14\n",
    "\n",
    "original_act = original_cache[sparse_autoencoder.cfg.hook_point]\n",
    "# token_df[\"q_norm\"] = torch.norm(original_act, dim=-1)[:,1:].flatten().tolist()\n",
    "sae_out, feature_acts, _, mse_loss, _ = sparse_autoencoder(original_act)\n",
    "head_hook_query_name = utils.get_act_name(\"q\", layer_idx)\n",
    "head_hook_resid_name = utils.get_act_name(\"resid_pre\", layer_idx)\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_top_k_sae_approximation(sparse_autoencoder, feature_acts, top_k_features = None, top_k = 10):\n",
    "    \n",
    "    # here we're using the topk by activation.\n",
    "    # this is the default. \n",
    "    if top_k_features is None:\n",
    "        top_k_features = torch.topk(feature_acts, top_k, dim=2, sorted=True).indices[0]\n",
    "    \n",
    "    feature_acts_top_k = torch.zeros_like(feature_acts)\n",
    "    feature_acts_top_k[:, :, top_k_features] = feature_acts[:, :, top_k_features]\n",
    "    new_sae_out = (feature_acts_top_k @ sparse_autoencoder.W_dec) + sparse_autoencoder.b_dec\n",
    "    return new_sae_out\n",
    "\n",
    "tok_k_tokens = 10\n",
    "max_features = (feature_acts[:, position_of_interest, :] > 0).sum().item()\n",
    "\n",
    "logits = model(tokens, return_type=\"logits\")\n",
    "top_k_vals, topk_token_inds = torch.topk(logits[0,-2,:], tok_k_tokens)\n",
    "topk_token_strs = [model.tokenizer.decode(i) for i in topk_token_inds]\n",
    "top_k_feature_vals, top_k_feature_inds = torch.topk(feature_acts[:, position_of_interest,:], max_features, dim=1, sorted=False)\n",
    "\n",
    "\n",
    "mary_unembed_dot_product = (sparse_autoencoder.W_dec[top_k_feature_inds.squeeze()] @ model.W_U[:,model.tokenizer.encode(\" Mary\")]).detach().cpu()\n",
    "\n",
    "# torch.topk(mary_unembed_dot_product, 10).indices\n",
    "\n",
    "assert  (logits[0,-2,topk_token_inds] == top_k_vals).all().item()\n",
    "\n",
    "\n",
    "vals = []\n",
    "# get the top k features by activation, and construct a new sae out \n",
    "k_s = range(max_features)\n",
    "for top_k in tqdm(k_s):\n",
    "    \n",
    "    \n",
    "    # Mary Unembed Ranking\n",
    "    # top_k_features   =top_k_feature_inds[:,torch.topk(mary_unembed_dot_product, top_k, dim=0).indices].squeeze()\n",
    "    # new_sae_out = get_top_k_sae_approximation(sparse_autoencoder, feature_acts,  top_k_features=top_k_features)\n",
    "    \n",
    "    # Feature activation ranking\n",
    "    new_sae_out = get_top_k_sae_approximation(sparse_autoencoder, feature_acts, top_k = top_k)\n",
    "    def replacement_hook(resid_pre, hook, new_resid_pre=new_sae_out, position = position_of_interest):\n",
    "        resid_pre[:,position,:] = new_resid_pre[:,position,:]\n",
    "        return resid_pre\n",
    "    \n",
    "    with model.hooks(fwd_hooks=[(head_hook_resid_name, replacement_hook)]):\n",
    "        logits = model(tokens, return_type=\"logits\")\n",
    "        token_logits = logits[0,position_of_interest,topk_token_inds]\n",
    "        \n",
    "        vals.append(token_logits.tolist())\n",
    "        \n",
    "        \n",
    "tmp = pd.DataFrame(\n",
    "    vals,\n",
    "    index = [f\"feature_{i}\" for i in top_k_feature_inds.flatten().tolist()], \n",
    "    columns = topk_token_strs).T\n",
    "\n",
    "px.line(tmp.T, title=\"Top k features vs. token logits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features over tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_acts_by_token_df = pd.DataFrame(\n",
    "    feature_acts[0,:,top_k_feature_inds[0,:]].detach().cpu().T,\n",
    "    index = [f\"feature_{i}\" for i in top_k_feature_inds.flatten().tolist()],\n",
    "    columns = token_df[\"unique_token\"])\n",
    "\n",
    "px.line(features_acts_by_token_df.sort_values(\" to/14\", ascending=False).T, title=\"Top k features by activation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap \n",
    "import hdbscan\n",
    "\n",
    "reducer = umap.UMAP(\n",
    "    n_neighbors=15,\n",
    "    min_dist=0.01,\n",
    "    n_components=2,\n",
    "    metric=\"cosine\",\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "\n",
    "ummap_result = reducer.fit_transform(sparse_autoencoder.W_dec[top_k_feature_inds[0].flatten()].detach().cpu().numpy())\n",
    "\n",
    "clusterer = hdbscan.HDBSCAN(\n",
    "    min_cluster_size=3,\n",
    "    metric=\"euclidean\",\n",
    ")\n",
    "\n",
    "clusterer.fit(ummap_result)\n",
    "\n",
    "\n",
    "temp_df = pd.DataFrame(\n",
    "    {\"feature\": top_k_feature_inds.flatten().detach().cpu(), \n",
    "        \"ummap_x\": ummap_result[:,0],\n",
    "        \"ummap_y\": ummap_result[:,1],\n",
    "        \"cluster\": [f\"cluster_{i}\" for i in clusterer.labels_],\n",
    "        })\n",
    "\n",
    "\n",
    "fig = px.scatter(\n",
    "        temp_df,#[~temp_df.outlier],\n",
    "        x=\"ummap_x\",\n",
    "        y=\"ummap_y\",\n",
    "        color=\"cluster\",\n",
    "        # color_continuous_midpoint=0,\n",
    "        color_continuous_scale=\"RdBu\", \n",
    "        hover_name=\"feature\",\n",
    "        opacity=0.5,\n",
    "        template=\"plotly\",\n",
    "    )\n",
    "\n",
    "# make it wide and tall\n",
    "fig.update_layout(height=400, width=600)\n",
    "    \n",
    "    \n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DLA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decomp, labels = original_cache.get_full_resid_decomposition(layer =  10, expand_neurons=False, return_labels=True)\n",
    "print(decomp.shape)\n",
    "inds = top_k_feature_inds.squeeze()\n",
    "tok1 = \" Michael\"\n",
    "tok2 = \" Bob\"\n",
    "test = (decomp[:,0,position_of_interest] @ model.W_U[:,model.tokenizer.encode(tok1)]).detach().cpu()\n",
    "\n",
    "tmp = pd.DataFrame(test.detach().cpu().numpy().T, columns = labels, index = [tok1])\n",
    "px.line(\n",
    "    tmp.T\n",
    ").show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DFA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's do DLA\n",
    "decomp, labels = original_cache.get_full_resid_decomposition(layer =  10, expand_neurons=False, return_labels=True)\n",
    "print(decomp.shape)\n",
    "inds = top_k_feature_inds.squeeze()\n",
    "test = (decomp[:,0,position_of_interest] @ sparse_autoencoder.W_enc[:,inds])\n",
    "# test = (decomp[:,0,-1] @ sparse_autoencoder.W_dec[inds].T) / sparse_autoencoder.W_enc[:,inds].norm(dim=0)\n",
    "tmp = pd.DataFrame(test.detach().cpu().numpy().T, columns = labels, index = [f\"feature_{i}\" for i in inds])\n",
    "px.line(\n",
    "    tmp.T[tmp.T.index.str.contains(\"mlp\")]\n",
    ").show()\n",
    "\n",
    "px.line(\n",
    "    tmp.T[tmp.T.index.str.contains(\"L\")]\n",
    ").show()\n",
    "\n",
    "# test = (decomp[:,0,-1] @ sparse_autoencoder.W_enc[:,inds])\n",
    "test = (decomp[:,0,position_of_interest] @ sparse_autoencoder.W_dec[inds].T) / sparse_autoencoder.W_enc[:,inds].norm(dim=0)\n",
    "tmp = pd.DataFrame(test.detach().cpu().numpy().T, columns = labels, index = [f\"feature_{i}\" for i in inds])\n",
    "px.line(\n",
    "    tmp.T[tmp.T.index.str.contains(\"mlp\")]\n",
    ").show()\n",
    "px.line(\n",
    "    tmp.T[tmp.T.index.str.contains(\"L\")]\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hdbscan\n",
    "\n",
    "log_feature_sparsity = torch.load(\"../week_8_jan/artifacts/sparse_autoencoder_gpt2-small_blocks.10.hook_resid_pre_49152:v28/log_feature_sparsity_5000_4.pt\")\n",
    "\n",
    "\n",
    "def get_sae_df(sparse_autoencoder, min_cluster_size=10):\n",
    "    \n",
    "    W_enc_normalized = sparse_autoencoder.W_enc.cpu() / sparse_autoencoder.W_enc.cpu().norm(dim=-2, keepdim=True)\n",
    "    d_e_projection = (sparse_autoencoder.W_dec.cpu() @ sparse_autoencoder.W_enc.cpu()).diag().detach().cpu()\n",
    "    # px.histogram(d_e_projection, nbins=100, log_x=False, title=\"d_e projection\").show()\n",
    "\n",
    "\n",
    "    d_e_projection_normalized = (sparse_autoencoder.W_dec.cpu() @ W_enc_normalized.cpu()).diag().detach().cpu()\n",
    "    # px.histogram(d_e_projection_normalized, nbins=100, log_x=False, title=\"d_e projection\").show()\n",
    "    \n",
    "    temp_df = pd.DataFrame({\n",
    "    \"log_feature_sparsity\": log_feature_sparsity,\n",
    "    \"d_e_projection\": d_e_projection,\n",
    "    \"d_e_projection_normalized\": d_e_projection_normalized,\n",
    "    \"b_enc\": sparse_autoencoder.b_enc.detach().cpu(),\n",
    "    \"feature\": [f\"feature_{i}\" for i in range(sparse_autoencoder.cfg.d_sae)],\n",
    "    \"index\": torch.arange(sparse_autoencoder.cfg.d_sae)\n",
    "    })\n",
    "\n",
    "    clusterer = hdbscan.HDBSCAN(min_cluster_size=min_cluster_size)\n",
    "    clusterer.fit(temp_df[[\"log_feature_sparsity\", \"d_e_projection\", \"b_enc\"]].values)\n",
    "    temp_df[\"cluster\"] = clusterer.labels_\n",
    "    temp_df[\"cluster_categorical\"] = temp_df[\"cluster\"].astype(str)\n",
    "    temp_df[\"outlier\"] = (temp_df.log_feature_sparsity < -9) | (temp_df.cluster == -1) | (temp_df.d_e_projection < -10000)\n",
    "    \n",
    "    return temp_df\n",
    "\n",
    "sae_df = get_sae_df(sparse_autoencoder, 50)\n",
    "display(sae_df.head())\n",
    "\n",
    "# px.bar(sae_df.cluster.value_counts().reset_index(), y = \"count\", color = \"cluster\", title = \"Cluster sizes\").show()\n",
    "\n",
    "\n",
    "def get_anthropic_3d_scatter(temp_df, remove_outliers_and_cluster = True):\n",
    "\n",
    "    if remove_outliers_and_cluster:\n",
    "        fig = px.scatter_3d(\n",
    "            # temp_df,#[~temp_df.outlier],\n",
    "            temp_df[~temp_df.outlier],\n",
    "            x=\"log_feature_sparsity\",\n",
    "            y=\"d_e_projection\",\n",
    "            z=\"b_enc\",\n",
    "            color=\"cluster\",\n",
    "            # color=\"index\",\n",
    "            hover_name=\"feature\",\n",
    "        )\n",
    "        \n",
    "    else:\n",
    "        fig = px.scatter_3d(\n",
    "            temp_df,\n",
    "            x=\"log_feature_sparsity\",\n",
    "            y=\"d_e_projection\",\n",
    "            z=\"b_enc\",\n",
    "            color=\"cluster\",\n",
    "            # color=\"index\",\n",
    "            hover_name=\"feature\",\n",
    "        )\n",
    "    fig.update_layout(\n",
    "        scene = dict(\n",
    "            xaxis_title='log10 feature sparsity',\n",
    "            yaxis_title='D/E Projection',\n",
    "            zaxis_title='Encoder Bias',\n",
    "        )\n",
    "    )\n",
    "\n",
    "    fig.update_traces(marker_size = 2)\n",
    "    fig.update_layout(\n",
    "        title=\"Feature Sparsity vs D/E projection vs Encoder Bias (Excluding Outliers)\",\n",
    "        width=1400,\n",
    "        height=1300,\n",
    "    ) \n",
    "\n",
    "    # increase font size everywhere\n",
    "    fig.update_layout(\n",
    "        font=dict(\n",
    "            size=18,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # increase marker size in legend (only)\n",
    "    fig.update_layout(legend= {'itemsizing': 'constant'})\n",
    "    fig.update_scenes(aspectmode='cube')\n",
    "\n",
    "    return fig\n",
    "\n",
    "# fig = get_anthropic_3d_scatter(sae_df, remove_outliers_and_cluster=True)\n",
    "\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k_feature_inds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sae_df.iloc[top_k_feature_inds[0].flatten().tolist()].sort_values(\"log_feature_sparsity\").style.background_gradient(\n",
    "    subset=[\"d_e_projection_normalized\", \"d_e_projection\", \"log_feature_sparsity\", \"b_enc\"],\n",
    "    cmap=\"coolwarm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring NMONLI dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "\n",
    "\n",
    "\n",
    "def load_monli_data(path):\n",
    "    monli_train = []\n",
    "\n",
    "    with open(path, \"r\") as f:\n",
    "        for line in f:\n",
    "            monli_train.append(json.loads(line))\n",
    "            \n",
    "    monli_train = pd.DataFrame(monli_train)\n",
    "    monli_train[\"answer\"] = monli_train.depth.apply(lambda x: \" Yes\" if x != -1 else \" No\").astype(str)\n",
    "    return monli_train\n",
    "\n",
    "path = \"/Users/josephbloom/GithubRepositories/mats_sae_training/research/week_15th_jan/nmonli_train.jsonl\"\n",
    "monli_train = load_monli_data(path)\n",
    "path = \"/Users/josephbloom/GithubRepositories/mats_sae_training/research/week_15th_jan/nmonli_new.jsonl\"\n",
    "monli_new = load_monli_data(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREPROMPT = \"\"\"\n",
    "Statement: There is not a single person walking in the city.\n",
    "Hypothesis: There is not a single photographer walking in the city.\n",
    "We can infer with certainty that the hypothesis is true. Yes or No?\n",
    "Yes\n",
    "Statement: The people are not playing clarinets.\n",
    "Hypothesis: The people are not playing instruments.\n",
    "We can infer with certainty that the hypothesis is true. Yes or No?\n",
    "No\n",
    "Statement: The men were outside trying to keep their voices down so as not to waken any woman indoors\n",
    "Hypothesis: The men were outside trying to keep their voices down so as not to waken any wife indoors\n",
    "We can infer with certainty that the hypothesis is true. Yes or No?\n",
    "No\n",
    "\"\"\"\n",
    "no_token_id = model.tokenizer.encode(\"No\")[0]\n",
    "yes_token_id = model.tokenizer.encode(\"Yes\")[0]\n",
    "\n",
    "for i, row in tqdm(monli_train.sample(3).iterrows(), total=3):\n",
    "    # print(row[\"sentence1\"])\n",
    "    # print(row[\"sentence2\"])\n",
    "    print(\"----\")\n",
    "    prompt = f\"Statement: {row['sentence1']}\\nHypothesis: {row['sentence2']}\\nWe can infer with certainty that the hypothesis is true. Yes or No?\\n\"\n",
    "    print(prompt)\n",
    "    prompt = f\"{PREPROMPT}{prompt}\"\n",
    "    answer = row[\"answer\"]\n",
    "    print(\"Answer: \", answer)\n",
    "    # utils.test_prompt(prompt, answer, model, prepend_space_to_answer=False, top_k=3)\n",
    "    logits = model(prompt, return_type=\"logits\")[:,-1]\n",
    "    probs = torch.softmax(logits, dim=-1)\n",
    "    logits_yes = logits[:, yes_token_id].item()\n",
    "    logits_no = logits[:, no_token_id].item()\n",
    "    probs_yes = probs[:, yes_token_id].item()\n",
    "    probs_no = probs[:, no_token_id].item()\n",
    "    \n",
    "    print(f\"probs_yes: {probs_yes:.2f}, probs_no: {probs_no:.2f}, logits_yes: {logits_yes:.2f}, logits_no: {logits_no:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def nmonli_eval(monli_train, model, n_shots = 3):\n",
    "\n",
    "    no_token_id = model.tokenizer.encode(\" No\")[0]\n",
    "    yes_token_id = model.tokenizer.encode(\" Yes\")[0]\n",
    "\n",
    "    probs_yes_list = []\n",
    "    probs_no_list = []\n",
    "    logits_yes_list = []\n",
    "    logits_no_list = []\n",
    "    ce_loss_list = []\n",
    "    \n",
    "    # get premprompt\n",
    "    preprompt = []\n",
    "    for i, row in tqdm(monli_train.sample(n_shots).iterrows(), total=len(monli_train)):\n",
    "        prompt = f\"The statement \\\"{row['sentence1']}\\\", implies that \\\"{row['sentence2']}\\\". Yes or No?{row['answer']}\"\n",
    "        preprompt.append(prompt)\n",
    "    \n",
    "    preprompt = \"\\n\".join(preprompt)\n",
    "    print(\"Selected Preprompt:\\n{}\".format(preprompt))\n",
    "    print(\"------\")\n",
    "\n",
    "    for i, row in tqdm(monli_train.iterrows(), total=len(monli_train)):\n",
    "        # print(row[\"sentence1\"])\n",
    "        # print(row[\"sentence2\"])\n",
    "        # print(\"----\")\n",
    "        prompt = f\"The statement \\\"{row['sentence1']}\\\", implies that \\\"{row['sentence2']}\\\". Yes or No?\"\n",
    "        # print(prompt)\n",
    "        prompt = f\"{preprompt}\\n{prompt}\"\n",
    "        # prompt = f\"{prompt}\"\n",
    "        # print(prompt)\n",
    "        answer = row[\"answer\"]\n",
    "        # print(\"Answer: \", answer)\n",
    "        # utils.test_prompt(prompt, answer, model, prepend_space_to_answer=False, top_k=3)\n",
    "        logits = model(prompt, return_type=\"logits\")[:,-1]\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        logits_yes = logits[:, yes_token_id].item()\n",
    "        logits_no = logits[:, no_token_id].item()\n",
    "        probs_yes = probs[:, yes_token_id].item()\n",
    "        probs_no = probs[:, no_token_id].item()\n",
    "        \n",
    "        target = torch.zeros_like(logits)\n",
    "        target_token_id = yes_token_id if answer == \" Yes\" else no_token_id\n",
    "        target[:, target_token_id] = 1\n",
    "        ce_loss = torch.nn.CrossEntropyLoss()(logits.unsqueeze(0), target.unsqueeze(0)).item()\n",
    "        \n",
    "        probs_yes_list.append(probs_yes)\n",
    "        probs_no_list.append(probs_no)\n",
    "        logits_yes_list.append(logits_yes)\n",
    "        logits_no_list.append(logits_no)\n",
    "        ce_loss_list.append(ce_loss)\n",
    "        \n",
    "    monli_train[\"probs_yes\"] = probs_yes_list\n",
    "    monli_train[\"probs_no\"] = probs_no_list\n",
    "    monli_train[\"logits_yes\"] = logits_yes_list\n",
    "    monli_train[\"logits_no\"] = logits_no_list\n",
    "    monli_train[\"ce_loss\"] = ce_loss_list\n",
    "    monli_train[\"yes_minus_no\"] = monli_train[\"logits_yes\"] - monli_train[\"logits_no\"]\n",
    "    monli_train.head()\n",
    "\n",
    "    return monli_train\n",
    "\n",
    "results_df = nmonli_eval(monli_train, model)\n",
    "px.strip(results_df, orientation=\"v\", y=\"probs_yes\", color=\"answer\", title=\"Probability of Yes\", hover_data=[\"sentence1\", \"sentence2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.strip(results_df, orientation=\"v\", y=\"probs_yes\", color=\"answer\", title=\"Probability of Yes\", hover_data=[\"sentence1\", \"sentence2\"], height=800, width=800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.histogram(results_df, x=\"yes_minus_no\", color=\"answer\", title=\"Yes - No\", hover_data=[\"sentence1\", \"sentence2\"], height=800, width=800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.strip(monli_train, x=\"probs_yes\", color=\"answer\", title=\"Probability of Yes\", hover_data=[\"sentence1\", \"sentence2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download(\"words\")\n",
    "\n",
    "from nltk.corpus import words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_vocab = model.tokenizer.vocab_size\n",
    "vocab_df = pd.DataFrame(\n",
    "    {\n",
    "        \"token\": np.arange(d_vocab),\n",
    "        \"string\": model.to_str_tokens(np.arange(d_vocab)),\n",
    "    }\n",
    ")\n",
    "vocab_df[\"string_lower_stripped\"] = vocab_df.string.str.lower().str.strip()\n",
    "vocab_df[\"is_alpha\"] = vocab_df.string.str.match(r\"^( ?)[a-z]+$\")\n",
    "vocab_df[\"is_word\"] = vocab_df.string_lower_stripped.isin(words.words())\n",
    "vocab_df[\"is_fragment\"] = vocab_df.string.str.match(r\"^[a-z]+$\")\n",
    "vocab_df[\"has_space\"] = vocab_df.string.str.match(r\"^ [A-Za-z]+$\")\n",
    "vocab_df[\"num_chars\"] = vocab_df.string.apply(lambda n: len(n.strip()))\n",
    "vocab_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_df = vocab_df[vocab_df.is_word & vocab_df.has_space & (vocab_df.num_chars == 5)]\n",
    "word_df.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_single_prompt():\n",
    "    words = [word_df.string.sample().item().strip() for _ in range(3)]\n",
    "    print(words)\n",
    "    sorted_words = sorted(words)\n",
    "    return f\"Question: Put the following words in order: {', '.join(words)}\\nAnswer: {', '.join(sorted_words)}\"\n",
    "make_single_prompt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import circuitsvis as cv\n",
    "\n",
    "example_prompt = make_single_prompt()\n",
    "logits, cache = model.run_with_cache(example_prompt)\n",
    "display(cv.logits.token_log_probs(model.to_tokens(example_prompt), model(example_prompt)[0].log_softmax(dim=-1), model.to_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_single_prompt():\n",
    "    word = short_chars_vocab_df.string.sample().item().strip()\n",
    "    return f\" {word}:\" + \"\".join([f\" {c.upper()}\" for c in word.strip()])\n",
    "\n",
    "\n",
    "def make_kshot_prompt(k=3):\n",
    "    return \"\\n\".join([make_single_prompt() for _ in range(k)])\n",
    "\n",
    "\n",
    "def make_kshot_prompts(n=10, k=3):\n",
    "    return [make_kshot_prompt(k) for _ in range(n)]\n",
    "\n",
    "\n",
    "def get_answer_index(prompts):\n",
    "    batch_size = len(prompts)\n",
    "    answer_index = torch.zeros((batch_size, 5), device=\"cuda\", dtype=torch.int64) - 1\n",
    "    for i in range(batch_size):\n",
    "        for j in range(5):\n",
    "            answer_index[i, j] = alphabet.index(prompts[i][2 * j - 9].lower())\n",
    "    return answer_index\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variable Binding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "E_0 = \"Jack\"\n",
    "A_0 = \"London\"\n",
    "E_1 = \"Mary\"\n",
    "A_1 = \"Paris\"\n",
    "qn_subject = \"Jack\"\n",
    "prompt = f\"\"\"Answer the question based on the context below. Keep the answer short.\n",
    "Context: {E_0} lives in the capital city of {A_0}.\n",
    "{E_1} lives in the capital city of {A_1}.\n",
    "Question: Which city does {qn_subject} live in?\n",
    "Answer: {qn_subject} lives in the city of\"\"\"\n",
    "print(prompt)\n",
    "\n",
    "answer = A_0\n",
    "utils.test_prompt(prompt, answer, model, prepend_space_to_answer=True, top_k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "# capital_cities = [\n",
    "#     \"Washington\", \"Ottawa\", \"Mexico City\", \"London\", \"Paris\", \"Berlin\", \n",
    "#     \"Moscow\", \"Beijing\", \"Tokyo\", \"Canberra\", \"New Delhi\", \"Brasília\", \n",
    "#     \"Cairo\", \"Pretoria\", \"Rome\", \"Madrid\", \"Ankara\", \"Buenos Aires\", \n",
    "#     \"Riyadh\", \"Ottawa\", \"Stockholm\", \"Helsinki\", \"Oslo\", \"Copenhagen\", \n",
    "#     \"Wellington\", \"Seoul\", \"Bangkok\", \"Kuala Lumpur\", \"Jakarta\", \"Hanoi\"\n",
    "# ]\n",
    "american_cities = [\n",
    "    \"Washington\", \"New York\", \"Los Angeles\", \"Chicago\", \"Houston\", \"Phoenix\",\n",
    "    \"Philadelphia\", \"San Antonio\", \"San Diego\", \"Dallas\", \"San Jose\", \"Austin\",\n",
    "    \"Jacksonville\", \"Fort Worth\", \"Columbus\", \"Charlotte\", \"San Francisco\",\n",
    "    \"Indianapolis\", \"Seattle\", \"Denver\", \"Washington\", \"Boston\", \"El Paso\",\n",
    "    \"Nashville\", \"Detroit\", \"Oklahoma City\", \"Portland\", \"Las Vegas\", \"Memphis\",\n",
    "    \"Louisville\", \"Baltimore\", \"Milwaukee\", \"Albuquerque\", \"Tucson\", \"Fresno\",\n",
    "    \"Mesa\", \"Sacramento\", \"Atlanta\", \"Kansas City\", \"Colorado Springs\",\n",
    "    \"Omaha\", \"Raleigh\", \"Miami\", \"Long Beach\", \"Virginia Beach\", \"Oakland\",\n",
    "    \"Minneapolis\", \"Tulsa\", \"Tampa\", \"Arlington\", \"New Orleans\", \"Wichita\",\n",
    "    \"Cleveland\", \"Bakersfield\", \"Aurora\", \"Anaheim\", \"Honolulu\", \"Santa Ana\",\n",
    "    \"Riverside\", \"Corpus Christi\", \"Lexington\", \"Stockton\", \"St. Louis\",\n",
    "    \"Saint Paul\", \"Henderson\", \"Pittsburgh\", \"Cincinnati\", \"Anchorage\",\n",
    "    \"Greensboro\", \"Plano\", \"Newark\", \"Lincoln\", \"Orlando\", \"Irvine\",\n",
    "    \"Toledo\", \"Jersey City\", \"Chula Vista\", \"Durham\", \"Fort Wayne\",\n",
    "    \"St. Petersburg\", \"Laredo\", \"Buffalo\", \"Madison\", \"Lubbock\", \"Chandler\",\n",
    "    \"Scottsdale\", \"Reno\", \"Glendale\", \"Gilbert\", \"Winston–Salem\", \"North Las Vegas\",\n",
    "    \"Norfolk\", \"Chesapeake\", \"Garland\", \"Irving\", \"Hialeah\", \"Fremont\",\n",
    "    \"Boise\", \"Richmond\", \"Baton Rouge\", \"Spokane\", \"Des Moines\", \"Tacoma\",\n",
    "    \"San Bernardino\", \"Modesto\", \"Fontana\", \"Santa Clarita\", \"Birmingham\",\n",
    "    \"Oxnard\", \"Fayetteville\", \"Moreno Valley\", \"Rochester\", \"Glendale\",\n",
    "]\n",
    "    \n",
    "\n",
    "american_names = [\n",
    "    \"Emma\", \"Noah\", \"Olivia\", \"Liam\", \"Ava\", \"Ethan\", \"Sophia\", \"Mason\", \n",
    "    \"Isabella\", \"Logan\", \"Mia\", \"James\", \"Charlotte\", \"Benjamin\", \"Amelia\", \n",
    "    \"Jacob\", \"Harper\", \"Michael\", \"Evelyn\", \"Alexander\", \"Abigail\", \n",
    "    \"William\", \"Emily\", \"Jack\", \"Elizabeth\", \"Henry\", \"Madison\", \n",
    "    \"Aiden\", \"Chloe\", \"Matthew\"\n",
    "]\n",
    "\n",
    "names = [\n",
    "    \"Ana\", \"Lei\", \"Ivan\", \"Zoe\", \"Juan\", \"Eva\", \"Liam\", \"Raj\", \"Sara\", \n",
    "    \"Ken\", \"Maya\", \"Noah\", \"Aya\", \"Omar\", \"Lena\", \"Yuki\", \"Chen\", \n",
    "    \"Eli\", \"Ria\", \"Hugo\", \"Tara\", \"Kai\", \"Nina\", \"Leo\", \"Gita\", \n",
    "    \"Max\", \"Lila\", \"Sam\", \"Zara\", \"Tom\"\n",
    "]\n",
    "\n",
    "\n",
    "def generate_all_possible_variable_binding_promts(names, capital_cities, n_prompts=100):\n",
    "    prompts = []\n",
    "    A_0s = []\n",
    "    A_1s = []\n",
    "    E_0s = []\n",
    "    E_1s = []\n",
    "    qn_subjects = []\n",
    "    answers = []\n",
    "    decoys = []\n",
    "    \n",
    "    for _ in range(n_prompts):\n",
    "        A_0 = random.choice(capital_cities)\n",
    "        A_1 = random.choice(capital_cities)\n",
    "        E_0 = random.choice(names)\n",
    "        E_1 = random.choice(names)\n",
    "        qn_subject = E_0 if random.random() < 0.5 else E_1\n",
    "        answer = A_0 if qn_subject == E_0 else A_1\n",
    "        decoy = A_1 if qn_subject == E_0 else A_0\n",
    "        \n",
    "        prompt = f\"\"\"Answer the question based on the context below. Keep the answer short.\n",
    "Context: {E_0} lives in the capital city of {A_0}.\n",
    "{E_1} lives in the capital city of {A_1}.\n",
    "Question: Which city does {qn_subject} live in?\n",
    "Answer: {qn_subject} lives in the city of\"\"\"\n",
    "\n",
    "        A_0s.append(A_0)\n",
    "        A_1s.append(A_1)\n",
    "        E_0s.append(E_0)\n",
    "        E_1s.append(E_1)\n",
    "        qn_subjects.append(qn_subject)\n",
    "        prompts.append(prompt)\n",
    "        answers.append(answer)\n",
    "        decoys.append(decoy)\n",
    "        \n",
    "    df = pd.DataFrame({\n",
    "        \"A_0\": A_0s,\n",
    "        \"A_1\": A_1s,\n",
    "        \"E_0\": E_0s,\n",
    "        \"E_1\": E_1s,\n",
    "        \"qn_subject\": qn_subjects,\n",
    "        \"answer\": answers,\n",
    "        \"decoy\": decoys,\n",
    "        \"prompt\": prompts,\n",
    "    })\n",
    "        \n",
    "    return df\n",
    "\n",
    "df = generate_all_possible_variable_binding_promts(american_names, american_cities, n_prompts=100)\n",
    "df.head()\n",
    "\n",
    "df.iloc[0].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "results_list = []\n",
    "\n",
    "for i, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    # print(row[\"prompt\"])\n",
    "    # print(row[\"qn_subject\"])\n",
    "    # utils.test_prompt(row[\"prompt\"], row[\"answer\"], model, prepend_space_to_answer=True, top_k=3)\n",
    "    # print(\"-----\")\n",
    "    \n",
    "    logits = model(row[\"prompt\"], return_type=\"logits\")[:,-1]\n",
    "    probs = torch.softmax(logits.squeeze(), dim=-1)\n",
    "    \n",
    "    target_token_id = model.tokenizer.encode(\" \" + row[\"answer\"].strip())[0]\n",
    "    decoy_token_id = model.tokenizer.encode(\" \" + row[\"decoy\"].strip())[0]\n",
    "    top_unembed_token = model.tokenizer.decode(torch.topk(logits, 1).indices.item())\n",
    "    logits_target = logits[:, target_token_id].item()\n",
    "    probs_target = probs[target_token_id].item()\n",
    "    logits_decoy = logits[:, decoy_token_id].item()\n",
    "    probs_decoy = probs[decoy_token_id].item()\n",
    "    \n",
    "    target = torch.zeros_like(logits)\n",
    "    target[:, target_token_id] = 1\n",
    "    ce_loss = torch.nn.CrossEntropyLoss()(logits.unsqueeze(0), target.unsqueeze(0)).item()\n",
    "\n",
    "    \n",
    "    # print(f\"probs_target: {probs_target:.2f}, probs_decoy: {probs_decoy:.2f}, logits_target: {logits_target:.2f}, logits_decoy: {logits_decoy:.2f}\")\n",
    "\n",
    "    \n",
    "    results_list.append({\n",
    "        \"E_0\": row[\"E_0\"],\n",
    "        \"A_0\": row[\"A_0\"],\n",
    "        \"E_1\": row[\"E_1\"],\n",
    "        \"A_1\": row[\"A_1\"],\n",
    "        \"qn_subject\": row[\"qn_subject\"],\n",
    "        \"answer\": row[\"answer\"],\n",
    "        \"prompt\": row[\"prompt\"],\n",
    "        \"top_unembed_token\": top_unembed_token,\n",
    "        \"target_token_id_string\": model.tokenizer.decode(target_token_id),\n",
    "        \"decoy_token_id_string\": model.tokenizer.decode(decoy_token_id),\n",
    "        \"probs_target\": probs_target,\n",
    "        \"probs_decoy\": probs_decoy,\n",
    "        \"logits_target\": logits_target,\n",
    "        \"logits_decoy\": logits_decoy,\n",
    "    })\n",
    "    \n",
    "results_df = pd.DataFrame(results_list)\n",
    "results_df[\"logit_diff\"] = results_df[\"logits_target\"] - results_df[\"logits_decoy\"]\n",
    "# results_df\n",
    "fraction_correct = (results_df.logit_diff > 0).mean()\n",
    "print(f\"Fraction correct: {fraction_correct:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.strip(\n",
    "    results_df,\n",
    "    y= \"logit_diff\",\n",
    "    # barmode=\"group\",\n",
    "    hover_data=[\"prompt\", \"qn_subject\", \"answer\", \"top_unembed_token\", \"target_token_id_string\", \"decoy_token_id_string\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"Up: Down, Top: Bottom, Left:\"\"\"\n",
    "answer = \" Right\"\n",
    "utils.test_prompt(prompt, answer, model, prepend_space_to_answer=True, top_k=3)\n",
    "\n",
    "prompt = f\"\"\"Happy: Sad, Good: Bad, Hot: Cold, Up:\"\"\"\n",
    "answer = \" Down\"\n",
    "utils.test_prompt(prompt, answer, model, prepend_space_to_answer=True, top_k=3)\n",
    "\n",
    "prompt = f\"\"\"Fire: Ice, Hot: Cold, Up:\"\"\"\n",
    "answer = \" Down\"\n",
    "utils.test_prompt(prompt, answer, model, prepend_space_to_answer=True, top_k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"Up: U, Top: T, Left: L, Right:\"\"\"\n",
    "answer = \" R\"\n",
    "utils.test_prompt(prompt, answer, model, prepend_space_to_answer=True, top_k=3)\n",
    "\n",
    "prompt = f\"\"\"Cat: C, Dog: D, Mouse: M, Elephant:\"\"\"\n",
    "answer = \" E\"\n",
    "utils.test_prompt(prompt, answer, model, prepend_space_to_answer=True, top_k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\" A B C D E F G H I J K L M N O P Q R S T U V W X Y\"\"\"\n",
    "answer = \" Z\"\n",
    "utils.test_prompt(prompt, answer, model, prepend_space_to_answer=True, top_k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"English: I love you. French:\"\n",
    "utils.test_prompt(prompt, \" Je\", model, prepend_space_to_answer=True, top_k=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mats_sae_training",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
