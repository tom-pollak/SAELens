{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dev Session:\n",
    "\n",
    "Goals:\n",
    "- Validate that the tokenizer / batching is done correctly. \n",
    "- Speed it up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os \n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"WANDB__SERVICE_WAIT\"] = \"300\"\n",
    "\n",
    "from sae_training.config import LanguageModelSAERunnerConfig\n",
    "from sae_training.lm_runner import language_model_sae_runner\n",
    "\n",
    "cfg = LanguageModelSAERunnerConfig(\n",
    "\n",
    "    # Data Generating Function (Model + Training Distibuion)\n",
    "    model_name = \"gpt2-small\",\n",
    "    hook_point = \"blocks.10.hook_resid_pre\",\n",
    "    hook_point_layer = 10,\n",
    "    d_in = 768,\n",
    "    dataset_path = \"Skylion007/openwebtext\",\n",
    "    is_dataset_tokenized=False,\n",
    "    \n",
    "    # SAE Parameters\n",
    "    expansion_factor = 64, # determines the dimension of the SAE.\n",
    "    \n",
    "    # Training Parameters\n",
    "    lr = 1e-4,\n",
    "    l1_coefficient = 5e-3,\n",
    "    lr_scheduler_name=None,\n",
    "    train_batch_size = 4096,\n",
    "    context_size = 128,\n",
    "    \n",
    "    # Activation Store Parameters\n",
    "    n_batches_in_buffer = 128,\n",
    "    total_training_tokens = 1_000_000 * 3, # 200M tokens seems doable overnight.\n",
    "    store_batch_size = 32,\n",
    "    \n",
    "    # Resampling protocol\n",
    "    feature_sampling_method = 'l2',\n",
    "    feature_sampling_window = 100,\n",
    "    feature_reinit_scale = 0.2,\n",
    "    dead_feature_window=5000,\n",
    "    dead_feature_threshold = 1e-7,\n",
    "    \n",
    "    # WANDB\n",
    "    log_to_wandb = True,\n",
    "    wandb_project= \"mats_sae_training_gpt2_small\",\n",
    "    wandb_entity = None,\n",
    "    wandb_log_frequency=20,\n",
    "    \n",
    "    # Misc\n",
    "    device = \"mps\",\n",
    "    seed = 42,\n",
    "    n_checkpoints = 0,\n",
    "    checkpoint_path = \"checkpoints\",\n",
    "    dtype = torch.float32,\n",
    "    )\n",
    "\n",
    "sparse_autoencoder = language_model_sae_runner(cfg)\n",
    "\n",
    "import cProfile, pstats, io\n",
    "from pstats import SortKey\n",
    "pr = cProfile.Profile()\n",
    "pr.enable()\n",
    "\n",
    "\n",
    "# sparse_autoencoder = language_model_sae_runner(cfg)\n",
    "pr.disable()\n",
    "s = io.StringIO()\n",
    "sortby = SortKey.CUMULATIVE\n",
    "ps = pstats.Stats(pr, stream=s).sort_stats(sortby)\n",
    "ps.print_stats()\n",
    "print(s.getvalue())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from mats_sae_training.activation_store import ActivationStore\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "\n",
    "import os \n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "def get_batch_tokens(iterable_dataset, cfg, model):\n",
    "        \"\"\"\n",
    "        Streams a batch of tokens from a dataset.\n",
    "        \"\"\"\n",
    "\n",
    "        batch_size = cfg.store_batch_size\n",
    "        context_size = cfg.context_size\n",
    "        device = cfg.device\n",
    "\n",
    "        batch_tokens = torch.LongTensor(size=(0, context_size)).to(device)\n",
    "\n",
    "        current_batch = []\n",
    "        current_length = 0\n",
    "\n",
    "        pbar = tqdm(total=batch_size, desc=\"Filling batches\")\n",
    "        while batch_tokens.shape[0] < batch_size:\n",
    "            if not cfg.is_dataset_tokenized:\n",
    "                s = next(iterable_dataset)[\"text\"]\n",
    "                tokens = model.to_tokens(s, truncate=False, move_to_device=True).squeeze(0)\n",
    "                assert len(tokens.shape) == 1, f\"tokens.shape should be 1D but was {tokens.shape}\"\n",
    "            else:\n",
    "                tokens = torch.tensor(\n",
    "                    next(iterable_dataset)[\"tokens\"],\n",
    "                    dtype=torch.long,\n",
    "                    device=device,\n",
    "                )\n",
    "            token_len = tokens.shape[0]\n",
    "\n",
    "            while token_len > 0:\n",
    "                # Space left in the current batch\n",
    "                space_left = context_size - current_length\n",
    "\n",
    "                # If the current tokens fit entirely into the remaining space\n",
    "                if token_len <= space_left:\n",
    "                    current_batch.append(tokens[:token_len])\n",
    "                    current_length += token_len\n",
    "                    break\n",
    "\n",
    "                else:\n",
    "                    # Take as much as will fit\n",
    "                    current_batch.append(tokens[:space_left])\n",
    "\n",
    "                    # Remove used part, add BOS\n",
    "                    tokens = tokens[space_left:]\n",
    "                    tokens = torch.cat(\n",
    "                        (\n",
    "                            torch.LongTensor([model.tokenizer.bos_token_id]).to(\n",
    "                                tokens.device\n",
    "                            ),\n",
    "                            tokens,\n",
    "                        ),\n",
    "                        dim=0,\n",
    "                    )\n",
    "\n",
    "                    token_len -= space_left\n",
    "                    token_len += 1\n",
    "                    current_length = context_size\n",
    "\n",
    "                # If a batch is full, concatenate and move to next batch\n",
    "                if current_length == context_size:\n",
    "                    full_batch = torch.cat(current_batch, dim=0)\n",
    "                    batch_tokens = torch.cat(\n",
    "                        (batch_tokens, full_batch.unsqueeze(0)), dim=0\n",
    "                    )\n",
    "                    current_batch = []\n",
    "                    current_length = 0\n",
    "\n",
    "            pbar.n = batch_tokens.shape[0]\n",
    "            pbar.refresh()\n",
    "\n",
    "        return batch_tokens[:batch_size]\n",
    "    \n",
    "    \n",
    "data_path = \"EleutherAI/the_pile_deduplicated\"\n",
    "dataset = load_dataset(data_path, split=\"train\", streaming=True)\n",
    "\n",
    "\n",
    "\n",
    "import cProfile, pstats, io\n",
    "from pstats import SortKey\n",
    "pr = cProfile.Profile()\n",
    "pr.enable()\n",
    "# ... do something ...\n",
    "for i in range(3):\n",
    "    batch_tokens = get_batch_tokens(iter(dataset), cfg, model)\n",
    "pr.disable()\n",
    "s = io.StringIO()\n",
    "sortby = SortKey.CUMULATIVE\n",
    "ps = pstats.Stats(pr, stream=s).sort_stats(sortby)\n",
    "ps.print_stats()\n",
    "print(s.getvalue())\n",
    "print(batch_tokens.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To do: preprocess_tokenized_dataset, preprocess_text_dataset, preprocess other dataset\n",
    "def preprocess_tokenized_dataset(source_batch: dict, context_size: int) -> dict:\n",
    "    tokenized_prompts = source_batch[\"tokens\"]\n",
    "\n",
    "    # Chunk each tokenized prompt into blocks of context_size,\n",
    "    # discarding the last block if too small.\n",
    "    context_size_prompts = []\n",
    "    for encoding in tokenized_prompts:\n",
    "        chunks = [\n",
    "            encoding[i : i + context_size]\n",
    "            for i in range(0, len(encoding), context_size)\n",
    "            if len(encoding[i : i + context_size]) == context_size\n",
    "        ]\n",
    "        context_size_prompts.extend(chunks)\n",
    "\n",
    "    return {\"input_ids\": context_size_prompts}\n",
    "\n",
    "\n",
    "def preprocess_text_data(source_batch: dict, context_size: int) -> dict:\n",
    "    prompts = source_batch[\"text\"]\n",
    "        \n",
    "\n",
    "    texts = self.text_batch(batch_size=batch_size)\n",
    "    return model.tokenizer(\n",
    "        texts,\n",
    "        return_tensors='pt',\n",
    "        max_length=self.ctx_len,\n",
    "        padding=True,\n",
    "        truncation=True\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "def get_mapped_dataset(cfg):\n",
    "    # Load the dataset\n",
    "    context_size = cfg.context_size\n",
    "    dataset_path = cfg.dataset_path\n",
    "    dataset_split = \"train\"\n",
    "    buffer_size: int = 1024\n",
    "    preprocess_batch_size: int = 1024\n",
    "\n",
    "    dataset = load_dataset(dataset_path, streaming=True, split=dataset_split)  # type: ignore\n",
    "    # ids = dataset.to_iterable_dataset() # try out shards here\n",
    "    # ids = ids.filter(filter_fn).map(process_fn) \n",
    "    \n",
    "    # Setup preprocessing\n",
    "    existing_columns = list(next(iter(dataset)).keys())\n",
    "    mapped_dataset = dataset.map(\n",
    "        preprocess_tokenized_dataset, # preprocess is what differentiates different datasets\n",
    "        batched=True,\n",
    "        batch_size=preprocess_batch_size,\n",
    "        fn_kwargs={\"context_size\": context_size},\n",
    "        remove_columns=existing_columns,\n",
    "    )\n",
    "\n",
    "    # Setup approximate shuffling. As the dataset is streamed, this just pre-downloads at least\n",
    "    # `buffer_size` items and then shuffles just that buffer.\n",
    "    # https://huggingface.co/docs/datasets/v2.14.5/stream#shuffle\n",
    "    dataset = mapped_dataset.shuffle(buffer_size=buffer_size)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "data_path = \"EleutherAI/the_pile_deduplicated\"\n",
    "dataset = load_dataset(data_path, split=\"train\", streaming=True)\n",
    "dataset = get_mapped_dataset(cfg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_batch(self, batch_size=None):\n",
    "    \"\"\"\n",
    "    Return a list of text\n",
    "    \"\"\"\n",
    "    if batch_size is None:\n",
    "        batch_size = self.in_batch_size\n",
    "    return [\n",
    "        next(self.data) for _ in range(batch_size)\n",
    "    ]\n",
    "\n",
    "\n",
    "def tokenized_batch(self, batch_size=None):\n",
    "    \"\"\"\n",
    "    Return a batch of tokenized inputs.\n",
    "    \"\"\"\n",
    "    texts = self.text_batch(batch_size=batch_size)\n",
    "    return self.model.tokenizer(\n",
    "        texts,\n",
    "        return_tensors='pt',\n",
    "        max_length=self.ctx_len,\n",
    "        padding=True,\n",
    "        truncation=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Dict, List\n",
    "import einops\n",
    "from datasets import load_dataset\n",
    "\n",
    "column_name = \"text\"\n",
    "add_bos_token = True\n",
    "seq_len = 128\n",
    "tokenizer = model.tokenizer\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "def tokenize_function(examples: Dict[str, List[str]]) -> Dict[str, np.ndarray]:\n",
    "    text = examples[column_name]\n",
    "    # Concatenate it all into an enormous string, separated by eos_tokens\n",
    "    full_text = tokenizer.eos_token.join(text)\n",
    "    # Divide into 20 chunks of ~ equal length\n",
    "    num_chunks = 20\n",
    "    chunk_length = (len(full_text) - 1) // num_chunks + 1\n",
    "    chunks = [\n",
    "        full_text[i * chunk_length : (i + 1) * chunk_length]\n",
    "        for i in range(num_chunks)\n",
    "    ]\n",
    "    # Tokenize the chunks in parallel. Uses NumPy because HuggingFace map doesn't want tensors returned\n",
    "    tokens = tokenizer(chunks, return_tensors=\"np\", padding=True)[\n",
    "        \"input_ids\"\n",
    "    ].flatten()\n",
    "    # Drop padding tokens\n",
    "    tokens = tokens[tokens != tokenizer.pad_token_id]\n",
    "    num_tokens = len(tokens)\n",
    "    num_batches = num_tokens // (seq_len)\n",
    "    # Drop the final tokens if not enough to make a full sequence\n",
    "    tokens = tokens[: seq_len * num_batches]\n",
    "    tokens = einops.rearrange(\n",
    "        tokens, \"(batch seq) -> batch seq\", batch=num_batches, seq=seq_len\n",
    "    )\n",
    "    if add_bos_token:\n",
    "        prefix = np.full((num_batches, 1), tokenizer.bos_token_id)\n",
    "        tokens = np.concatenate([prefix, tokens], axis=1)\n",
    "    return {\"tokens\": tokens}\n",
    "\n",
    "data_path = \"EleutherAI/the_pile_deduplicated\"\n",
    "dataset = load_dataset(data_path, split=\"train\", streaming=True)\n",
    "\n",
    "tokenized_dataset = dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=[column_name],\n",
    ")\n",
    "buffer_size = 1024\n",
    "dataset = tokenized_dataset.shuffle(buffer_size=buffer_size)\n",
    "# tokenized_dataset.set_format(type=\"torch\", columns=[\"tokens\"])\n",
    "batch_tokens = next(iter(dataset))\n",
    "batch_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_tokens = torch.stack(\n",
    "    [next(iter(dataset))[\"tokens\"] for _ in range(32)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset.set_format(type=\"torch\", columns=[\"tokens\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to_string(batch_tokens['tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    dataset, \n",
    "    batch_size=32, \n",
    "    num_workers=0, \n",
    "    collate_fn= torch.stack)\n",
    "\n",
    "batch_tokens = next(iter(dataloader))\n",
    "batch_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_tokens['tokens']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mats_sae_training",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
