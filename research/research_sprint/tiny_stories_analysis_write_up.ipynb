{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tiny Stories Analysis Write Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/josephbloom/miniforge3/envs/mats_sae_training/lib/python3.11/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/Users/josephbloom/miniforge3/envs/mats_sae_training/lib/python3.11/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/Users/josephbloom/miniforge3/envs/mats_sae_training/lib/python3.11/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x1283a3f10>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys \n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from importlib import reload\n",
    "\n",
    "import joseph\n",
    "from joseph.analysis import *\n",
    "from joseph.visualisation import *\n",
    "from joseph.utils import *\n",
    "from joseph.data import *\n",
    "\n",
    "\n",
    "reload(joseph.analysis)\n",
    "reload(joseph.visualisation)\n",
    "reload(joseph.utils)\n",
    "reload(joseph.data)\n",
    "\n",
    "from joseph.analysis import *\n",
    "from joseph.visualisation import *\n",
    "from joseph.utils import *\n",
    "from joseph.data import *\n",
    "\n",
    "# turn torch grad tracking off\n",
    "torch.set_grad_enabled(False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/josephbloom/GithubRepositories/mats_sae_training/research/tiny_stories_analysis_write_up.ipynb Cell 4\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/josephbloom/GithubRepositories/mats_sae_training/research/tiny_stories_analysis_write_up.ipynb#X21sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model \u001b[39m=\u001b[39m HookedTransformer\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/josephbloom/GithubRepositories/mats_sae_training/research/tiny_stories_analysis_write_up.ipynb#X21sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39mtiny-stories-2L-33M\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/josephbloom/GithubRepositories/mats_sae_training/research/tiny_stories_analysis_write_up.ipynb#X21sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39m# center_unembed=True,\u001b[39;49;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/josephbloom/GithubRepositories/mats_sae_training/research/tiny_stories_analysis_write_up.ipynb#X21sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39m# center_writing_weights=True,\u001b[39;49;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/josephbloom/GithubRepositories/mats_sae_training/research/tiny_stories_analysis_write_up.ipynb#X21sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39m# fold_ln=True,\u001b[39;49;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/josephbloom/GithubRepositories/mats_sae_training/research/tiny_stories_analysis_write_up.ipynb#X21sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39m# refactor_factored_attn_matrices=True,\u001b[39;49;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/josephbloom/GithubRepositories/mats_sae_training/research/tiny_stories_analysis_write_up.ipynb#X21sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m )\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/josephbloom/GithubRepositories/mats_sae_training/research/tiny_stories_analysis_write_up.ipynb#X21sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# model.set_use_split_qkv_input(True)\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/josephbloom/GithubRepositories/mats_sae_training/research/tiny_stories_analysis_write_up.ipynb#X21sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# model.set_use_attn_result(True)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/josephbloom/GithubRepositories/mats_sae_training/research/tiny_stories_analysis_write_up.ipynb#X21sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtransformer_lens\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mevals\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mevals\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/mats_sae_training/lib/python3.11/site-packages/transformer_lens/HookedTransformer.py:1282\u001b[0m, in \u001b[0;36mHookedTransformer.from_pretrained\u001b[0;34m(cls, model_name, fold_ln, center_writing_weights, center_unembed, refactor_factored_attn_matrices, checkpoint_index, checkpoint_value, hf_model, device, n_devices, tokenizer, move_to_device, fold_value_biases, default_prepend_bos, default_padding_side, dtype, **from_pretrained_kwargs)\u001b[0m\n\u001b[1;32m   1278\u001b[0m         center_writing_weights \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m   1280\u001b[0m \u001b[39m# Get the state dict of the model (ie a mapping of parameter names to tensors), processed to\u001b[39;00m\n\u001b[1;32m   1281\u001b[0m \u001b[39m# match the HookedTransformer parameter names.\u001b[39;00m\n\u001b[0;32m-> 1282\u001b[0m state_dict \u001b[39m=\u001b[39m loading\u001b[39m.\u001b[39;49mget_pretrained_state_dict(\n\u001b[1;32m   1283\u001b[0m     official_model_name, cfg, hf_model, dtype\u001b[39m=\u001b[39;49mdtype, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfrom_pretrained_kwargs\n\u001b[1;32m   1284\u001b[0m )\n\u001b[1;32m   1286\u001b[0m \u001b[39m# Create the HookedTransformer object\u001b[39;00m\n\u001b[1;32m   1287\u001b[0m model \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m(\n\u001b[1;32m   1288\u001b[0m     cfg,\n\u001b[1;32m   1289\u001b[0m     tokenizer,\n\u001b[1;32m   1290\u001b[0m     move_to_device\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m   1291\u001b[0m     default_padding_side\u001b[39m=\u001b[39mdefault_padding_side,\n\u001b[1;32m   1292\u001b[0m )\n",
      "File \u001b[0;32m~/miniforge3/envs/mats_sae_training/lib/python3.11/site-packages/transformer_lens/loading_from_pretrained.py:1067\u001b[0m, in \u001b[0;36mget_pretrained_state_dict\u001b[0;34m(official_model_name, cfg, hf_model, dtype, **kwargs)\u001b[0m\n\u001b[1;32m   1063\u001b[0m         hf_model \u001b[39m=\u001b[39m BertForPreTraining\u001b[39m.\u001b[39mfrom_pretrained(\n\u001b[1;32m   1064\u001b[0m             official_model_name, torch_dtype\u001b[39m=\u001b[39mdtype, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[1;32m   1065\u001b[0m         )\n\u001b[1;32m   1066\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1067\u001b[0m         hf_model \u001b[39m=\u001b[39m AutoModelForCausalLM\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[1;32m   1068\u001b[0m             official_model_name, torch_dtype\u001b[39m=\u001b[39;49mdtype, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m   1069\u001b[0m         )\n\u001b[1;32m   1071\u001b[0m     \u001b[39m# Load model weights, and fold in layer norm weights\u001b[39;00m\n\u001b[1;32m   1073\u001b[0m \u001b[39mfor\u001b[39;00m param \u001b[39min\u001b[39;00m hf_model\u001b[39m.\u001b[39mparameters():\n",
      "File \u001b[0;32m~/miniforge3/envs/mats_sae_training/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:488\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    485\u001b[0m \u001b[39mif\u001b[39;00m commit_hash \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    486\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(config, PretrainedConfig):\n\u001b[1;32m    487\u001b[0m         \u001b[39m# We make a call to the config file first (which may be absent) to get the commit hash as soon as possible\u001b[39;00m\n\u001b[0;32m--> 488\u001b[0m         resolved_config_file \u001b[39m=\u001b[39m cached_file(\n\u001b[1;32m    489\u001b[0m             pretrained_model_name_or_path,\n\u001b[1;32m    490\u001b[0m             CONFIG_NAME,\n\u001b[1;32m    491\u001b[0m             _raise_exceptions_for_missing_entries\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    492\u001b[0m             _raise_exceptions_for_connection_errors\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    493\u001b[0m             \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mhub_kwargs,\n\u001b[1;32m    494\u001b[0m         )\n\u001b[1;32m    495\u001b[0m         commit_hash \u001b[39m=\u001b[39m extract_commit_hash(resolved_config_file, commit_hash)\n\u001b[1;32m    496\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniforge3/envs/mats_sae_training/lib/python3.11/site-packages/transformers/utils/hub.py:430\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    427\u001b[0m user_agent \u001b[39m=\u001b[39m http_user_agent(user_agent)\n\u001b[1;32m    428\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    429\u001b[0m     \u001b[39m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 430\u001b[0m     resolved_file \u001b[39m=\u001b[39m hf_hub_download(\n\u001b[1;32m    431\u001b[0m         path_or_repo_id,\n\u001b[1;32m    432\u001b[0m         filename,\n\u001b[1;32m    433\u001b[0m         subfolder\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m \u001b[39mif\u001b[39;49;00m \u001b[39mlen\u001b[39;49m(subfolder) \u001b[39m==\u001b[39;49m \u001b[39m0\u001b[39;49m \u001b[39melse\u001b[39;49;00m subfolder,\n\u001b[1;32m    434\u001b[0m         repo_type\u001b[39m=\u001b[39;49mrepo_type,\n\u001b[1;32m    435\u001b[0m         revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m    436\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m    437\u001b[0m         user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[1;32m    438\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[1;32m    439\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m    440\u001b[0m         resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[1;32m    441\u001b[0m         token\u001b[39m=\u001b[39;49mtoken,\n\u001b[1;32m    442\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m    443\u001b[0m     )\n\u001b[1;32m    444\u001b[0m \u001b[39mexcept\u001b[39;00m GatedRepoError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    445\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    446\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mYou are trying to access a gated repo.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mMake sure to request access at \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    447\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mhttps://huggingface.co/\u001b[39m\u001b[39m{\u001b[39;00mpath_or_repo_id\u001b[39m}\u001b[39;00m\u001b[39m and pass a token having permission to this repo either \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    448\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mby logging in with `huggingface-cli login` or by passing `token=<your_token>`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    449\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/mats_sae_training/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[39mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    116\u001b[0m     kwargs \u001b[39m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[39m=\u001b[39mfn\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, has_token\u001b[39m=\u001b[39mhas_token, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[0;32m--> 118\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniforge3/envs/mats_sae_training/lib/python3.11/site-packages/huggingface_hub/file_download.py:1247\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout, endpoint)\u001b[0m\n\u001b[1;32m   1245\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1246\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1247\u001b[0m         metadata \u001b[39m=\u001b[39m get_hf_file_metadata(\n\u001b[1;32m   1248\u001b[0m             url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m   1249\u001b[0m             token\u001b[39m=\u001b[39;49mtoken,\n\u001b[1;32m   1250\u001b[0m             proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m   1251\u001b[0m             timeout\u001b[39m=\u001b[39;49metag_timeout,\n\u001b[1;32m   1252\u001b[0m         )\n\u001b[1;32m   1253\u001b[0m     \u001b[39mexcept\u001b[39;00m EntryNotFoundError \u001b[39mas\u001b[39;00m http_error:\n\u001b[1;32m   1254\u001b[0m         \u001b[39m# Cache the non-existence of the file and raise\u001b[39;00m\n\u001b[1;32m   1255\u001b[0m         commit_hash \u001b[39m=\u001b[39m http_error\u001b[39m.\u001b[39mresponse\u001b[39m.\u001b[39mheaders\u001b[39m.\u001b[39mget(HUGGINGFACE_HEADER_X_REPO_COMMIT)\n",
      "File \u001b[0;32m~/miniforge3/envs/mats_sae_training/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[39mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    116\u001b[0m     kwargs \u001b[39m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[39m=\u001b[39mfn\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, has_token\u001b[39m=\u001b[39mhas_token, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[0;32m--> 118\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniforge3/envs/mats_sae_training/lib/python3.11/site-packages/huggingface_hub/file_download.py:1624\u001b[0m, in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout)\u001b[0m\n\u001b[1;32m   1621\u001b[0m headers[\u001b[39m\"\u001b[39m\u001b[39mAccept-Encoding\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39midentity\u001b[39m\u001b[39m\"\u001b[39m  \u001b[39m# prevent any compression => we want to know the real size of the file\u001b[39;00m\n\u001b[1;32m   1623\u001b[0m \u001b[39m# Retrieve metadata\u001b[39;00m\n\u001b[0;32m-> 1624\u001b[0m r \u001b[39m=\u001b[39m _request_wrapper(\n\u001b[1;32m   1625\u001b[0m     method\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mHEAD\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m   1626\u001b[0m     url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m   1627\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m   1628\u001b[0m     allow_redirects\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m   1629\u001b[0m     follow_relative_redirects\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   1630\u001b[0m     proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m   1631\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m   1632\u001b[0m )\n\u001b[1;32m   1633\u001b[0m hf_raise_for_status(r)\n\u001b[1;32m   1635\u001b[0m \u001b[39m# Return\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/mats_sae_training/lib/python3.11/site-packages/huggingface_hub/file_download.py:402\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    400\u001b[0m \u001b[39m# 2. Force relative redirection\u001b[39;00m\n\u001b[1;32m    401\u001b[0m \u001b[39mif\u001b[39;00m follow_relative_redirects:\n\u001b[0;32m--> 402\u001b[0m     response \u001b[39m=\u001b[39m _request_wrapper(\n\u001b[1;32m    403\u001b[0m         method\u001b[39m=\u001b[39;49mmethod,\n\u001b[1;32m    404\u001b[0m         url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m    405\u001b[0m         follow_relative_redirects\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    406\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mparams,\n\u001b[1;32m    407\u001b[0m     )\n\u001b[1;32m    409\u001b[0m     \u001b[39m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[1;32m    410\u001b[0m     \u001b[39m# This is useful in case of a renamed repository.\u001b[39;00m\n\u001b[1;32m    411\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m300\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m response\u001b[39m.\u001b[39mstatus_code \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m399\u001b[39m:\n",
      "File \u001b[0;32m~/miniforge3/envs/mats_sae_training/lib/python3.11/site-packages/huggingface_hub/file_download.py:425\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[39mreturn\u001b[39;00m response\n\u001b[1;32m    424\u001b[0m \u001b[39m# Perform request and return if status_code is not in the retry list.\u001b[39;00m\n\u001b[0;32m--> 425\u001b[0m response \u001b[39m=\u001b[39m get_session()\u001b[39m.\u001b[39;49mrequest(method\u001b[39m=\u001b[39;49mmethod, url\u001b[39m=\u001b[39;49murl, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mparams)\n\u001b[1;32m    426\u001b[0m hf_raise_for_status(response)\n\u001b[1;32m    427\u001b[0m \u001b[39mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/miniforge3/envs/mats_sae_training/lib/python3.11/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[39m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(prep, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49msend_kwargs)\n\u001b[1;32m    591\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/miniforge3/envs/mats_sae_training/lib/python3.11/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[39m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[39m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[39m=\u001b[39m adapter\u001b[39m.\u001b[39;49msend(request, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    705\u001b[0m \u001b[39m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[39m=\u001b[39m preferred_clock() \u001b[39m-\u001b[39m start\n",
      "File \u001b[0;32m~/miniforge3/envs/mats_sae_training/lib/python3.11/site-packages/huggingface_hub/utils/_http.py:63\u001b[0m, in \u001b[0;36mUniqueRequestIdAdapter.send\u001b[0;34m(self, request, *args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Catch any RequestException to append request id to the error message for debugging.\"\"\"\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 63\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49msend(request, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     64\u001b[0m \u001b[39mexcept\u001b[39;00m requests\u001b[39m.\u001b[39mRequestException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     65\u001b[0m     request_id \u001b[39m=\u001b[39m request\u001b[39m.\u001b[39mheaders\u001b[39m.\u001b[39mget(X_AMZN_TRACE_ID)\n",
      "File \u001b[0;32m~/miniforge3/envs/mats_sae_training/lib/python3.11/site-packages/requests/adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    483\u001b[0m     timeout \u001b[39m=\u001b[39m TimeoutSauce(connect\u001b[39m=\u001b[39mtimeout, read\u001b[39m=\u001b[39mtimeout)\n\u001b[1;32m    485\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 486\u001b[0m     resp \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49murlopen(\n\u001b[1;32m    487\u001b[0m         method\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mmethod,\n\u001b[1;32m    488\u001b[0m         url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m    489\u001b[0m         body\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mbody,\n\u001b[1;32m    490\u001b[0m         headers\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    491\u001b[0m         redirect\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    492\u001b[0m         assert_same_host\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    493\u001b[0m         preload_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    494\u001b[0m         decode_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    495\u001b[0m         retries\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_retries,\n\u001b[1;32m    496\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    497\u001b[0m         chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    498\u001b[0m     )\n\u001b[1;32m    500\u001b[0m \u001b[39mexcept\u001b[39;00m (ProtocolError, \u001b[39mOSError\u001b[39;00m) \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m    501\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m(err, request\u001b[39m=\u001b[39mrequest)\n",
      "File \u001b[0;32m~/miniforge3/envs/mats_sae_training/lib/python3.11/site-packages/urllib3/connectionpool.py:790\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    787\u001b[0m response_conn \u001b[39m=\u001b[39m conn \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m release_conn \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    789\u001b[0m \u001b[39m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 790\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n\u001b[1;32m    791\u001b[0m     conn,\n\u001b[1;32m    792\u001b[0m     method,\n\u001b[1;32m    793\u001b[0m     url,\n\u001b[1;32m    794\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout_obj,\n\u001b[1;32m    795\u001b[0m     body\u001b[39m=\u001b[39;49mbody,\n\u001b[1;32m    796\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    797\u001b[0m     chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    798\u001b[0m     retries\u001b[39m=\u001b[39;49mretries,\n\u001b[1;32m    799\u001b[0m     response_conn\u001b[39m=\u001b[39;49mresponse_conn,\n\u001b[1;32m    800\u001b[0m     preload_content\u001b[39m=\u001b[39;49mpreload_content,\n\u001b[1;32m    801\u001b[0m     decode_content\u001b[39m=\u001b[39;49mdecode_content,\n\u001b[1;32m    802\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mresponse_kw,\n\u001b[1;32m    803\u001b[0m )\n\u001b[1;32m    805\u001b[0m \u001b[39m# Everything went great!\u001b[39;00m\n\u001b[1;32m    806\u001b[0m clean_exit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/mats_sae_training/lib/python3.11/site-packages/urllib3/connectionpool.py:467\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    464\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    465\u001b[0m     \u001b[39m# Trigger any extra validation we need to do.\u001b[39;00m\n\u001b[1;32m    466\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 467\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_conn(conn)\n\u001b[1;32m    468\u001b[0m     \u001b[39mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    469\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_raise_timeout(err\u001b[39m=\u001b[39me, url\u001b[39m=\u001b[39murl, timeout_value\u001b[39m=\u001b[39mconn\u001b[39m.\u001b[39mtimeout)\n",
      "File \u001b[0;32m~/miniforge3/envs/mats_sae_training/lib/python3.11/site-packages/urllib3/connectionpool.py:1096\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[0;34m(self, conn)\u001b[0m\n\u001b[1;32m   1094\u001b[0m \u001b[39m# Force connect early to allow us to validate the connection.\u001b[39;00m\n\u001b[1;32m   1095\u001b[0m \u001b[39mif\u001b[39;00m conn\u001b[39m.\u001b[39mis_closed:\n\u001b[0;32m-> 1096\u001b[0m     conn\u001b[39m.\u001b[39;49mconnect()\n\u001b[1;32m   1098\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m conn\u001b[39m.\u001b[39mis_verified:\n\u001b[1;32m   1099\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m   1100\u001b[0m         (\n\u001b[1;32m   1101\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnverified HTTPS request is being made to host \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mconn\u001b[39m.\u001b[39mhost\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1106\u001b[0m         InsecureRequestWarning,\n\u001b[1;32m   1107\u001b[0m     )\n",
      "File \u001b[0;32m~/miniforge3/envs/mats_sae_training/lib/python3.11/site-packages/urllib3/connection.py:611\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconnect\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    610\u001b[0m     sock: socket\u001b[39m.\u001b[39msocket \u001b[39m|\u001b[39m ssl\u001b[39m.\u001b[39mSSLSocket\n\u001b[0;32m--> 611\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msock \u001b[39m=\u001b[39m sock \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_new_conn()\n\u001b[1;32m    612\u001b[0m     server_hostname: \u001b[39mstr\u001b[39m \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhost\n\u001b[1;32m    613\u001b[0m     tls_in_tls \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/mats_sae_training/lib/python3.11/site-packages/urllib3/connection.py:203\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Establish a socket connection and set nodelay settings on it.\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \n\u001b[1;32m    200\u001b[0m \u001b[39m:return: New socket connection.\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 203\u001b[0m     sock \u001b[39m=\u001b[39m connection\u001b[39m.\u001b[39;49mcreate_connection(\n\u001b[1;32m    204\u001b[0m         (\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dns_host, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mport),\n\u001b[1;32m    205\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtimeout,\n\u001b[1;32m    206\u001b[0m         source_address\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msource_address,\n\u001b[1;32m    207\u001b[0m         socket_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msocket_options,\n\u001b[1;32m    208\u001b[0m     )\n\u001b[1;32m    209\u001b[0m \u001b[39mexcept\u001b[39;00m socket\u001b[39m.\u001b[39mgaierror \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    210\u001b[0m     \u001b[39mraise\u001b[39;00m NameResolutionError(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhost, \u001b[39mself\u001b[39m, e) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/mats_sae_training/lib/python3.11/site-packages/urllib3/util/connection.py:73\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[39mif\u001b[39;00m source_address:\n\u001b[1;32m     72\u001b[0m     sock\u001b[39m.\u001b[39mbind(source_address)\n\u001b[0;32m---> 73\u001b[0m sock\u001b[39m.\u001b[39;49mconnect(sa)\n\u001b[1;32m     74\u001b[0m \u001b[39m# Break explicitly a reference cycle\u001b[39;00m\n\u001b[1;32m     75\u001b[0m err \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "model = HookedTransformer.from_pretrained(\n",
    "    \"tiny-stories-2L-33M\",\n",
    "    # center_unembed=True,\n",
    "    # center_writing_weights=True,\n",
    "    # fold_ln=True,\n",
    "    # refactor_factored_attn_matrices=True,\n",
    ")\n",
    "# model.set_use_split_qkv_input(True)\n",
    "# model.set_use_attn_result(True)\n",
    "import transformer_lens.evals as evals\n",
    "evals.sanity_check(model)\n",
    "\n",
    "path = \"checkpoints/399ihu5z/final_sparse_autoencoder_tiny-stories-2L-33M_blocks.1.attn.hook_q_4096.pt\"\n",
    "sparse_autoencoder = SparseAutoencoder.load_from_pretrained(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_3fe34_row0_col0, #T_3fe34_row0_col2, #T_3fe34_row1_col0, #T_3fe34_row1_col2, #T_3fe34_row2_col0, #T_3fe34_row2_col2, #T_3fe34_row3_col0, #T_3fe34_row3_col2, #T_3fe34_row4_col0, #T_3fe34_row4_col2, #T_3fe34_row4_col3, #T_3fe34_row5_col0, #T_3fe34_row5_col2, #T_3fe34_row6_col0, #T_3fe34_row6_col2, #T_3fe34_row7_col0, #T_3fe34_row7_col2, #T_3fe34_row8_col0, #T_3fe34_row8_col1, #T_3fe34_row8_col2, #T_3fe34_row9_col0, #T_3fe34_row9_col1, #T_3fe34_row9_col2, #T_3fe34_row9_col4 {\n",
       "  background-color: #f7fbff;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_3fe34_row0_col1, #T_3fe34_row1_col1, #T_3fe34_row2_col1, #T_3fe34_row4_col1, #T_3fe34_row5_col1 {\n",
       "  background-color: #c6dbef;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_3fe34_row0_col3, #T_3fe34_row0_col4, #T_3fe34_row3_col1, #T_3fe34_row6_col1, #T_3fe34_row7_col1 {\n",
       "  background-color: #08306b;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_3fe34_row1_col3, #T_3fe34_row7_col3 {\n",
       "  background-color: #bfd8ed;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_3fe34_row1_col4 {\n",
       "  background-color: #5ba3d0;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_3fe34_row2_col3 {\n",
       "  background-color: #08488e;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_3fe34_row2_col4 {\n",
       "  background-color: #c9ddf0;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_3fe34_row3_col3 {\n",
       "  background-color: #2676b8;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_3fe34_row3_col4, #T_3fe34_row4_col4 {\n",
       "  background-color: #e3eef8;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_3fe34_row5_col3 {\n",
       "  background-color: #7db8da;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_3fe34_row5_col4 {\n",
       "  background-color: #e7f0fa;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_3fe34_row6_col3 {\n",
       "  background-color: #e5eff9;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_3fe34_row6_col4 {\n",
       "  background-color: #eaf3fb;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_3fe34_row7_col4 {\n",
       "  background-color: #eef5fc;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_3fe34_row8_col3 {\n",
       "  background-color: #d3e4f3;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_3fe34_row8_col4 {\n",
       "  background-color: #f6faff;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_3fe34_row9_col3 {\n",
       "  background-color: #5ca4d0;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_3fe34\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_3fe34_level0_col0\" class=\"col_heading level0 col0\" >Layer1</th>\n",
       "      <th id=\"T_3fe34_level0_col1\" class=\"col_heading level0 col1\" >Head1</th>\n",
       "      <th id=\"T_3fe34_level0_col2\" class=\"col_heading level0 col2\" >Layer2</th>\n",
       "      <th id=\"T_3fe34_level0_col3\" class=\"col_heading level0 col3\" >Head2</th>\n",
       "      <th id=\"T_3fe34_level0_col4\" class=\"col_heading level0 col4\" >Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_3fe34_level0_row0\" class=\"row_heading level0 row0\" >219</th>\n",
       "      <td id=\"T_3fe34_row0_col0\" class=\"data row0 col0\" >0</td>\n",
       "      <td id=\"T_3fe34_row0_col1\" class=\"data row0 col1\" >6</td>\n",
       "      <td id=\"T_3fe34_row0_col2\" class=\"data row0 col2\" >1</td>\n",
       "      <td id=\"T_3fe34_row0_col3\" class=\"data row0 col3\" >11</td>\n",
       "      <td id=\"T_3fe34_row0_col4\" class=\"data row0 col4\" >0.100765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3fe34_level0_row1\" class=\"row_heading level0 row1\" >211</th>\n",
       "      <td id=\"T_3fe34_row1_col0\" class=\"data row1 col0\" >0</td>\n",
       "      <td id=\"T_3fe34_row1_col1\" class=\"data row1 col1\" >6</td>\n",
       "      <td id=\"T_3fe34_row1_col2\" class=\"data row1 col2\" >1</td>\n",
       "      <td id=\"T_3fe34_row1_col3\" class=\"data row1 col3\" >3</td>\n",
       "      <td id=\"T_3fe34_row1_col4\" class=\"data row1 col4\" >0.080608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3fe34_level0_row2\" class=\"row_heading level0 row2\" >218</th>\n",
       "      <td id=\"T_3fe34_row2_col0\" class=\"data row2 col0\" >0</td>\n",
       "      <td id=\"T_3fe34_row2_col1\" class=\"data row2 col1\" >6</td>\n",
       "      <td id=\"T_3fe34_row2_col2\" class=\"data row2 col2\" >1</td>\n",
       "      <td id=\"T_3fe34_row2_col3\" class=\"data row2 col3\" >10</td>\n",
       "      <td id=\"T_3fe34_row2_col4\" class=\"data row2 col4\" >0.066766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3fe34_level0_row3\" class=\"row_heading level0 row3\" >408</th>\n",
       "      <td id=\"T_3fe34_row3_col0\" class=\"data row3 col0\" >0</td>\n",
       "      <td id=\"T_3fe34_row3_col1\" class=\"data row3 col1\" >12</td>\n",
       "      <td id=\"T_3fe34_row3_col2\" class=\"data row3 col2\" >1</td>\n",
       "      <td id=\"T_3fe34_row3_col3\" class=\"data row3 col3\" >8</td>\n",
       "      <td id=\"T_3fe34_row3_col4\" class=\"data row3 col4\" >0.060902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3fe34_level0_row4\" class=\"row_heading level0 row4\" >208</th>\n",
       "      <td id=\"T_3fe34_row4_col0\" class=\"data row4 col0\" >0</td>\n",
       "      <td id=\"T_3fe34_row4_col1\" class=\"data row4 col1\" >6</td>\n",
       "      <td id=\"T_3fe34_row4_col2\" class=\"data row4 col2\" >1</td>\n",
       "      <td id=\"T_3fe34_row4_col3\" class=\"data row4 col3\" >0</td>\n",
       "      <td id=\"T_3fe34_row4_col4\" class=\"data row4 col4\" >0.060876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3fe34_level0_row5\" class=\"row_heading level0 row5\" >213</th>\n",
       "      <td id=\"T_3fe34_row5_col0\" class=\"data row5 col0\" >0</td>\n",
       "      <td id=\"T_3fe34_row5_col1\" class=\"data row5 col1\" >6</td>\n",
       "      <td id=\"T_3fe34_row5_col2\" class=\"data row5 col2\" >1</td>\n",
       "      <td id=\"T_3fe34_row5_col3\" class=\"data row5 col3\" >5</td>\n",
       "      <td id=\"T_3fe34_row5_col4\" class=\"data row5 col4\" >0.059937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3fe34_level0_row6\" class=\"row_heading level0 row6\" >401</th>\n",
       "      <td id=\"T_3fe34_row6_col0\" class=\"data row6 col0\" >0</td>\n",
       "      <td id=\"T_3fe34_row6_col1\" class=\"data row6 col1\" >12</td>\n",
       "      <td id=\"T_3fe34_row6_col2\" class=\"data row6 col2\" >1</td>\n",
       "      <td id=\"T_3fe34_row6_col3\" class=\"data row6 col3\" >1</td>\n",
       "      <td id=\"T_3fe34_row6_col4\" class=\"data row6 col4\" >0.059086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3fe34_level0_row7\" class=\"row_heading level0 row7\" >403</th>\n",
       "      <td id=\"T_3fe34_row7_col0\" class=\"data row7 col0\" >0</td>\n",
       "      <td id=\"T_3fe34_row7_col1\" class=\"data row7 col1\" >12</td>\n",
       "      <td id=\"T_3fe34_row7_col2\" class=\"data row7 col2\" >1</td>\n",
       "      <td id=\"T_3fe34_row7_col3\" class=\"data row7 col3\" >3</td>\n",
       "      <td id=\"T_3fe34_row7_col4\" class=\"data row7 col4\" >0.058306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3fe34_level0_row8\" class=\"row_heading level0 row8\" >146</th>\n",
       "      <td id=\"T_3fe34_row8_col0\" class=\"data row8 col0\" >0</td>\n",
       "      <td id=\"T_3fe34_row8_col1\" class=\"data row8 col1\" >4</td>\n",
       "      <td id=\"T_3fe34_row8_col2\" class=\"data row8 col2\" >1</td>\n",
       "      <td id=\"T_3fe34_row8_col3\" class=\"data row8 col3\" >2</td>\n",
       "      <td id=\"T_3fe34_row8_col4\" class=\"data row8 col4\" >0.056562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3fe34_level0_row9\" class=\"row_heading level0 row9\" >150</th>\n",
       "      <td id=\"T_3fe34_row9_col0\" class=\"data row9 col0\" >0</td>\n",
       "      <td id=\"T_3fe34_row9_col1\" class=\"data row9 col1\" >4</td>\n",
       "      <td id=\"T_3fe34_row9_col2\" class=\"data row9 col2\" >1</td>\n",
       "      <td id=\"T_3fe34_row9_col3\" class=\"data row9 col3\" >6</td>\n",
       "      <td id=\"T_3fe34_row9_col4\" class=\"data row9 col4\" >0.056216</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x600f5ae50>"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import itertools\n",
    "import numpy as np\n",
    "\n",
    "def tensor_to_long_data_frame(tensor_result, dimension_names):\n",
    "    assert len(tensor_result.shape) == len(\n",
    "        dimension_names\n",
    "    ), \"The number of dimension names must match the number of dimensions in the tensor\"\n",
    "\n",
    "    tensor_2d = tensor_result.reshape(-1)\n",
    "    df = pd.DataFrame(tensor_2d.detach().numpy(), columns=[\"Score\"])\n",
    "\n",
    "    indices = pd.MultiIndex.from_tuples(\n",
    "        list(np.ndindex(tensor_result.shape)),\n",
    "        names=dimension_names,\n",
    "    )\n",
    "    df.index = indices\n",
    "    df.reset_index(inplace=True)\n",
    "    return df\n",
    "\n",
    "q_composition_scores = model.all_composition_scores(mode = \"Q\")\n",
    "q_comp_df = tensor_to_long_data_frame(q_composition_scores.detach().cpu(), [\"Layer1\", \"Head1\", \"Layer2\", \"Head2\"])\n",
    "q_comp_df.sort_values(\"Score\", ascending=False).head(10).style.background_gradient(cmap=\"Blues\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_webtext(seed: int = 420, dataset=\"stas/openwebtext-10k\", split=\"train[:1%]\") -> List[str]:\n",
    "    \"\"\"Get 10,000 sentences from the OpenWebText dataset\"\"\"\n",
    "\n",
    "    # Let's see some WEBTEXT\n",
    "    train_dataset = load_dataset(dataset, split=split)\n",
    "    # train_dataset = raw_dataset[\"train\"]\n",
    "    dataset = [train_dataset[i][\"text\"] for i in range(len(train_dataset))]\n",
    "\n",
    "    # Shuffle the dataset (I don't want the Hitler thing being first so use a seeded shuffle)\n",
    "    np.random.seed(seed)\n",
    "    np.random.shuffle(dataset)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "data = get_webtext(dataset=\"roneneldan/TinyStories\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 58/200 [00:42<01:48,  1.31it/s]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "str_token_list = []\n",
    "loss_list = []\n",
    "ablated_loss_list = []\n",
    "\n",
    "import joseph.analysis as analysis\n",
    "reload(analysis)\n",
    "\n",
    "NUM_PROMPTS = 200\n",
    "# MAX_PROMPT_LEN = 100\n",
    "# BATCH_SIZE = 10\n",
    "dataframe_list = []\n",
    "with torch.no_grad():\n",
    "    model.reset_hooks()\n",
    "    for i in tqdm(range(NUM_PROMPTS)):\n",
    "        \n",
    "        # Get Token Data\n",
    "        prompt = data[i]\n",
    "        # new_str = data[BATCH_SIZE * i: BATCH_SIZE * (i + 1)]\n",
    "        token_df, _, _= analysis.eval_prompt(prompt, model=model, sparse_autoencoder=sparse_autoencoder)\n",
    "        token_df[\"batch\"] = i\n",
    "        token_df[\"label\"] = token_df[\"batch\"].astype(str) + \"/\" + token_df[\"pos\"].astype(str)\n",
    "        dataframe_list.append(token_df)\n",
    "        \n",
    "head_analysis_df = pd.concat(dataframe_list)\n",
    "\n",
    "print(head_analysis_df.columns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(head_analysis_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_cols = [\"unique_token\", \"context\", \"label\", \"loss\", \"loss_diff\", \n",
    "               \"mse_loss\", \"num_active_features\", \"max_idx_tok\", \"rec_q_max_idx_tok\",\n",
    "               \"explained_variance\", \"top10_token_suppression_inds\",\"top10_token_boosting_inds\"]\n",
    "\n",
    "tmp = head_analysis_df[filter_cols].sort_values(\"loss_diff\", ascending=False)\n",
    "tmp.head(10).reset_index().style.background_gradient(\n",
    "    subset=[\"loss_diff\", \"mse_loss\",\"explained_variance\", \"num_active_features\"],# \"top10_token_suppression_diffs\", \"top10_token_suppression_inds\"],\n",
    "    cmap=\"coolwarm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head_analysis_df[\"max_idx_tok_suppressed\"] = head_analysis_df.apply(lambda x: x[\"max_idx_tok\"] in x[\"top10_token_suppression_inds\"], axis=1)\n",
    "head_analysis_df[\"rec_q_max_idx_tok_suppressed\"] = head_analysis_df.apply(lambda x: x[\"rec_q_max_idx_tok\"] in x[\"top10_token_suppression_inds\"], axis=1)\n",
    "print(head_analysis_df[\"max_idx_tok_suppressed\"].mean())\n",
    "print(head_analysis_df[\"rec_q_max_idx_tok_suppressed\"].mean())\n",
    "\n",
    "head_analysis_df[\"max_idx_tok_boosted\"] = head_analysis_df.apply(lambda x: x[\"max_idx_tok\"] in x[\"top10_token_boosting_inds\"], axis=1)\n",
    "head_analysis_df[\"rec_q_max_idx_tok_boosted\"] = head_analysis_df.apply(lambda x: x[\"rec_q_max_idx_tok\"] in x[\"top10_token_boosting_inds\"], axis=1)\n",
    "print(head_analysis_df[\"max_idx_tok_boosted\"].mean())\n",
    "print(head_analysis_df[\"rec_q_max_idx_tok_boosted\"].mean())\n",
    "\n",
    "px.bar(pd.DataFrame( {\"Most Attended Token is Suppressed\": head_analysis_df[\"max_idx_tok_suppressed\"].mean(),\n",
    "        \"Most Attended Token is Boosted\": head_analysis_df[\"max_idx_tok_boosted\"].mean(),\n",
    "        \"Most Attended Token is Suppressed (Rec Q)\": head_analysis_df[\"rec_q_max_idx_tok_suppressed\"].mean(),\n",
    "        \"Most Attended Token is Boosted (Rec Q)\": head_analysis_df[\"rec_q_max_idx_tok_boosted\"].mean(),\n",
    "    }, index=[0]).T\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.strip(head_analysis_df, x=\"loss_diff\", color=\"max_idx_tok_suppressed\", hover_data=[\"context\", \"max_idx_tok\", \"top10_token_suppression_inds\"]).show()\n",
    "px.strip(head_analysis_df, x=\"loss_diff\", color=\"max_idx_tok_boosted\", hover_data=[\"context\", \"max_idx_tok\", \"top10_token_boosting_inds\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_cols = [\"str_tokens\", \"label\",\"context\", \"loss_diff\", \"max_idx_tok\", \"rec_q_max_idx_tok\", \"top10_token_suppression_inds\",\"top_k_features\"]\n",
    "head_analysis_df[head_analysis_df.max_idx_tok_suppressed][filter_cols].sample(30).sort_values(\"loss_diff\", ascending=True).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do these features have high dot product with \",\" unembed?\n",
    "token = \",\"\n",
    "tmp = head_analysis_df[head_analysis_df.max_idx_tok_suppressed]\n",
    "tmp = tmp[tmp.max_idx_tok == token]\n",
    "top_k_features_for_comma = tmp.explode(\"top_k_features\").top_k_features.value_counts()[:2]\n",
    "features = top_k_features_for_comma.index\n",
    "token_id = model.to_single_token(token)\n",
    "query_feature_unembed_virtual_weight = sparse_autoencoder.W_dec @ model.W_Q[LAYER_IDX,HEAD_IDX].T @ model.W_U[:,token_id]\n",
    "px.bar(x = [str(i) for i in range(4096)], y =query_feature_unembed_virtual_weight.detach().cpu()).show()\n",
    "random_features = torch.randint(0, 4096, (1000,))\n",
    "random_feature_unembed_virtual_weight = sparse_autoencoder.W_dec[random_features] @ model.W_Q[LAYER_IDX,HEAD_IDX].T @ model.W_U[:,token_id]\n",
    "px.histogram(random_feature_unembed_virtual_weight.flatten().detach().cpu(), log_y=True).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_virtual_weights = sparse_autoencoder.W_dec @ model.W_Q[LAYER_IDX,HEAD_IDX].T @ model.W_U\n",
    "torch.quantile(all_virtual_weights[all_virtual_weights > 0.05], q = 0.99999)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at diagrams:\n",
    "- we can see that 1949 appears to be genuinely a \",\" is next feature (though it fires often for \"Suddenly\").\n",
    "- whilst 474 might be slightly more a \". Then\" (two token?) feature. \n",
    "- 3619 looks like a dense feature so my guess is it has positive high average scores\n",
    "- 2722 is often \"Day\" (often appears before a \",\"). Lower activations for things like birthday.\n",
    "\n",
    "What this means is that when \",\" is often suppressed by this head but can the SAE splits that up into token specific situations. \n",
    "Let's check the cosine sim on the encoder/decoder weights.\n",
    "\n",
    "It seems like 474 and 1949 which should be pretty mutually exclusive might be somewhat confused with each other due to both being partly about inhibiting \",\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can we say that a feature specifies given a tokem, whether or not we are doing suppression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = \",\"\n",
    "tmp = head_analysis_df.query(f\"max_idx_tok == '{token}'\")\n",
    "filter_cols = [\"unique_token\", \"context\", \"label\", \"loss_diff\", \"max_idx_tok\", \"rec_q_max_idx_tok\", \"top10_token_suppression_inds\",\"max_idx_tok_suppressed\", \"max_idx_tok_value\"]\n",
    "print(tmp.max_idx_tok_suppressed.mean())\n",
    "tmp[filter_cols].sort_values(\"max_idx_tok_value\", ascending=False).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tmp = head_analysis_df[head_analysis_df.max_idx_tok_suppressed]\n",
    "tmp = tmp[tmp.max_idx_tok == token]\n",
    "tmp[filter_cols].sample(30).sort_values(\"loss_diff\", ascending=True).reset_index().style.background_gradient(\n",
    "    subset=[\"loss_diff\", \"num_active_features\"],# \"top10_token_suppression_diffs\", \"top10_token_suppression_inds\"],\n",
    "    cmap=\"coolwarm\")\n",
    "\n",
    "top_k_features_for_comma = tmp.explode(\"top_k_features\").top_k_features.value_counts()[:2]\n",
    "top_k_features_for_comma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = 3506; token = token\n",
    "feature_token_mask = head_analysis_df.query(f\"max_idx_tok == '{token}'\").top_k_features.apply(lambda x: feature in x)\n",
    "tmp = head_analysis_df.query(f\"max_idx_tok == '{token}'\")[feature_token_mask]\n",
    "px.histogram(tmp.apply(lambda x: x[\"top_k_feature_acts\"][x[\"top_k_features\"].index(feature)], axis =1)).show()\n",
    "filter_cols = [\"unique_token\", \"context\", \"label\", \"loss_diff\", \"max_idx_tok\", \"rec_q_max_idx_tok\", \"top10_token_suppression_inds\",\"max_idx_tok_suppressed\", \"max_idx_tok_value\"]\n",
    "tmp[filter_cols].sort_values(\"max_idx_tok_value\", ascending=False).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set pandas width unlimited\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "filter_cols = [\"unique_token\", \"context\", \"label\", \"loss_diff\", \"max_idx_tok\", \"rec_q_max_idx_tok\", \"top10_token_suppression_inds\",\"max_idx_tok_suppressed\"]\n",
    "\n",
    "top_k_features_for_comma = tmp.explode(\"top_k_features\").top_k_features.value_counts()[:10].index\n",
    "comma_attended_to_set = head_analysis_df.query(\"max_idx_tok == ','\")\n",
    "comma_not_attended_to_set = head_analysis_df.query(\"max_idx_tok != ','\")\n",
    "for i in top_k_features_for_comma:\n",
    "    print(i)\n",
    "    print(\"feature prob given token\", comma_attended_to_set.top_k_features.apply(lambda x: i in x).mean())\n",
    "    print(\"feature prob given not token\", comma_not_attended_to_set.top_k_features.apply(lambda x: i in x).mean())\n",
    "    print(\"ratio prob:\", comma_attended_to_set.top_k_features.apply(lambda x: i in x).mean() / comma_not_attended_to_set.top_k_features.apply(lambda x: i in x).mean())\n",
    "    print(\"prob max idx suppressed given token\", comma_attended_to_set[comma_attended_to_set.top_k_features.apply(lambda x: i in x)].max_idx_tok_suppressed.mean())\n",
    "    print(\"prob max idx suppressed given not token\", comma_not_attended_to_set[comma_not_attended_to_set.top_k_features.apply(lambda x: i in x)].max_idx_tok_suppressed.mean())\n",
    "    display(comma_attended_to_set[comma_attended_to_set.top_k_features.apply(lambda x: i in x)].sort_values(\n",
    "        \"max_idx_tok_value\").head(10)[filter_cols])\n",
    "    display(comma_not_attended_to_set[comma_not_attended_to_set.top_k_features.apply(lambda x: i in x)].sort_values(\n",
    "        \"max_idx_tok_value\").head(10)[filter_cols])\n",
    "    print(\"------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_qk_via_feature(model, 2722, sparse_autoencoder, feature_name = \"\", highlight_tokens=[\",\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster import hierarchy\n",
    "\n",
    "def plot_cosine_similarity_heatmap(df, restricted_labels, reorder = False, title=\"Pairwise Cosine Similarity Heatmap\"):\n",
    "    data_array = df.to_numpy()\n",
    "    linkage = hierarchy.linkage(data_array)\n",
    "    dendrogram = hierarchy.dendrogram(\n",
    "        linkage, no_plot=True, color_threshold=-np.inf\n",
    "    )\n",
    "    if reorder:\n",
    "        reordered_ind = dendrogram[\"leaves\"]\n",
    "        # reorder df by ind\n",
    "        df = df.iloc[reordered_ind, reordered_ind]\n",
    "        # data_array = df.to_numpy()\n",
    "\n",
    "    # plot the cosine similarity matrix\n",
    "    fig = fig = px.imshow(\n",
    "            df,\n",
    "            color_continuous_scale=\"RdBu\",\n",
    "            color_continuous_midpoint=0.0,\n",
    "            labels={\"color\": \"Cosine Similarity\"},\n",
    "        )\n",
    "    fig.update_xaxes(\n",
    "        tickmode=\"array\",\n",
    "        tickvals=list(range(len(restricted_labels))),\n",
    "        ticktext=restricted_labels,\n",
    "        showgrid=False,\n",
    "    )\n",
    "    fig.update_yaxes(\n",
    "        tickmode=\"array\",\n",
    "        tickvals=list(range(len(restricted_labels))),\n",
    "        ticktext=restricted_labels,\n",
    "        showgrid=False,\n",
    "    )\n",
    "\n",
    "    # don't show axes if there are more than 20 rows \n",
    "    if df.shape[0] > 20:\n",
    "        fig.update_xaxes(\n",
    "            visible=False,\n",
    "        )\n",
    "        fig.update_yaxes(\n",
    "            visible=False,\n",
    "        )\n",
    "    return fig\n",
    "\n",
    "feature_dirs = pd.DataFrame(sparse_autoencoder.W_dec[features].detach().cpu().numpy(), index= features).reset_index(drop=True).T.corr()\n",
    "plot_cosine_similarity_heatmap(feature_dirs, restricted_labels=features).show()\n",
    "feature_dirs = pd.DataFrame(sparse_autoencoder.W_enc[:,features].T.detach().cpu().numpy(), index= features).reset_index(drop=True).T.corr()\n",
    "plot_cosine_similarity_heatmap(feature_dirs, restricted_labels=features).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last thing to do before writing up the report:\n",
    "- Show that these features are doing copy suppression causally. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_qk_via_feature(model, 1949, sparse_autoencoder, feature_name = \"\", highlight_tokens=[\",\"])\n",
    "# plot_qk_via_feature(model, 3102, sparse_autoencoder, feature_name = \"\", highlight_tokens=[\" Lily\"])\n",
    "# plot_qk_via_feature(model, 2656, sparse_autoencoder, feature_name = \"\", highlight_tokens=[\" Lily\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = 118; pos = 94\n",
    "# batch = 48; pos = 15\n",
    "prompt = data[batch]\n",
    "tokens = model.to_tokens(prompt)\n",
    "example_prompt = model.to_string(tokens[:,:pos+1])[0]\n",
    "example_prompt_answer = model.to_string(tokens[:,pos+1])\n",
    "# example_prompt_answer = \" Lily\"\n",
    "example_prompt_tokens = tokens[:,:pos]\n",
    "_, cache_original = model.run_with_cache(example_prompt_tokens, prepend_bos=False)\n",
    "\n",
    "def test_prompt_with_sae(example_prompt, example_prompt_answer, model, sparse_autoencoder, cache_original):\n",
    "    \n",
    "    model.reset_hooks()\n",
    "    utils.test_prompt(example_prompt, example_prompt_answer, model, prepend_space_to_answer=False, prepend_bos=False)\n",
    "\n",
    "    with model.hooks(fwd_hooks=[(HEAD_HOOK_RESULT_NAME, hook_to_ablate_head)]):\n",
    "        utils.test_prompt(example_prompt, example_prompt_answer, model, prepend_bos=False, prepend_space_to_answer=False)\n",
    "\n",
    "    sae_out, feature_acts, loss, mse_loss, l1_loss = sparse_autoencoder(\n",
    "        cache_original[sparse_autoencoder.cfg.hook_point][0,HEAD_IDX]\n",
    "    )\n",
    "    def reconstr_query_hook(hook_in, hook, reconstructed_query=sae_out, head = HEAD_IDX):\n",
    "        hook_in[:, head, :] = reconstructed_query\n",
    "        return hook_in\n",
    "\n",
    "    with model.hooks(fwd_hooks=[(sparse_autoencoder.cfg.hook_point, reconstr_query_hook)]):\n",
    "        utils.test_prompt(example_prompt, example_prompt_answer, model,  prepend_bos=False, prepend_space_to_answer=False)\n",
    "        \n",
    "    \n",
    "test_prompt_with_sae(example_prompt, example_prompt_answer, model, sparse_autoencoder, cache_original)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.reset_hooks()\n",
    "token_df, original_cache, cache_reconstructed_query = eval_prompt([example_prompt], model, sparse_autoencoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_acts = original_cache[sparse_autoencoder.cfg.hook_point]\n",
    "print(original_acts.shape)\n",
    "sae_out, feature_acts, loss, mse_loss, l1_loss = sparse_autoencoder(\n",
    "    original_cache[sparse_autoencoder.cfg.hook_point][:,-1,HEAD_IDX]\n",
    ")\n",
    "print(feature_acts.shape)\n",
    "vals, inds = torch.topk(feature_acts[0], 10)\n",
    "print(inds)\n",
    "plot_line_with_top_10_labels(feature_acts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " model.W_Q[LAYER_IDX,HEAD_IDX].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_get_attn_sae_feature_removal(example_prompt, \n",
    "                                      example_prompt_answer, \n",
    "                                      model, \n",
    "                                      sparse_autoencoder,\n",
    "                                      features_to_remove=[0],\n",
    "                                      token_unembed_to_remove=[]):\n",
    "    \n",
    "    \n",
    "    example_prompt_tokens = model.to_tokens(example_prompt)\n",
    "    attn_df, original_cache, cache_reconstructed_query = eval_prompt([example_prompt], model, sparse_autoencoder)\n",
    "\n",
    "    original_acts = original_cache[sparse_autoencoder.cfg.hook_point]\n",
    "    print(original_acts.shape)\n",
    "    sae_out, feature_acts, loss, mse_loss, l1_loss = sparse_autoencoder(\n",
    "       original_acts[0,:,HEAD_IDX]\n",
    "    )\n",
    "    \n",
    "\n",
    "    def remove_feature_hook(hook_in, hook, head = HEAD_IDX, features_to_remove = features_to_remove):\n",
    "        for feature_to_remove in features_to_remove:\n",
    "            feature_dir = feature_acts[-1,feature_to_remove]*sparse_autoencoder.W_dec[feature_to_remove]\n",
    "            hook_in[:, :, head] -= feature_dir\n",
    "        return hook_in\n",
    "    \n",
    "    with model.hooks(fwd_hooks=[(sparse_autoencoder.cfg.hook_point, remove_feature_hook)]):\n",
    "        _, cache_removed_feature = model.run_with_cache(example_prompt_tokens, return_type=\"loss\", loss_per_token=True)\n",
    "        \n",
    "        \n",
    "    def replace_with_feature_hook(hook_in, hook, head = HEAD_IDX, features_to_add = features_to_remove):\n",
    "        new_act = torch.zeros_like(hook_in[:, :, head])\n",
    "        for features_to_add in features_to_remove:\n",
    "            feature_dir = feature_acts[-1,features_to_add]*sparse_autoencoder.W_dec[features_to_add]\n",
    "            new_act += feature_dir\n",
    "        hook_in[:, :, head] = new_act\n",
    "        return hook_in\n",
    "    \n",
    "    with model.hooks(fwd_hooks=[(sparse_autoencoder.cfg.hook_point, replace_with_feature_hook)]):\n",
    "        _, cache_only_features = model.run_with_cache(example_prompt_tokens, return_type=\"loss\", loss_per_token=True)\n",
    "        \n",
    "\n",
    "    def remove_token_unembed_hook(hook_in, hook, head = HEAD_IDX, token_unembed_to_remove = token_unembed_to_remove):\n",
    "        for token_id in token_unembed_to_remove:\n",
    "            unembed_dir = model.W_U[:, token_id] @ model.W_Q[LAYER_IDX,HEAD_IDX]\n",
    "            unembed_dir_proj =  hook_in[:, :, head] @ unembed_dir.T\n",
    "            hook_in[:, :, head] -= unembed_dir_proj[:,:,None]*unembed_dir\n",
    "        return hook_in\n",
    "    \n",
    "    with model.hooks(fwd_hooks=[(sparse_autoencoder.cfg.hook_point, remove_token_unembed_hook)]):\n",
    "        _, cache_removed_token_unembed = model.run_with_cache(example_prompt_tokens, return_type=\"loss\", loss_per_token=True)\n",
    "\n",
    "    patterns = original_cache[f\"blocks.{LAYER_IDX}.attn.hook_pattern\"][0,HEAD_IDX].detach().cpu()\n",
    "    attn_df = make_token_df(model, example_prompt_tokens)\n",
    "    # remove last row of attn df\n",
    "    attn_df[\"original_attn\"] = patterns[-1,]\n",
    "    patterns = cache_reconstructed_query[f\"blocks.{LAYER_IDX}.attn.hook_pattern\"][0,HEAD_IDX].detach().cpu()\n",
    "    attn_df[\"reconstructed_attn\"] = patterns[-1,]\n",
    "    patterns = cache_removed_feature[f\"blocks.{LAYER_IDX}.attn.hook_pattern\"][0,HEAD_IDX].detach().cpu()\n",
    "    attn_df[\"ablated_feature_attn\"] = patterns[-1,]\n",
    "    patterns = cache_only_features[f\"blocks.{LAYER_IDX}.attn.hook_pattern\"][0,HEAD_IDX].detach().cpu()\n",
    "    attn_df[\"cache_only_features\"] = patterns[-1,]\n",
    "    patterns = cache_removed_token_unembed[f\"blocks.{LAYER_IDX}.attn.hook_pattern\"][0,HEAD_IDX].detach().cpu()\n",
    "    attn_df[\"cache_removed_token_unembed\"] = patterns[-1,]\n",
    "    \n",
    "    if len(token_unembed_to_remove) > 0:\n",
    "        fig = px.line(attn_df, \n",
    "                    y=[\"original_attn\",\"reconstructed_attn\", \"ablated_feature_attn\", \"cache_only_features\", \"cache_removed_token_unembed\"],\n",
    "                    hover_name=\"str_tokens\", \n",
    "                    hover_data=[\"pos\", \"batch\", \"label\"], \n",
    "                    title=\"Original vs Reconstructed attention\")\n",
    "    else:\n",
    "        fig = px.line(attn_df, \n",
    "                    y=[\"original_attn\",\"reconstructed_attn\", \"ablated_feature_attn\", \"cache_only_features\"],\n",
    "                    hover_name=\"str_tokens\", \n",
    "                    hover_data=[\"pos\", \"batch\", \"label\"], \n",
    "                    title=\"Original vs Reconstructed attention\")\n",
    "    \n",
    "    # increase figure height\n",
    "    fig.update_layout(height=800)\n",
    "    fig.show()\n",
    "\n",
    "def test_prompt_with_sae_feature_removal(example_prompt, example_prompt_answer, model, sparse_autoencoder, features_to_remove=[0]):\n",
    "    \n",
    "    model.reset_hooks()\n",
    "    print(\"ORIGINAL\")\n",
    "    utils.test_prompt(example_prompt, example_prompt_answer, model, prepend_space_to_answer=False, prepend_bos=False)\n",
    "\n",
    "    print(\"HEAD ABLATED\")\n",
    "    with model.hooks(fwd_hooks=[(HEAD_HOOK_RESULT_NAME, hook_to_ablate_head)]):\n",
    "        utils.test_prompt(example_prompt, example_prompt_answer, model, prepend_bos=False, prepend_space_to_answer=False)\n",
    "\n",
    "    token_df, original_cache, cache_reconstructed_query = eval_prompt([example_prompt], model, sparse_autoencoder)\n",
    "    sae_out, feature_acts, loss, mse_loss, l1_loss = sparse_autoencoder(\n",
    "        original_cache[sparse_autoencoder.cfg.hook_point][0,HEAD_IDX]\n",
    "    )\n",
    "    def reconstr_query_hook(hook_in, hook, reconstructed_query=sae_out, head = HEAD_IDX):\n",
    "        hook_in[:, head, :] = reconstructed_query\n",
    "        return hook_in\n",
    "\n",
    "    def remove_feature_hook(hook_in, hook, head = HEAD_IDX, features_to_remove = features_to_remove):\n",
    "        print(hook_in.shape)\n",
    "        for feature_to_remove in features_to_remove:\n",
    "            feature_dir = feature_acts[-1,feature_to_remove]*sparse_autoencoder.W_dec[feature_to_remove]\n",
    "            hook_in[:, :, head] -= feature_dir\n",
    "        return hook_in\n",
    "    \n",
    "    print(\"FEATURE REMOVED\")\n",
    "    with model.hooks(fwd_hooks=[(sparse_autoencoder.cfg.hook_point, remove_feature_hook)]):\n",
    "        utils.test_prompt(example_prompt, example_prompt_answer, model,  prepend_bos=False, prepend_space_to_answer=False)\n",
    "\n",
    "    def replace_with_feature_hook(hook_in, hook, head = HEAD_IDX, features_to_add = features_to_remove):\n",
    "        new_act = torch.zeros_like(hook_in[:, :, head])\n",
    "        for features_to_add in features_to_remove:\n",
    "            feature_dir = feature_acts[-1,features_to_add]*sparse_autoencoder.W_dec[features_to_add]\n",
    "            new_act += feature_dir\n",
    "        hook_in[:, :, head] = new_act\n",
    "        return hook_in\n",
    "    \n",
    "    print(\"ONLY FEATURE USED\")\n",
    "    with model.hooks(fwd_hooks=[(sparse_autoencoder.cfg.hook_point, replace_with_feature_hook)]):\n",
    "        utils.test_prompt(example_prompt, example_prompt_answer, model,  prepend_bos=False, prepend_space_to_answer=False)\n",
    "        \n",
    "\n",
    "test_get_attn_sae_feature_removal(\n",
    "    example_prompt, \n",
    "    example_prompt_answer, model, \n",
    "    sparse_autoencoder, features_to_remove=[1949, 474], token_unembed_to_remove=[model.to_single_token(\",\")])\n",
    "\n",
    "test_prompt_with_sae_feature_removal(\n",
    "    example_prompt, \n",
    "    example_prompt_answer, model, \n",
    "    sparse_autoencoder, features_to_remove=[1949])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! So clearly these features are able to cause copy suppression for that token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "from sae_analysis.visualizer import data_fns\n",
    "from sae_analysis.visualizer.data_fns import FeatureData\n",
    "from typing import Dict\n",
    "reload(data_fns)\n",
    "\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "dataset=\"roneneldan/TinyStories\"\n",
    "train_dataset = load_dataset(dataset, split=\"train[:3%]\")\n",
    "tokenized_data = utils.tokenize_and_concatenate(train_dataset, model.tokenizer, max_length=128)\n",
    "tokenized_data = tokenized_data.shuffle(42)\n",
    "all_tokens = tokenized_data[\"tokens\"]\n",
    "\n",
    "\n",
    "# Currently, don't think much more time can be squeezed out of it. Maybe the best saving would be to\n",
    "# make the entire sequence indexing parallelized, but that's possibly not worth it right now.\n",
    "\n",
    "max_batch_size = 32\n",
    "total_batch_size = 512 * 50\n",
    "feature_idx = top_k_features_for_comma.index.to_list()\n",
    "feature_idx = inds.tolist()\n",
    "# max_batch_size = 512\n",
    "# total_batch_size = 16384\n",
    "# feature_idx = list(range(1000))\n",
    "\n",
    "tokens = all_tokens[:total_batch_size]\n",
    "\n",
    "feature_data: Dict[int, FeatureData] = data_fns.get_feature_data(\n",
    "    encoder=sparse_autoencoder,\n",
    "    # encoder_B=sparse_autoencoder,\n",
    "    model=model,\n",
    "    hook_point=sparse_autoencoder.cfg.hook_point,\n",
    "    hook_point_layer=sparse_autoencoder.cfg.hook_point_layer - 1,\n",
    "    hook_point_head_index=sparse_autoencoder.cfg.hook_point_head_index,\n",
    "    tokens=tokens,\n",
    "    feature_idx=feature_idx,\n",
    "    max_batch_size=max_batch_size,\n",
    "    left_hand_k = 3,\n",
    "    buffer = (5, 5),\n",
    "    n_groups = 10,\n",
    "    first_group_size = 20,\n",
    "    other_groups_size = 5,\n",
    "    verbose = True,\n",
    ")\n",
    "\n",
    "for test_idx in feature_idx:\n",
    "    html_str = feature_data[test_idx].get_all_html()\n",
    "    path = f\"tiny_stories_features/{test_idx:04}.html\"\n",
    "    print(f\"Saving to {path}\")\n",
    "    with open(path, \"w\") as f:\n",
    "        f.write(html_str)\n",
    "\n",
    "\n",
    "del feature_data\n",
    "torch.mps.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's identify the features firing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TO DO:\n",
    "\n",
    "1. We can now track copy suppression and copying behavior. \n",
    "2. Following from that we want to check that this is real and that we can find the relevant features.\n",
    "3. Given some feature we want:\n",
    "   1. The enc / dec direction.\n",
    "   2. The unembed of the token/s it's supposed to suppress.\n",
    "   3. The virtual weight between the two. \n",
    "   4. Causal intervention to remove the unembed dir and see what effect we get"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del feature_data\n",
    "torch.mps.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.DataFrame({\"1\": [1,2,3], \"2\": [4,5,6], \"3\":[ {1:2, 2:3}, {3:4, 4:5}, {5:6, 1:3}]})\n",
    "test.explode(\"3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mats_sae_training",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
