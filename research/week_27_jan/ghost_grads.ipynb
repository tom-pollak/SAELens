{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append(\"../..\")\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from importlib import reload\n",
    "\n",
    "import joseph\n",
    "from joseph.analysis import *\n",
    "from joseph.visualisation import *\n",
    "from joseph.utils import *\n",
    "from joseph.data import *\n",
    "\n",
    "\n",
    "reload(joseph.analysis)\n",
    "reload(joseph.visualisation)\n",
    "reload(joseph.utils)\n",
    "reload(joseph.data)\n",
    "\n",
    "from joseph.analysis import *\n",
    "from joseph.visualisation import *\n",
    "from joseph.utils import *\n",
    "from joseph.data import *\n",
    "\n",
    "# turn torch grad tracking off\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "\n",
    "model = HookedTransformer.from_pretrained(\n",
    "    \"gpt2-small\",\n",
    "    # \"tiny-stories-2L-33M\",\n",
    "    # \"attn-only-2l\",\n",
    "    # center_unembed=True,\n",
    "    # center_writing_weights=True,\n",
    "    # fold_ln=True,\n",
    "    # refactor_factored_attn_matrices=True,\n",
    ")\n",
    "model.set_use_split_qkv_input(True)\n",
    "model.set_use_attn_result(True)\n",
    "\n",
    "\n",
    "# sanity check\n",
    "text = \"Many important transition points in the history of science have been moments when science 'zoomed in.' At these points, we develop a visualization or tool that allows us to see the world in a new level of detail, and a new field of science develops to study the world through this lens.\"\n",
    "model(text, return_type=\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sae_training.utils import LMSparseAutoencoderSessionloader\n",
    "\n",
    "\n",
    "path = \"../week_8_jan/artifacts/sparse_autoencoder_gpt2-small_blocks.10.hook_resid_pre_49152:v28/1100001280_sparse_autoencoder_gpt2-small_blocks.10.hook_resid_pre_49152.pt\"\n",
    "model, sparse_autoencoder, activation_store = LMSparseAutoencoderSessionloader.load_session_from_pretrained(\n",
    "    path\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sparse_autoencoder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msparse_autoencoder\u001b[49m\u001b[38;5;241m.\u001b[39mcfg\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sparse_autoencoder' is not defined"
     ]
    }
   ],
   "source": [
    "sparse_autoencoder.cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
<<<<<<< Updated upstream
=======
<<<<<<< Updated upstream
   "source": []
=======
>>>>>>> Stashed changes
   "source": [
    "log_feature_sparsity_10 = torch.load(\n",
    "    \"../week_8_jan/artifacts/sparse_autoencoder_gpt2-small_blocks.10.hook_resid_pre_49152:v28/log_feature_sparsity_5000_4.pt\"\n",
    ")\n",
    "\n",
    "px.histogram(log_feature_sparsity_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_autoencoder.training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dead_neuron_mask = log_feature_sparsity_10 == -10\n",
    "dead_neuron_mask.nonzero().squeeze().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "original_act = activation_store.next_batch()\n",
    "print(original_act.shape)\n",
    "\n",
    "sae_out, feature_acts, loss, mse_loss, l1_loss = sparse_autoencoder(\n",
    "    original_act\n",
    ")\n",
    "\n",
    "residual = original_act - sae_out\n",
    "\n",
    "l2_norm_original_act = torch.norm(original_act, dim = -1)\n",
    "l2_norm_sae_out = torch.norm(sae_out, dim =-1)\n",
    "l2_norm_residual = torch.norm(residual, dim=-1)\n",
    "\n",
    "print(l2_norm_original_act.mean().item(),l2_norm_sae_out.mean().item(), l2_norm_residual.mean().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_acts_dead_neurons_only = torch.exp(feature_acts[:, dead_neuron_mask])\n",
    "ghost_out =  feature_acts_dead_neurons_only @ sparse_autoencoder.W_dec[dead_neuron_mask,:]\n",
    "print(ghost_out.shape)\n",
    "l2_norm_ghost_out = torch.norm(ghost_out, dim = -1)\n",
    "norm_scaling_factor = l2_norm_residual / (l2_norm_ghost_out* 2)\n",
    "px.histogram(norm_scaling_factor.detach().cpu(), height = 200) \n",
    "print(l2_norm_ghost_out.mean().item())\n",
    "ghost_out = ghost_out*norm_scaling_factor[:, None].detach()\n",
    "\n",
    "l2_norm_ghost_out = torch.norm(ghost_out, dim = -1)\n",
    "print(l2_norm_ghost_out.mean().item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_loss_ghost_resid = (\n",
    "    torch.pow((ghost_out - residual.float()), 2) / (residual**2).sum(dim=-1, keepdim=True).sqrt()\n",
    ").mean()\n",
    "print(mse_loss_ghost_resid.item())\n",
    "\n",
    "mse_rescaling_factor = (mse_loss / mse_loss_ghost_resid).detach()\n",
    "mse_loss_ghost_resid = mse_rescaling_factor * mse_loss_ghost_resid\n",
    "\n",
    "print(mse_loss.item(), l1_loss.item(), mse_loss_ghost_resid.item())\n",
    "new_loss = mse_loss + l1_loss + mse_loss_ghost_resid"
   ]
<<<<<<< Updated upstream
=======
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([49152])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(feature_acts > 0).float().sum(-2).shape"
   ]
>>>>>>> Stashed changes
>>>>>>> Stashed changes
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mats_sae_training",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
