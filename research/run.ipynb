{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gelu-2L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os \n",
    "\n",
    "from sae_training.config import LanguageModelSAERunnerConfig\n",
    "from sae_training.lm_runner import language_model_sae_runner\n",
    "\n",
    "import cProfile\n",
    "\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "cfg = LanguageModelSAERunnerConfig(\n",
    "\n",
    "    # Data Generating Function (Model + Training Distibuion)\n",
    "    model_name = \"gelu-2l\",\n",
    "    hook_point = \"blocks.0.hook_mlp_out\",\n",
    "    hook_point_layer = 0,\n",
    "    d_in = 512,\n",
    "    dataset_path = \"NeelNanda/c4-tokenized-2b\",\n",
    "    is_dataset_tokenized=True,\n",
    "    \n",
    "    # SAE Parameters\n",
    "    expansion_factor = 32,\n",
    "    \n",
    "    # Training Parameters\n",
    "    lr = 1e-4,\n",
    "    l1_coefficient = 3e-4,\n",
    "    train_batch_size = 4096,\n",
    "    context_size = 128,\n",
    "    \n",
    "    # Activation Store Parameters\n",
    "    n_batches_in_buffer = 128,\n",
    "    total_training_tokens = 1_000_000 * 500, \n",
    "    store_batch_size = 32,\n",
    "    \n",
    "    # Resampling protocol\n",
    "    feature_sampling_method = 'l2',\n",
    "    feature_sampling_window = 2500,\n",
    "    feature_reinit_scale = 0.2,\n",
    "    dead_feature_window=1250,\n",
    "    dead_feature_threshold = 1e-8,\n",
    "    \n",
    "    # WANDB\n",
    "    log_to_wandb = True,\n",
    "    wandb_project= \"mats_sae_training_language_models_gelu_2l\",\n",
    "    wandb_entity = None,\n",
    "    wandb_log_frequency=10,\n",
    "    \n",
    "    # Misc\n",
    "    device = \"mps\",\n",
    "    seed = 42,\n",
    "    n_checkpoints = 10,\n",
    "    checkpoint_path = \"checkpoints\",\n",
    "    dtype = torch.float32,\n",
    "    )\n",
    "\n",
    "\n",
    "sparse_autoencoder = language_model_sae_runner(cfg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT2 - Small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os \n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"WANDB__SERVICE_WAIT\"] = \"300\"\n",
    "\n",
    "from sae_training.config import LanguageModelSAERunnerConfig\n",
    "from sae_training.lm_runner import language_model_sae_runner\n",
    "\n",
    "cfg = LanguageModelSAERunnerConfig(\n",
    "\n",
    "    # Data Generating Function (Model + Training Distibuion)\n",
    "    model_name = \"gpt2-small\",\n",
    "    hook_point = \"blocks.10.hook_resid_pre\",\n",
    "    hook_point_layer = 11,\n",
    "    d_in = 768,\n",
    "    dataset_path = \"Skylion007/openwebtext\",\n",
    "    is_dataset_tokenized=False,\n",
    "    \n",
    "    # SAE Parameters\n",
    "    expansion_factor = 64, # determines the dimension of the SAE.\n",
    "    \n",
    "    # Training Parameters\n",
    "    lr = 1e-5,\n",
    "    l1_coefficient = 5e-4,\n",
    "    lr_scheduler_name=None,\n",
    "    train_batch_size = 4096,\n",
    "    context_size = 128,\n",
    "    \n",
    "    # Activation Store Parameters\n",
    "    n_batches_in_buffer = 128,\n",
    "    total_training_tokens = 1_000_000 * 200, # 200M tokens seems doable overnight.\n",
    "    store_batch_size = 32,\n",
    "    \n",
    "    # Resampling protocol\n",
    "    feature_sampling_method = 'l2',\n",
    "    feature_sampling_window = 1000,\n",
    "    feature_reinit_scale = 0.2,\n",
    "    dead_feature_window=5000,\n",
    "    dead_feature_threshold = 1e-7,\n",
    "    \n",
    "    # WANDB\n",
    "    log_to_wandb = True,\n",
    "    wandb_project= \"mats_sae_training_gpt2_small\",\n",
    "    wandb_entity = None,\n",
    "    wandb_log_frequency=50,\n",
    "    \n",
    "    # Misc\n",
    "    device = \"mps\",\n",
    "    seed = 42,\n",
    "    n_checkpoints = 5,\n",
    "    checkpoint_path = \"checkpoints\",\n",
    "    dtype = torch.float32,\n",
    "    )\n",
    "\n",
    "sparse_autoencoder = language_model_sae_runner(cfg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT2-Small Hook Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/josephbloom/miniforge3/envs/mats_sae_training/lib/python3.11/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/Users/josephbloom/miniforge3/envs/mats_sae_training/lib/python3.11/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/Users/josephbloom/miniforge3/envs/mats_sae_training/lib/python3.11/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_tokens_per_buffer (millions): 2.097152\n",
      "Lower bound: n_contexts_per_buffer (millions): 0.016384\n",
      "Total training steps: 11596\n",
      "Total wandb updates: 386\n",
      "n_dead_feature_samples: 2\n",
      "Loaded pretrained model gpt2-small into HookedTransformer\n",
      "Moving model to device:  mps\n",
      "Dataset is not tokenized! Updating config.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjbloom\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/josephbloom/GithubRepositories/mats_sae_training/research/wandb/run-20231219_161200-8atjsx2k</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jbloom/mats_sae_training_gpt2_small_hook_q_new/runs/8atjsx2k' target=\"_blank\">stellar-sun-4</a></strong> to <a href='https://wandb.ai/jbloom/mats_sae_training_gpt2_small_hook_q_new' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jbloom/mats_sae_training_gpt2_small_hook_q_new' target=\"_blank\">https://wandb.ai/jbloom/mats_sae_training_gpt2_small_hook_q_new</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jbloom/mats_sae_training_gpt2_small_hook_q_new/runs/8atjsx2k' target=\"_blank\">https://wandb.ai/jbloom/mats_sae_training_gpt2_small_hook_q_new/runs/8atjsx2k</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "773| MSE Loss 17.911 | L1 23.663:   7%|▋         | 3170304/47500000 [02:44<31:59, 23089.88it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to checkpoints/80yd4j7z/3170304_sparse_autoencoder_gpt2-small_blocks.10.attn.hook_q_65536.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1546| MSE Loss 16.244 | L1 23.661:  13%|█▎        | 6336512/47500000 [05:58<28:54, 23735.99it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to checkpoints/80yd4j7z/6336512_sparse_autoencoder_gpt2-small_blocks.10.attn.hook_q_65536.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2319| MSE Loss 15.766 | L1 22.999:  20%|██        | 9502720/47500000 [08:19<27:39, 22895.40it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to checkpoints/80yd4j7z/9502720_sparse_autoencoder_gpt2-small_blocks.10.attn.hook_q_65536.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2998| MSE Loss 15.226 | L1 22.680:  26%|██▌       | 12283904/47500000 [10:21<25:22, 23133.12it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resampled 1 neurons\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3092| MSE Loss 15.455 | L1 22.170:  27%|██▋       | 12668928/47500000 [10:42<23:29, 24711.16it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to checkpoints/80yd4j7z/12668928_sparse_autoencoder_gpt2-small_blocks.10.attn.hook_q_65536.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3865| MSE Loss 14.881 | L1 21.697:  33%|███▎      | 15835136/47500000 [12:59<21:14, 24841.11it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to checkpoints/80yd4j7z/15835136_sparse_autoencoder_gpt2-small_blocks.10.attn.hook_q_65536.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4638| MSE Loss 14.449 | L1 21.378:  40%|████      | 19001344/47500000 [15:20<19:12, 24733.38it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to checkpoints/80yd4j7z/19001344_sparse_autoencoder_gpt2-small_blocks.10.attn.hook_q_65536.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5411| MSE Loss 14.558 | L1 20.535:  47%|████▋     | 22167552/47500000 [17:40<18:28, 22856.45it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to checkpoints/80yd4j7z/22167552_sparse_autoencoder_gpt2-small_blocks.10.attn.hook_q_65536.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6184| MSE Loss 14.328 | L1 19.962:  53%|█████▎    | 25333760/47500000 [19:57<15:01, 24576.24it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to checkpoints/80yd4j7z/25333760_sparse_autoencoder_gpt2-small_blocks.10.attn.hook_q_65536.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6958| MSE Loss 14.363 | L1 19.510:  60%|██████    | 28504064/47500000 [22:17<14:03, 22509.32it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to checkpoints/80yd4j7z/28504064_sparse_autoencoder_gpt2-small_blocks.10.attn.hook_q_65536.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7731| MSE Loss 14.096 | L1 19.397:  67%|██████▋   | 31670272/47500000 [24:33<10:22, 25431.61it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to checkpoints/80yd4j7z/31670272_sparse_autoencoder_gpt2-small_blocks.10.attn.hook_q_65536.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8504| MSE Loss 14.236 | L1 19.207:  73%|███████▎  | 34836480/47500000 [26:53<08:22, 25215.41it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to checkpoints/80yd4j7z/34836480_sparse_autoencoder_gpt2-small_blocks.10.attn.hook_q_65536.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9277| MSE Loss 13.791 | L1 19.171:  80%|████████  | 38002688/47500000 [29:09<06:43, 23515.85it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to checkpoints/80yd4j7z/38002688_sparse_autoencoder_gpt2-small_blocks.10.attn.hook_q_65536.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10050| MSE Loss 14.000 | L1 18.660:  87%|████████▋ | 41168896/47500000 [31:30<04:15, 24749.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to checkpoints/80yd4j7z/41168896_sparse_autoencoder_gpt2-small_blocks.10.attn.hook_q_65536.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10823| MSE Loss 14.000 | L1 18.748:  93%|█████████▎| 44335104/47500000 [33:50<02:04, 25329.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to checkpoints/80yd4j7z/44335104_sparse_autoencoder_gpt2-small_blocks.10.attn.hook_q_65536.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11596| MSE Loss 14.277 | L1 18.413: : 47501312it [36:05, 24876.35it/s]                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to checkpoints/80yd4j7z/47501312_sparse_autoencoder_gpt2-small_blocks.10.attn.hook_q_65536.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11596| MSE Loss 14.277 | L1 18.413: : 47501312it [36:06, 21927.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to checkpoints/80yd4j7z/final_sparse_autoencoder_gpt2-small_blocks.10.attn.hook_q_65536.pt\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os \n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"WANDB__SERVICE_WAIT\"] = \"300\"\n",
    "\n",
    "from sae_training.config import LanguageModelSAERunnerConfig\n",
    "from sae_training.lm_runner import language_model_sae_runner\n",
    "\n",
    "\n",
    "# for l1_coefficient in [9e-4,8e-4,7e-4]:\n",
    "cfg = LanguageModelSAERunnerConfig(\n",
    "\n",
    "    # Data Generating Function (Model + Training Distibuion)\n",
    "    model_name = \"gpt2-small\",\n",
    "    hook_point = \"blocks.10.attn.hook_q\",\n",
    "    hook_point_layer = 10,\n",
    "    hook_point_head_index=7,\n",
    "    d_in = 64,\n",
    "    dataset_path = \"Skylion007/openwebtext\",\n",
    "    is_dataset_tokenized=False,\n",
    "    use_cached_activations=True,\n",
    "    cached_activations_path=\"../activations/\",\n",
    "    \n",
    "    # SAE Parameters\n",
    "    expansion_factor = 64 * 16, # 65536\n",
    "    \n",
    "    # Training Parameters\n",
    "    lr = 1e-3,\n",
    "    l1_coefficient = 4e-4,\n",
    "    # lr_scheduler_name=\"LinearWarmupDecay\",\n",
    "    lr_warm_up_steps=2200,\n",
    "    train_batch_size = 4096,\n",
    "    context_size = 128,\n",
    "    \n",
    "    # Activation Store Parameters\n",
    "    n_batches_in_buffer = 512,\n",
    "    total_training_tokens = 1_000_000 * 50 - 2_500_000,# avoid having to muse a buffer we don't have.\n",
    "    store_batch_size = 32,\n",
    "    \n",
    "    # Resampling protocol\n",
    "    feature_sampling_method = 'l2',\n",
    "    feature_sampling_window = 1000,\n",
    "    feature_reinit_scale = 0.2,\n",
    "    dead_feature_window=3000,\n",
    "    dead_feature_threshold = 5e-6,\n",
    "    \n",
    "    # WANDB\n",
    "    log_to_wandb = True,\n",
    "    wandb_project= \"mats_sae_training_gpt2_small_hook_q_new\",\n",
    "    wandb_entity = None,\n",
    "    wandb_log_frequency=30,\n",
    "    \n",
    "    # Misc\n",
    "    device = \"mps\",\n",
    "    seed = 42,\n",
    "    n_checkpoints = 15,\n",
    "    checkpoint_path = \"checkpoints\",\n",
    "    dtype = torch.float32,\n",
    "    )\n",
    "\n",
    "sparse_autoencoder = language_model_sae_runner(cfg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(1_000_000 * 500 - 4096) / 4096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_autoencoder.save_model(\"./overnight_sae_resid_pre_10_gpt_2_small.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pythia 70-M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os \n",
    "\n",
    "from sae_training.config import LanguageModelSAERunnerConfig\n",
    "from sae_training.lm_runner import language_model_sae_runner\n",
    "\n",
    "import cProfile\n",
    "\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "cfg = LanguageModelSAERunnerConfig(\n",
    "\n",
    "    # Data Generating Function (Model + Training Distibuion)\n",
    "    model_name = \"pythia-70m\",\n",
    "    hook_point = \"blocks.0.hook_mlp_out\",\n",
    "    hook_point_layer = 0,\n",
    "    d_in = 512,\n",
    "    dataset_path = \"EleutherAI/the_pile_deduplicated\",\n",
    "    is_dataset_tokenized=False,\n",
    "    \n",
    "    # SAE Parameters\n",
    "    expansion_factor = 16,\n",
    "    \n",
    "    # Training Parameters\n",
    "    lr = 3e-4,\n",
    "    l1_coefficient = 1e-3,\n",
    "    train_batch_size = 4096,\n",
    "    context_size = 128,\n",
    "    \n",
    "    # Activation Store Parameters\n",
    "    n_batches_in_buffer = 64,\n",
    "    total_training_tokens = 1_000_000 * 5, \n",
    "    store_batch_size = 32,\n",
    "    \n",
    "    # Resampling protocol\n",
    "    feature_sampling_method = 'l2',\n",
    "    feature_sampling_window = 2500, # Doesn't currently matter.\n",
    "    feature_reinit_scale = 0.2,\n",
    "    dead_feature_window=1250,\n",
    "    dead_feature_threshold = 1e-8,\n",
    "    \n",
    "    # WANDB\n",
    "    log_to_wandb = True,\n",
    "    wandb_project= \"mats_sae_training_language_benchmark_tests\",\n",
    "    wandb_entity = None,\n",
    "    wandb_log_frequency=10,\n",
    "    \n",
    "    # Misc\n",
    "    device = \"mps\",\n",
    "    seed = 42,\n",
    "    n_checkpoints = 0,\n",
    "    checkpoint_path = \"checkpoints\",\n",
    "    dtype = torch.float32,\n",
    "    )\n",
    "\n",
    "def main():\n",
    "    sparse_autoencoder = language_model_sae_runner(cfg)\n",
    "\n",
    "main()\n",
    "# os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "# import cProfile, pstats, io\n",
    "# from pstats import SortKey\n",
    "# pr = cProfile.Profile()\n",
    "# pr.enable()\n",
    "# # ... do something ...\n",
    "# main()\n",
    "# pr.disable()\n",
    "# s = io.StringIO()\n",
    "# sortby = SortKey.CUMULATIVE\n",
    "# ps = pstats.Stats(pr, stream=s).sort_stats(sortby)\n",
    "# ps.print_stats()\n",
    "# print(s.getvalue())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tiny Stories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os \n",
    "\n",
    "from sae_training.config import LanguageModelSAERunnerConfig\n",
    "from sae_training.lm_runner import language_model_sae_runner\n",
    "\n",
    "\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "cfg = LanguageModelSAERunnerConfig(\n",
    "\n",
    "    # Data Generating Function (Model + Training Distibuion)\n",
    "    model_name = \"tiny-stories-2L-33M\",\n",
    "    hook_point = \"blocks.1.mlp.hook_post\",\n",
    "    hook_point_layer = 1,\n",
    "    d_in = 4096,\n",
    "    dataset_path = \"roneneldan/TinyStories\",\n",
    "    is_dataset_tokenized=False,\n",
    "    \n",
    "    # SAE Parameters\n",
    "    expansion_factor = 4,\n",
    "    \n",
    "    # Training Parameters\n",
    "    lr = 1e-4,\n",
    "    l1_coefficient = 3e-4,\n",
    "    train_batch_size = 4096,\n",
    "    context_size = 128,\n",
    "    \n",
    "    # Activation Store Parameters\n",
    "    n_batches_in_buffer = 128,\n",
    "    total_training_tokens = 1_000_000 * 10, # want 500M eventually.\n",
    "    store_batch_size = 32,\n",
    "    \n",
    "    # Resampling protocol\n",
    "    feature_sampling_method = 'l2',\n",
    "    feature_sampling_window = 2500, # Doesn't currently matter.\n",
    "    feature_reinit_scale = 0.2,\n",
    "    dead_feature_window=1250,\n",
    "    dead_feature_threshold = 0.0005,\n",
    "    \n",
    "    # WANDB\n",
    "    log_to_wandb = True,\n",
    "    wandb_project= \"mats_sae_training_language_benchmark_tests\",\n",
    "    wandb_entity = None,\n",
    "    wandb_log_frequency=10,\n",
    "    \n",
    "    # Misc\n",
    "    device = \"mps\",\n",
    "    seed = 42,\n",
    "    n_checkpoints = 0,\n",
    "    checkpoint_path = \"checkpoints\",\n",
    "    dtype = torch.float32,\n",
    "    )\n",
    "\n",
    "sparse_autoencoder = language_model_sae_runner(cfg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toy Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sae_training.toy_model_runner import SAEToyModelRunnerConfig, toy_model_sae_runner\n",
    "\n",
    "\n",
    "cfg = SAEToyModelRunnerConfig(\n",
    "    \n",
    "    # Model Details\n",
    "    n_features=200,\n",
    "    n_hidden=5,\n",
    "    n_correlated_pairs=0,\n",
    "    n_anticorrelated_pairs=0,\n",
    "    feature_probability=0.025,\n",
    "    model_training_steps=10_000,\n",
    "    \n",
    "    # SAE Parameters\n",
    "    d_sae=240,\n",
    "    l1_coefficient=0.001,\n",
    "    \n",
    "    # SAE Train Config\n",
    "    train_batch_size=1028,\n",
    "    feature_sampling_window=3_000,\n",
    "    dead_feature_window=1_000,\n",
    "    feature_reinit_scale=0.5,\n",
    "    total_training_tokens=4096*300,\n",
    "    \n",
    "    # Other parameters\n",
    "    log_to_wandb=True,\n",
    "    wandb_project=\"sae-training-test\",\n",
    "    wandb_log_frequency=5,\n",
    "    device=\"mps\",\n",
    ")\n",
    "\n",
    "trained_sae = toy_model_sae_runner(cfg)\n",
    "\n",
    "assert trained_sae is not None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run caching of activations to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os \n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"WANDB__SERVICE_WAIT\"] = \"300\"\n",
    "\n",
    "from sae_training.config import CacheActivationsRunnerConfig\n",
    "from sae_training.cache_activations_runner import cache_activations_runner\n",
    "\n",
    "cfg = CacheActivationsRunnerConfig(\n",
    "\n",
    "    # Data Generating Function (Model + Training Distibuion)\n",
    "    model_name = \"gpt2-small\",\n",
    "    hook_point = \"blocks.10.attn.hook_q\",\n",
    "    hook_point_layer = 11,\n",
    "    hook_point_head_index=7,\n",
    "    d_in = 64,\n",
    "    dataset_path = \"Skylion007/openwebtext\",\n",
    "    is_dataset_tokenized=False,\n",
    "    cached_activations_path=\"../activations/\",\n",
    "    \n",
    "    # Activation Store Parameters\n",
    "    n_batches_in_buffer = 16,\n",
    "    total_training_tokens = 500_000_000, \n",
    "    store_batch_size = 32,\n",
    "\n",
    "    # Activation caching shuffle parameters\n",
    "    n_shuffles_final = 16,\n",
    "    \n",
    "    # Misc\n",
    "    device = \"mps\",\n",
    "    seed = 42,\n",
    "    dtype = torch.float32,\n",
    "    )\n",
    "\n",
    "cache_activations_runner(cfg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train an SAE using the cached activations stored on disk\n",
    "Pass `use_cached_activations=True` into the config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os \n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"WANDB__SERVICE_WAIT\"] = \"300\"\n",
    "from sae_training.config import LanguageModelSAERunnerConfig\n",
    "from sae_training.lm_runner import language_model_sae_runner\n",
    "\n",
    "cfg = LanguageModelSAERunnerConfig(\n",
    "\n",
    "    # Data Generating Function (Model + Training Distibuion)\n",
    "    model_name = \"gpt2-small\",\n",
    "    hook_point = \"blocks.10.hook_resid_pre\",\n",
    "    hook_point_layer = 11,\n",
    "    d_in = 768,\n",
    "    dataset_path = \"Skylion007/openwebtext\",\n",
    "    is_dataset_tokenized=False,\n",
    "    use_cached_activations=True,\n",
    "    \n",
    "    # SAE Parameters\n",
    "    expansion_factor = 64, # determines the dimension of the SAE.\n",
    "    \n",
    "    # Training Parameters\n",
    "    lr = 1e-5,\n",
    "    l1_coefficient = 5e-4,\n",
    "    lr_scheduler_name=None,\n",
    "    train_batch_size = 4096,\n",
    "    context_size = 128,\n",
    "    \n",
    "    # Activation Store Parameters\n",
    "    n_batches_in_buffer = 64,\n",
    "    total_training_tokens = 200_000,\n",
    "    store_batch_size = 32,\n",
    "    \n",
    "    # Resampling protocol\n",
    "    feature_sampling_method = 'l2',\n",
    "    feature_sampling_window = 1000,\n",
    "    feature_reinit_scale = 0.2,\n",
    "    dead_feature_window=5000,\n",
    "    dead_feature_threshold = 1e-7,\n",
    "    \n",
    "    # WANDB\n",
    "    log_to_wandb = True,\n",
    "    wandb_project= \"mats_sae_training_gpt2_small\",\n",
    "    wandb_entity = None,\n",
    "    wandb_log_frequency=50,\n",
    "    \n",
    "    # Misc\n",
    "    device = \"mps\",\n",
    "    seed = 42,\n",
    "    n_checkpoints = 5,\n",
    "    checkpoint_path = \"checkpoints\",\n",
    "    dtype = torch.float32,\n",
    "    )\n",
    "\n",
    "sparse_autoencoder = language_model_sae_runner(cfg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/josephbloom/miniforge3/envs/mats_sae_training/lib/python3.11/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/Users/josephbloom/miniforge3/envs/mats_sae_training/lib/python3.11/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/Users/josephbloom/miniforge3/envs/mats_sae_training/lib/python3.11/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_tokens_per_buffer (millions): 2.097152\n",
      "Lower bound: n_contexts_per_buffer (millions): 0.016384\n",
      "Total training steps: 732\n",
      "Total wandb updates: 146\n",
      "n_dead_feature_samples: 2\n",
      "Loaded pretrained model gpt2-small into HookedTransformer\n",
      "Moving model to device:  mps\n",
      "Dataset is not tokenized! Updating config.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjbloom\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/josephbloom/GithubRepositories/mats_sae_training/research/wandb/run-20231219_155222-0g1egmus</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jbloom/mats_sae_training_gpt2_small_hook_q_dev/runs/0g1egmus' target=\"_blank\">cerulean-sky-23</a></strong> to <a href='https://wandb.ai/jbloom/mats_sae_training_gpt2_small_hook_q_dev' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jbloom/mats_sae_training_gpt2_small_hook_q_dev' target=\"_blank\">https://wandb.ai/jbloom/mats_sae_training_gpt2_small_hook_q_dev</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jbloom/mats_sae_training_gpt2_small_hook_q_dev/runs/0g1egmus' target=\"_blank\">https://wandb.ai/jbloom/mats_sae_training_gpt2_small_hook_q_dev/runs/0g1egmus</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "732| MSE Loss 7.234 | L1 18.154: : 3002368it [01:45, 28515.37it/s]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to checkpoints/obxe76cw/final_sparse_autoencoder_gpt2-small_blocks.10.attn.hook_q_4096.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e98637a3948147918c505436292a0933",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='31.940 MB of 41.399 MB uploaded\\r'), FloatProgress(value=0.771506892761734, max=1.…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>details/current_learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>details/n_training_tokens</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>losses/l1_loss</td><td>█▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>losses/mse_loss</td><td>██▅▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>losses/overall_loss</td><td>█▄▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>metrics/CE_loss_score</td><td>▁▆▂▂▆▅▃▄▅██▇▆▆</td></tr><tr><td>metrics/ce_loss_with_ablation</td><td>▇▅▃▅▂▁▇▃▄█▂▆▆█</td></tr><tr><td>metrics/ce_loss_with_sae</td><td>▇▅▃▅▂▁█▃▄█▂▆▆█</td></tr><tr><td>metrics/ce_loss_without_sae</td><td>▇▅▃▅▂▁█▃▄█▂▆▆█</td></tr><tr><td>metrics/explained_variance</td><td>▁▁▄▆▇▇▇▇▇▇██████████████████████████████</td></tr><tr><td>metrics/explained_variance_std</td><td>▅█▄▂▁▁▁▁▁▁▁▁▂▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂▂</td></tr><tr><td>metrics/kldiv_ablation</td><td>▄▃▅▁▆▂▆▅▇▇█▆▃▄</td></tr><tr><td>metrics/kldiv_reconstructed</td><td>█▆▆▅▄▄▅▃▃▂▃▂▁▂</td></tr><tr><td>metrics/l0</td><td>█▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>metrics/l2_norm</td><td>▁▃▆▇▆▆▇█▇▇█▇▇▇</td></tr><tr><td>metrics/l2_ratio</td><td>▁▄▆▇▆▆▇▇▇██▇██</td></tr><tr><td>metrics/n_resampled_neurons</td><td>▁▁▁</td></tr><tr><td>sparsity/below_1e-5</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>sparsity/below_1e-6</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>sparsity/dead_features</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>details/current_learning_rate</td><td>0.001</td></tr><tr><td>details/n_training_tokens</td><td>2990080</td></tr><tr><td>losses/l1_loss</td><td>17.97697</td></tr><tr><td>losses/mse_loss</td><td>7.28971</td></tr><tr><td>losses/overall_loss</td><td>25.26668</td></tr><tr><td>metrics/CE_loss_score</td><td>0.74266</td></tr><tr><td>metrics/ce_loss_with_ablation</td><td>3.8</td></tr><tr><td>metrics/ce_loss_with_sae</td><td>3.74677</td></tr><tr><td>metrics/ce_loss_without_sae</td><td>3.72833</td></tr><tr><td>metrics/explained_variance</td><td>0.91121</td></tr><tr><td>metrics/explained_variance_std</td><td>0.04923</td></tr><tr><td>metrics/kldiv_ablation</td><td>2.97408</td></tr><tr><td>metrics/kldiv_reconstructed</td><td>0.17199</td></tr><tr><td>metrics/l0</td><td>157.49023</td></tr><tr><td>metrics/l2_norm</td><td>7.63263</td></tr><tr><td>metrics/l2_ratio</td><td>0.81848</td></tr><tr><td>metrics/n_resampled_neurons</td><td>0</td></tr><tr><td>sparsity/below_1e-5</td><td>0.0</td></tr><tr><td>sparsity/below_1e-6</td><td>0.0</td></tr><tr><td>sparsity/dead_features</td><td>0.0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">cerulean-sky-23</strong> at: <a href='https://wandb.ai/jbloom/mats_sae_training_gpt2_small_hook_q_dev/runs/0g1egmus' target=\"_blank\">https://wandb.ai/jbloom/mats_sae_training_gpt2_small_hook_q_dev/runs/0g1egmus</a><br/>Synced 7 W&B file(s), 84 media file(s), 1 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231219_155222-0g1egmus/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import os \n",
    "import sys \n",
    "sys.path.append(\"..\")\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"WANDB__SERVICE_WAIT\"] = \"300\"\n",
    "from sae_training.config import LanguageModelSAERunnerConfig\n",
    "from sae_training.lm_runner import language_model_sae_runner\n",
    "\n",
    "\n",
    "\n",
    "# for l1_coefficient in [9e-4,8e-4,7e-4]:\n",
    "cfg = LanguageModelSAERunnerConfig(\n",
    "\n",
    "    # Data Generating Function (Model + Training Distibuion)\n",
    "    model_name = \"gpt2-small\",\n",
    "    hook_point = \"blocks.10.attn.hook_q\",\n",
    "    hook_point_layer = 10,\n",
    "    hook_point_head_index=7,\n",
    "    d_in = 64,\n",
    "    dataset_path = \"Skylion007/openwebtext\",\n",
    "    is_dataset_tokenized=False,\n",
    "    use_cached_activations=True,\n",
    "    cached_activations_path=\"../activations/\",\n",
    "    \n",
    "    # SAE Parameters\n",
    "    expansion_factor = 64, # determines the dimension of the SAE. (64*64 = 4096, 64*4*64 = 32768)\n",
    "    \n",
    "    # Training Parameters\n",
    "    lr = 1e-3,\n",
    "    l1_coefficient = 2e-4,\n",
    "    # lr_scheduler_name=\"LinearWarmupDecay\",\n",
    "    lr_warm_up_steps=2200,\n",
    "    train_batch_size = 4096,\n",
    "    context_size = 128,\n",
    "    \n",
    "    # Activation Store Parameters\n",
    "    n_batches_in_buffer = 512,\n",
    "    total_training_tokens = 3_000_000,\n",
    "    store_batch_size = 32,\n",
    "    \n",
    "    # Resampling protocol\n",
    "    feature_sampling_method = 'l2',\n",
    "    feature_sampling_window = 1000,\n",
    "    feature_reinit_scale = 0.2,\n",
    "    dead_feature_window=200,\n",
    "    dead_feature_threshold = 5e-6,\n",
    "    \n",
    "    # WANDB\n",
    "    log_to_wandb = True,\n",
    "    wandb_project= \"mats_sae_training_gpt2_small_hook_q_dev\",\n",
    "    wandb_entity = None,\n",
    "    wandb_log_frequency=5,\n",
    "    \n",
    "    # Misc\n",
    "    device = \"mps\",\n",
    "    seed = 42,\n",
    "    n_checkpoints = 0,\n",
    "    checkpoint_path = \"checkpoints\",\n",
    "    dtype = torch.float32,\n",
    "    )\n",
    "\n",
    "# cfg.d_sae\n",
    "sparse_autoencoder = language_model_sae_runner(cfg)\n",
    "# assert sparse_autoencoder is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mats_sae_training",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
