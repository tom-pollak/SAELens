{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gelu-2L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run name: 16384-L1-0.0006-LR-0.0012-Tokens-1.000e+08\n",
      "n_tokens_per_buffer (millions): 0.524288\n",
      "Lower bound: n_contexts_per_buffer (millions): 0.004096\n",
      "Total training steps: 24414\n",
      "Total wandb updates: 2441\n",
      "n_dead_feature_samples: 3\n",
      "Loaded pretrained model gelu-2l into HookedTransformer\n",
      "Moving model to device:  cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90f295931ea744df8b6f48ef8bc87578",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset is tokenized! Updating config.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjbloom\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/paperspace/mats_sae_training/research/wandb/run-20231221_223239-0lixs04m</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jbloom/mats_sae_training_language_models_gelu_2l_test/runs/0lixs04m' target=\"_blank\">16384-L1-0.0006-LR-0.0012-Tokens-1.000e+08</a></strong> to <a href='https://wandb.ai/jbloom/mats_sae_training_language_models_gelu_2l_test' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jbloom/mats_sae_training_language_models_gelu_2l_test' target=\"_blank\">https://wandb.ai/jbloom/mats_sae_training_language_models_gelu_2l_test</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jbloom/mats_sae_training_language_models_gelu_2l_test/runs/0lixs04m' target=\"_blank\">https://wandb.ai/jbloom/mats_sae_training_language_models_gelu_2l_test/runs/0lixs04m</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting losses for resampling...: 100%|██████████| 32/32 [00:01<00:00, 18.66it/s]1029.76it/s] \n",
      "Collecting losses for resampling...: 100%|██████████| 32/32 [00:01<00:00, 19.83it/s]13914.56it/s]\n",
      "Collecting losses for resampling...: 100%|██████████| 32/32 [00:01<00:00, 18.27it/s]145740.16it/s]\n",
      "Collecting losses for resampling...: 100%|██████████| 32/32 [00:01<00:00, 18.25it/s]178218.82it/s]\n",
      "20953| MSE Loss 0.012 | L1 0.006:  86%|████████▌ | 85823488/100000000 [10:03<01:40, 141029.74it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/paperspace/mats_sae_training/research/run.ipynb Cell 2\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2250617065727370616365227d/home/paperspace/mats_sae_training/research/run.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m os\u001b[39m.\u001b[39menviron[\u001b[39m\"\u001b[39m\u001b[39mTOKENIZERS_PARALLELISM\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mfalse\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2250617065727370616365227d/home/paperspace/mats_sae_training/research/run.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m cfg \u001b[39m=\u001b[39m LanguageModelSAERunnerConfig(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2250617065727370616365227d/home/paperspace/mats_sae_training/research/run.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2250617065727370616365227d/home/paperspace/mats_sae_training/research/run.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m     \u001b[39m# Data Generating Function (Model + Training Distibuion)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2250617065727370616365227d/home/paperspace/mats_sae_training/research/run.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=54'>55</a>\u001b[0m     dtype \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mbfloat16,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2250617065727370616365227d/home/paperspace/mats_sae_training/research/run.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=55'>56</a>\u001b[0m     )\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2250617065727370616365227d/home/paperspace/mats_sae_training/research/run.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=58'>59</a>\u001b[0m sparse_autoencoder \u001b[39m=\u001b[39m language_model_sae_runner(cfg)\n",
      "File \u001b[0;32m~/mats_sae_training/research/../sae_training/lm_runner.py:29\u001b[0m, in \u001b[0;36mlanguage_model_sae_runner\u001b[0;34m(cfg)\u001b[0m\n\u001b[1;32m     26\u001b[0m     wandb\u001b[39m.\u001b[39minit(project\u001b[39m=\u001b[39mcfg\u001b[39m.\u001b[39mwandb_project, config\u001b[39m=\u001b[39mcfg, name\u001b[39m=\u001b[39mcfg\u001b[39m.\u001b[39mrun_name)\n\u001b[1;32m     28\u001b[0m \u001b[39m# train SAE\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m sparse_autoencoder \u001b[39m=\u001b[39m train_sae_on_language_model(\n\u001b[1;32m     30\u001b[0m     model, sparse_autoencoder, activations_loader,\n\u001b[1;32m     31\u001b[0m     n_checkpoints\u001b[39m=\u001b[39;49mcfg\u001b[39m.\u001b[39;49mn_checkpoints,\n\u001b[1;32m     32\u001b[0m     batch_size \u001b[39m=\u001b[39;49m cfg\u001b[39m.\u001b[39;49mtrain_batch_size,\n\u001b[1;32m     33\u001b[0m     feature_sampling_method \u001b[39m=\u001b[39;49m cfg\u001b[39m.\u001b[39;49mfeature_sampling_method,\n\u001b[1;32m     34\u001b[0m     feature_sampling_window \u001b[39m=\u001b[39;49m cfg\u001b[39m.\u001b[39;49mfeature_sampling_window,\n\u001b[1;32m     35\u001b[0m     feature_reinit_scale \u001b[39m=\u001b[39;49m cfg\u001b[39m.\u001b[39;49mfeature_reinit_scale,\n\u001b[1;32m     36\u001b[0m     dead_feature_threshold \u001b[39m=\u001b[39;49m cfg\u001b[39m.\u001b[39;49mdead_feature_threshold,\n\u001b[1;32m     37\u001b[0m     dead_feature_window\u001b[39m=\u001b[39;49mcfg\u001b[39m.\u001b[39;49mdead_feature_window,\n\u001b[1;32m     38\u001b[0m     use_wandb \u001b[39m=\u001b[39;49m cfg\u001b[39m.\u001b[39;49mlog_to_wandb,\n\u001b[1;32m     39\u001b[0m     wandb_log_frequency \u001b[39m=\u001b[39;49m cfg\u001b[39m.\u001b[39;49mwandb_log_frequency\n\u001b[1;32m     40\u001b[0m )\n\u001b[1;32m     42\u001b[0m \u001b[39m# save sae to checkpoints folder\u001b[39;00m\n\u001b[1;32m     43\u001b[0m path \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mcfg\u001b[39m.\u001b[39mcheckpoint_path\u001b[39m}\u001b[39;00m\u001b[39m/final_\u001b[39m\u001b[39m{\u001b[39;00msparse_autoencoder\u001b[39m.\u001b[39mget_name()\u001b[39m}\u001b[39;00m\u001b[39m.pt\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[0;32m~/mats_sae_training/research/../sae_training/train_sae_on_language_model.py:211\u001b[0m, in \u001b[0;36mtrain_sae_on_language_model\u001b[0;34m(model, sparse_autoencoder, activation_store, batch_size, n_checkpoints, feature_sampling_method, feature_sampling_window, feature_reinit_scale, dead_feature_threshold, dead_feature_window, use_wandb, wandb_log_frequency)\u001b[0m\n\u001b[1;32m    206\u001b[0m     pbar\u001b[39m.\u001b[39mset_description(\n\u001b[1;32m    207\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mn_training_steps\u001b[39m}\u001b[39;00m\u001b[39m| MSE Loss \u001b[39m\u001b[39m{\u001b[39;00mmse_loss\u001b[39m.\u001b[39mitem()\u001b[39m:\u001b[39;00m\u001b[39m.3f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m | L1 \u001b[39m\u001b[39m{\u001b[39;00ml1_loss\u001b[39m.\u001b[39mitem()\u001b[39m:\u001b[39;00m\u001b[39m.3f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    208\u001b[0m     )\n\u001b[1;32m    209\u001b[0m     pbar\u001b[39m.\u001b[39mupdate(batch_size)\n\u001b[0;32m--> 211\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m    212\u001b[0m sparse_autoencoder\u001b[39m.\u001b[39mremove_gradient_parallel_to_decoder_directions()\n\u001b[1;32m    213\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/miniconda3/envs/mats_sae_training/lib/python3.11/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    493\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    494\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/mats_sae_training/lib/python3.11/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[39m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m     tensors,\n\u001b[1;32m    253\u001b[0m     grad_tensors_,\n\u001b[1;32m    254\u001b[0m     retain_graph,\n\u001b[1;32m    255\u001b[0m     create_graph,\n\u001b[1;32m    256\u001b[0m     inputs,\n\u001b[1;32m    257\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    258\u001b[0m     accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    259\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os \n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from sae_training.config import LanguageModelSAERunnerConfig\n",
    "from sae_training.lm_runner import language_model_sae_runner\n",
    "\n",
    "import cProfile\n",
    "\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "cfg = LanguageModelSAERunnerConfig(\n",
    "\n",
    "    # Data Generating Function (Model + Training Distibuion)\n",
    "    model_name = \"gelu-2l\",\n",
    "    hook_point = \"blocks.0.hook_mlp_out\",\n",
    "    hook_point_layer = 0,\n",
    "    d_in = 512,\n",
    "    dataset_path = \"NeelNanda/c4-tokenized-2b\",\n",
    "    is_dataset_tokenized=True,\n",
    "    \n",
    "    # SAE Parameters\n",
    "    expansion_factor = 32,\n",
    "    \n",
    "    # Training Parameters\n",
    "    lr = 0.0012,\n",
    "    lr_scheduler_name=\"constantwithwarmup\",\n",
    "    l1_coefficient = 0.0006,\n",
    "    train_batch_size = 4096,\n",
    "    context_size = 128,\n",
    "    \n",
    "    # Activation Store Parameters\n",
    "    n_batches_in_buffer = 128,\n",
    "    total_training_tokens = 1_000_000 * 100, \n",
    "    store_batch_size = 32,\n",
    "    \n",
    "    # Resampling protocol\n",
    "    feature_sampling_method = 'anthropic',\n",
    "    feature_reinit_scale = 0.2,\n",
    "    dead_feature_window=5000,\n",
    "    dead_feature_threshold = 1e-4,\n",
    "    \n",
    "    # WANDB\n",
    "    log_to_wandb = True,\n",
    "    wandb_project= \"mats_sae_training_language_models_gelu_2l_test\",\n",
    "    wandb_entity = None,\n",
    "    wandb_log_frequency=10,\n",
    "    \n",
    "    # Misc\n",
    "    device = \"cuda\",\n",
    "    seed = 42,\n",
    "    n_checkpoints = 0,\n",
    "    checkpoint_path = \"checkpoints\",\n",
    "    dtype = torch.bfloat16,\n",
    "    )\n",
    "\n",
    "\n",
    "sparse_autoencoder = language_model_sae_runner(cfg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT2 - Small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os \n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"WANDB__SERVICE_WAIT\"] = \"300\"\n",
    "\n",
    "from sae_training.config import LanguageModelSAERunnerConfig\n",
    "from sae_training.lm_runner import language_model_sae_runner\n",
    "\n",
    "cfg = LanguageModelSAERunnerConfig(\n",
    "\n",
    "    # Data Generating Function (Model + Training Distibuion)\n",
    "    model_name = \"gpt2-small\",\n",
    "    hook_point = \"blocks.10.hook_resid_pre\",\n",
    "    hook_point_layer = 11,\n",
    "    d_in = 768,\n",
    "    dataset_path = \"Skylion007/openwebtext\",\n",
    "    is_dataset_tokenized=False,\n",
    "    \n",
    "    # SAE Parameters\n",
    "    expansion_factor = 64, # determines the dimension of the SAE.\n",
    "    \n",
    "    # Training Parameters\n",
    "    lr = 1e-5,\n",
    "    l1_coefficient = 5e-4,\n",
    "    lr_scheduler_name=None,\n",
    "    train_batch_size = 4096,\n",
    "    context_size = 128,\n",
    "    \n",
    "    # Activation Store Parameters\n",
    "    n_batches_in_buffer = 128,\n",
    "    total_training_tokens = 1_000_000 * 200, # 200M tokens seems doable overnight.\n",
    "    store_batch_size = 32,\n",
    "    \n",
    "    # Resampling protocol\n",
    "    feature_sampling_method = 'l2',\n",
    "    feature_sampling_window = 1000,\n",
    "    feature_reinit_scale = 0.2,\n",
    "    dead_feature_window=5000,\n",
    "    dead_feature_threshold = 1e-7,\n",
    "    \n",
    "    # WANDB\n",
    "    log_to_wandb = True,\n",
    "    wandb_project= \"mats_sae_training_gpt2_small\",\n",
    "    wandb_entity = None,\n",
    "    wandb_log_frequency=50,\n",
    "    \n",
    "    # Misc\n",
    "    device = \"mps\",\n",
    "    seed = 42,\n",
    "    n_checkpoints = 5,\n",
    "    checkpoint_path = \"checkpoints\",\n",
    "    dtype = torch.float32,\n",
    "    )\n",
    "\n",
    "sparse_autoencoder = language_model_sae_runner(cfg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT2-Small Hook Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os \n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"WANDB__SERVICE_WAIT\"] = \"300\"\n",
    "\n",
    "from sae_training.config import LanguageModelSAERunnerConfig\n",
    "from sae_training.lm_runner import language_model_sae_runner\n",
    "\n",
    "\n",
    "\n",
    "cfg = LanguageModelSAERunnerConfig(\n",
    "\n",
    "    # Data Generating Function (Model + Training Distibuion)\n",
    "    model_name = \"gpt2-small\",\n",
    "    hook_point = \"blocks.10.attn.hook_q\",\n",
    "    hook_point_layer = 10,\n",
    "    hook_point_head_index=7,\n",
    "    d_in = 64,\n",
    "    dataset_path = \"Skylion007/openwebtext\",\n",
    "    is_dataset_tokenized=False,\n",
    "    use_cached_activations=True,\n",
    "    cached_activations_path=\"../activations/\",\n",
    "    \n",
    "    # SAE Parameters\n",
    "    expansion_factor = 2048, # 64 -> 4096, 128 -> 8192, 256 -> 16384, 512 -> 32768, 1024 -> 65536\n",
    "    \n",
    "    # Training Parameters\n",
    "    lr = 0.0012,\n",
    "    l1_coefficient = 0.008,\n",
    "    lr_scheduler_name=\"constantwithwarmup\",\n",
    "    lr_warm_up_steps=1000, # about 4 million tokens.\n",
    "    train_batch_size = 4096,\n",
    "    context_size = 128,\n",
    "    \n",
    "    # Activation Store Parameters\n",
    "    n_batches_in_buffer = 128,\n",
    "    total_training_tokens = 1_000_000 * 500, #- 2_500_000,# avoid having to muse a buffer we don't have.\n",
    "    store_batch_size = 32,\n",
    "    \n",
    "    # Resampling protocol\n",
    "    feature_sampling_method = 'l2',\n",
    "    feature_sampling_window = 1000,# doesn't do anything currently.\n",
    "    feature_reinit_scale = 0.2,\n",
    "    dead_feature_window=21000,\n",
    "    dead_feature_threshold = 1e-6,\n",
    "    \n",
    "    # WANDB\n",
    "    log_to_wandb = True,\n",
    "    wandb_project= \"mats_sae_training_gpt2_small_hook_q_new2\",\n",
    "    wandb_entity = None,\n",
    "    wandb_log_frequency=200,\n",
    "    \n",
    "    # Misc\n",
    "    device = \"cuda\",\n",
    "    seed = 42,\n",
    "    n_checkpoints = 5,\n",
    "    checkpoint_path = \"checkpoints\",\n",
    "    dtype = torch.bfloat16,\n",
    "    )\n",
    "\n",
    "sparse_autoencoder = language_model_sae_runner(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test from Pretrained:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os \n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"WANDB__SERVICE_WAIT\"] = \"300\"\n",
    "\n",
    "from sae_training.config import LanguageModelSAERunnerConfig\n",
    "from sae_training.lm_runner import language_model_sae_runner\n",
    "\n",
    "pretrained_path = \"checkpoints/xvnq6hwz/300003328_sparse_autoencoder_gpt2-small_blocks.10.attn.hook_q_131072.pt\"\n",
    "\n",
    "cfg = LanguageModelSAERunnerConfig(\n",
    "\n",
    "    # Data Generating Function (Model + Training Distibuion)\n",
    "    model_name = \"gpt2-small\",\n",
    "    hook_point = \"blocks.10.attn.hook_q\",\n",
    "    hook_point_layer = 10,\n",
    "    hook_point_head_index=7,\n",
    "    d_in = 64,\n",
    "    dataset_path = \"Skylion007/openwebtext\",\n",
    "    is_dataset_tokenized=False,\n",
    "    use_cached_activations=True,\n",
    "    cached_activations_path=\"../activations/\",\n",
    "    \n",
    "    # SAE Parameters\n",
    "    from_pretrained_path = pretrained_path,\n",
    "    expansion_factor = 2048, # 64 -> 4096, 128 -> 8192, 256 -> 16384, 512 -> 32768, 1024 -> 65536\n",
    "    \n",
    "    # Training Parameters\n",
    "    lr = 0.0012,\n",
    "    l1_coefficient = 0.008,\n",
    "    lr_scheduler_name=\"constantwithwarmup\",\n",
    "    lr_warm_up_steps=1000, # about 4 million tokens.\n",
    "    train_batch_size = 4096,\n",
    "    context_size = 128,\n",
    "    \n",
    "    # Activation Store Parameters\n",
    "    n_batches_in_buffer = 128,\n",
    "    total_training_tokens = 1_000_000 * 500, #- 2_500_000,# avoid having to muse a buffer we don't have.\n",
    "    store_batch_size = 32,\n",
    "    \n",
    "    # Resampling protocol\n",
    "    feature_sampling_method = 'l2',\n",
    "    feature_sampling_window = 1000,# doesn't do anything currently.\n",
    "    feature_reinit_scale = 0.2,\n",
    "    dead_feature_window=2000,\n",
    "    dead_feature_threshold = 1e-6,\n",
    "    \n",
    "    # WANDB\n",
    "    log_to_wandb = False,\n",
    "    wandb_project= \"mats_sae_training_gpt2_small_hook_q_new2\",\n",
    "    wandb_entity = None,\n",
    "    wandb_log_frequency=200,\n",
    "    \n",
    "    # Misc\n",
    "    device = \"cuda\",\n",
    "    seed = 42,\n",
    "    n_checkpoints = 5,\n",
    "    checkpoint_path = \"checkpoints\",\n",
    "    dtype = torch.bfloat16,\n",
    "    )\n",
    "\n",
    "sparse_autoencoder = language_model_sae_runner(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(1_000_000 * 500 - 4096) / 4096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_autoencoder.save_model(\"./overnight_sae_resid_pre_10_gpt_2_small.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pythia 70-M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os \n",
    "\n",
    "from sae_training.config import LanguageModelSAERunnerConfig\n",
    "from sae_training.lm_runner import language_model_sae_runner\n",
    "\n",
    "import cProfile\n",
    "\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "cfg = LanguageModelSAERunnerConfig(\n",
    "\n",
    "    # Data Generating Function (Model + Training Distibuion)\n",
    "    model_name = \"pythia-70m\",\n",
    "    hook_point = \"blocks.0.hook_mlp_out\",\n",
    "    hook_point_layer = 0,\n",
    "    d_in = 512,\n",
    "    dataset_path = \"EleutherAI/the_pile_deduplicated\",\n",
    "    is_dataset_tokenized=False,\n",
    "    \n",
    "    # SAE Parameters\n",
    "    expansion_factor = 16,\n",
    "    \n",
    "    # Training Parameters\n",
    "    lr = 3e-4,\n",
    "    l1_coefficient = 1e-3,\n",
    "    train_batch_size = 4096,\n",
    "    context_size = 128,\n",
    "    \n",
    "    # Activation Store Parameters\n",
    "    n_batches_in_buffer = 64,\n",
    "    total_training_tokens = 1_000_000 * 5, \n",
    "    store_batch_size = 32,\n",
    "    \n",
    "    # Resampling protocol\n",
    "    feature_sampling_method = 'l2',\n",
    "    feature_sampling_window = 2500, # Doesn't currently matter.\n",
    "    feature_reinit_scale = 0.2,\n",
    "    dead_feature_window=1250,\n",
    "    dead_feature_threshold = 1e-8,\n",
    "    \n",
    "    # WANDB\n",
    "    log_to_wandb = True,\n",
    "    wandb_project= \"mats_sae_training_language_benchmark_tests\",\n",
    "    wandb_entity = None,\n",
    "    wandb_log_frequency=10,\n",
    "    \n",
    "    # Misc\n",
    "    device = \"mps\",\n",
    "    seed = 42,\n",
    "    n_checkpoints = 0,\n",
    "    checkpoint_path = \"checkpoints\",\n",
    "    dtype = torch.float32,\n",
    "    )\n",
    "\n",
    "def main():\n",
    "    sparse_autoencoder = language_model_sae_runner(cfg)\n",
    "\n",
    "main()\n",
    "# os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "# import cProfile, pstats, io\n",
    "# from pstats import SortKey\n",
    "# pr = cProfile.Profile()\n",
    "# pr.enable()\n",
    "# # ... do something ...\n",
    "# main()\n",
    "# pr.disable()\n",
    "# s = io.StringIO()\n",
    "# sortby = SortKey.CUMULATIVE\n",
    "# ps = pstats.Stats(pr, stream=s).sort_stats(sortby)\n",
    "# ps.print_stats()\n",
    "# print(s.getvalue())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tiny Stories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os \n",
    "\n",
    "from sae_training.config import LanguageModelSAERunnerConfig\n",
    "from sae_training.lm_runner import language_model_sae_runner\n",
    "\n",
    "\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "cfg = LanguageModelSAERunnerConfig(\n",
    "\n",
    "    # Data Generating Function (Model + Training Distibuion)\n",
    "    model_name = \"tiny-stories-2L-33M\",\n",
    "    hook_point = \"blocks.1.mlp.hook_post\",\n",
    "    hook_point_layer = 1,\n",
    "    d_in = 4096,\n",
    "    dataset_path = \"roneneldan/TinyStories\",\n",
    "    is_dataset_tokenized=False,\n",
    "    \n",
    "    # SAE Parameters\n",
    "    expansion_factor = 4,\n",
    "    \n",
    "    # Training Parameters\n",
    "    lr = 1e-4,\n",
    "    l1_coefficient = 3e-4,\n",
    "    train_batch_size = 4096,\n",
    "    context_size = 128,\n",
    "    \n",
    "    # Activation Store Parameters\n",
    "    n_batches_in_buffer = 128,\n",
    "    total_training_tokens = 1_000_000 * 10, # want 500M eventually.\n",
    "    store_batch_size = 32,\n",
    "    \n",
    "    # Resampling protocol\n",
    "    feature_sampling_method = 'l2',\n",
    "    feature_sampling_window = 2500, # Doesn't currently matter.\n",
    "    feature_reinit_scale = 0.2,\n",
    "    dead_feature_window=1250,\n",
    "    dead_feature_threshold = 0.0005,\n",
    "    \n",
    "    # WANDB\n",
    "    log_to_wandb = True,\n",
    "    wandb_project= \"mats_sae_training_language_benchmark_tests\",\n",
    "    wandb_entity = None,\n",
    "    wandb_log_frequency=10,\n",
    "    \n",
    "    # Misc\n",
    "    device = \"mps\",\n",
    "    seed = 42,\n",
    "    n_checkpoints = 0,\n",
    "    checkpoint_path = \"checkpoints\",\n",
    "    dtype = torch.float32,\n",
    "    )\n",
    "\n",
    "sparse_autoencoder = language_model_sae_runner(cfg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toy Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sae_training.toy_model_runner import SAEToyModelRunnerConfig, toy_model_sae_runner\n",
    "\n",
    "\n",
    "cfg = SAEToyModelRunnerConfig(\n",
    "    \n",
    "    # Model Details\n",
    "    n_features=200,\n",
    "    n_hidden=5,\n",
    "    n_correlated_pairs=0,\n",
    "    n_anticorrelated_pairs=0,\n",
    "    feature_probability=0.025,\n",
    "    model_training_steps=10_000,\n",
    "    \n",
    "    # SAE Parameters\n",
    "    d_sae=240,\n",
    "    l1_coefficient=0.001,\n",
    "    \n",
    "    # SAE Train Config\n",
    "    train_batch_size=1028,\n",
    "    feature_sampling_window=3_000,\n",
    "    dead_feature_window=1_000,\n",
    "    feature_reinit_scale=0.5,\n",
    "    total_training_tokens=4096*300,\n",
    "    \n",
    "    # Other parameters\n",
    "    log_to_wandb=True,\n",
    "    wandb_project=\"sae-training-test\",\n",
    "    wandb_log_frequency=5,\n",
    "    device=\"mps\",\n",
    ")\n",
    "\n",
    "trained_sae = toy_model_sae_runner(cfg)\n",
    "\n",
    "assert trained_sae is not None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os \n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"WANDB__SERVICE_WAIT\"] = \"300\"\n",
    "\n",
    "from sae_training.config import CacheActivationsRunnerConfig\n",
    "from sae_training.cache_activations_runner import cache_activations_runner\n",
    "\n",
    "cfg = CacheActivationsRunnerConfig(\n",
    "\n",
    "    # Data Generating Function (Model + Training Distibuion)\n",
    "    model_name = \"gpt2-small\",\n",
    "    hook_point = \"blocks.10.attn.hook_q\",\n",
    "    hook_point_layer = 10,\n",
    "    hook_point_head_index=7,\n",
    "    d_in = 64,\n",
    "    dataset_path = \"Skylion007/openwebtext\",\n",
    "    is_dataset_tokenized=False,\n",
    "    cached_activations_path=\"../activations/\",\n",
    "    \n",
    "    # Activation Store Parameters\n",
    "    n_batches_in_buffer = 16,\n",
    "    total_training_tokens = 500_000_000, \n",
    "    store_batch_size = 32,\n",
    "\n",
    "    # Activation caching shuffle parameters\n",
    "    n_shuffles_final = 16,\n",
    "    \n",
    "    # Misc\n",
    "    device = \"mps\",\n",
    "    seed = 42,\n",
    "    dtype = torch.float32,\n",
    "    )\n",
    "\n",
    "cache_activations_runner(cfg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train an SAE using the cached activations stored on disk\n",
    "Pass `use_cached_activations=True` into the config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os \n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"WANDB__SERVICE_WAIT\"] = \"300\"\n",
    "from sae_training.config import LanguageModelSAERunnerConfig\n",
    "from sae_training.lm_runner import language_model_sae_runner\n",
    "\n",
    "cfg = LanguageModelSAERunnerConfig(\n",
    "\n",
    "    # Data Generating Function (Model + Training Distibuion)\n",
    "    model_name = \"gpt2-small\",\n",
    "    hook_point = \"blocks.10.hook_resid_pre\",\n",
    "    hook_point_layer = 11,\n",
    "    d_in = 768,\n",
    "    dataset_path = \"Skylion007/openwebtext\",\n",
    "    is_dataset_tokenized=False,\n",
    "    use_cached_activations=True,\n",
    "    \n",
    "    # SAE Parameters\n",
    "    expansion_factor = 64, # determines the dimension of the SAE.\n",
    "    \n",
    "    # Training Parameters\n",
    "    lr = 1e-5,\n",
    "    l1_coefficient = 5e-4,\n",
    "    lr_scheduler_name=None,\n",
    "    train_batch_size = 4096,\n",
    "    context_size = 128,\n",
    "    \n",
    "    # Activation Store Parameters\n",
    "    n_batches_in_buffer = 64,\n",
    "    total_training_tokens = 200_000,\n",
    "    store_batch_size = 32,\n",
    "    \n",
    "    # Resampling protocol\n",
    "    feature_sampling_method = 'l2',\n",
    "    feature_sampling_window = 1000,\n",
    "    feature_reinit_scale = 0.2,\n",
    "    dead_feature_window=5000,\n",
    "    dead_feature_threshold = 1e-7,\n",
    "    \n",
    "    # WANDB\n",
    "    log_to_wandb = True,\n",
    "    wandb_project= \"mats_sae_training_gpt2_small\",\n",
    "    wandb_entity = None,\n",
    "    wandb_log_frequency=50,\n",
    "    \n",
    "    # Misc\n",
    "    device = \"mps\",\n",
    "    seed = 42,\n",
    "    n_checkpoints = 5,\n",
    "    checkpoint_path = \"checkpoints\",\n",
    "    dtype = torch.float32,\n",
    "    )\n",
    "\n",
    "sparse_autoencoder = language_model_sae_runner(cfg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run caching of activations to disk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os \n",
    "import sys \n",
    "sys.path.append(\"..\")\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"WANDB__SERVICE_WAIT\"] = \"300\"\n",
    "from sae_training.config import LanguageModelSAERunnerConfig\n",
    "from sae_training.lm_runner import language_model_sae_runner\n",
    "\n",
    "\n",
    "\n",
    "# for l1_coefficient in [9e-4,8e-4,7e-4]:\n",
    "cfg = LanguageModelSAERunnerConfig(\n",
    "\n",
    "    # Data Generating Function (Model + Training Distibuion)\n",
    "    model_name = \"gpt2-small\",\n",
    "    hook_point = \"blocks.10.attn.hook_q\",\n",
    "    hook_point_layer = 10,\n",
    "    hook_point_head_index=7,\n",
    "    d_in = 64,\n",
    "    dataset_path = \"Skylion007/openwebtext\",\n",
    "    is_dataset_tokenized=False,\n",
    "    use_cached_activations=True,\n",
    "    cached_activations_path=\"../activations/\",\n",
    "    \n",
    "    # SAE Parameters\n",
    "    expansion_factor = 64, # determines the dimension of the SAE. (64*64 = 4096, 64*4*64 = 32768)\n",
    "    \n",
    "    # Training Parameters\n",
    "    lr = 1e-3,\n",
    "    l1_coefficient = 2e-4,\n",
    "    # lr_scheduler_name=\"LinearWarmupDecay\",\n",
    "    lr_warm_up_steps=2200,\n",
    "    train_batch_size = 4096,\n",
    "    context_size = 128,\n",
    "    \n",
    "    # Activation Store Parameters\n",
    "    n_batches_in_buffer = 512,\n",
    "    total_training_tokens = 3_000_000,\n",
    "    store_batch_size = 32,\n",
    "    \n",
    "    # Resampling protocol\n",
    "    feature_sampling_method = 'l2',\n",
    "    feature_sampling_window = 1000,\n",
    "    feature_reinit_scale = 0.2,\n",
    "    dead_feature_window=200,\n",
    "    dead_feature_threshold = 5e-6,\n",
    "    \n",
    "    # WANDB\n",
    "    log_to_wandb = True,\n",
    "    wandb_project= \"mats_sae_training_gpt2_small_hook_q_dev\",\n",
    "    wandb_entity = None,\n",
    "    wandb_log_frequency=5,\n",
    "    \n",
    "    # Misc\n",
    "    device = \"mps\",\n",
    "    seed = 42,\n",
    "    n_checkpoints = 0,\n",
    "    checkpoint_path = \"checkpoints\",\n",
    "    dtype = torch.float32,\n",
    "    )\n",
    "\n",
    "# cfg.d_sae\n",
    "sparse_autoencoder = language_model_sae_runner(cfg)\n",
    "# assert sparse_autoencoder is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mats_sae_training",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
